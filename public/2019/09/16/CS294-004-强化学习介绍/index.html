<!-- build time:Mon Jan 20 2020 11:32:24 GMT+0800 (GMT+08:00) --><!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 3.9.0"><meta name="referrer" content="no-referrer"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png"><link rel="mask-icon" href="/images/logo.svg" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css"><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:new URL("http://yoursite.com").hostname,root:"/",scheme:"Gemini",version:"7.7.0",exturl:!1,sidebar:{position:"left",display:"always",padding:18,offset:12,onmobile:!1},copycode:{enable:!1,show_result:!1,style:null},back2top:{enable:!0,sidebar:!0,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:null,storage:!0,lazyload:!1,nav:null},algolia:{appID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},path:"./public/search.xml",motion:{enable:!1,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}}}</script><meta name="description" content="定义符号$s_t$ 表示状态，$o_t$表示观测值，$a_t$表示动作，$\pi(a_t|o_t)$表示部分观测下的策略，$\pi(a_t|s_t)$表示完全观测下的策略。模仿学习收集训练数据$\lt o_t,a_t\gt$ ，进行监督学习得到$\pi_\theta(a_t|o_t)$奖励函数$r(s, a)$ 定义了在某个状态下执行某个动作后得到的奖励。但强化学习期望得到的是全局的收益最大，即暂"><meta name="keywords" content="RL"><meta property="og:type" content="article"><meta property="og:title" content="CS294-004-强化学习介绍"><meta property="og:url" content="http://yoursite.com/2019/09/16/CS294-004-强化学习介绍/index.html"><meta property="og:site_name" content="Xu Hang&#39;s Blog"><meta property="og:description" content="定义符号$s_t$ 表示状态，$o_t$表示观测值，$a_t$表示动作，$\pi(a_t|o_t)$表示部分观测下的策略，$\pi(a_t|s_t)$表示完全观测下的策略。模仿学习收集训练数据$\lt o_t,a_t\gt$ ，进行监督学习得到$\pi_\theta(a_t|o_t)$奖励函数$r(s, a)$ 定义了在某个状态下执行某个动作后得到的奖励。但强化学习期望得到的是全局的收益最大，即暂"><meta property="og:locale" content="zh-CN"><meta property="og:image" content="https://ww1.sinaimg.cn/large/006A69aEly1gb2s41hc0oj31l10axwhy.jpg"><meta property="og:image" content="https://ww1.sinaimg.cn/large/006A69aEgy1g72qzb478ej30ek09vglu.jpg"><meta property="og:image" content="https://ww1.sinaimg.cn/large/006A69aEly1gb2tuu7mnqj30z1083aan.jpg"><meta property="og:image" content="https://ww1.sinaimg.cn/large/006A69aEgy1gb2uwl8l4aj30wi0eiq49.jpg"><meta property="og:image" content="https://ww1.sinaimg.cn/large/006A69aEgy1gb2uvyxvelj30xr0fc0u0.jpg"><meta property="og:image" content="https://ww1.sinaimg.cn/large/006A69aEgy1gb2uxbtqz6j30vm0f4jsl.jpg"><meta property="og:image" content="https://ww1.sinaimg.cn/large/006A69aEgy1gb2uuupfnuj30ri0fm0tz.jpg"><meta property="og:image" content="https://ww1.sinaimg.cn/large/006A69aEly1g73skv6974j30oy06i74n.jpg"><meta property="og:updated_time" content="2020-01-20T03:32:11.298Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="CS294-004-强化学习介绍"><meta name="twitter:description" content="定义符号$s_t$ 表示状态，$o_t$表示观测值，$a_t$表示动作，$\pi(a_t|o_t)$表示部分观测下的策略，$\pi(a_t|s_t)$表示完全观测下的策略。模仿学习收集训练数据$\lt o_t,a_t\gt$ ，进行监督学习得到$\pi_\theta(a_t|o_t)$奖励函数$r(s, a)$ 定义了在某个状态下执行某个动作后得到的奖励。但强化学习期望得到的是全局的收益最大，即暂"><meta name="twitter:image" content="https://ww1.sinaimg.cn/large/006A69aEly1gb2s41hc0oj31l10axwhy.jpg"><link rel="canonical" href="http://yoursite.com/2019/09/16/CS294-004-强化学习介绍/"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0}</script><title>CS294-004-强化学习介绍 | Xu Hang's Blog</title><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?3c4cabfcaad01105839ab3c0c61698ab";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-meta"><div><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">Xu Hang's Blog</span> <span class="logo-line-after"><i></i></span></a></div></div><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索</a></li></ul></nav><div class="site-search"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocorrect="off" autocapitalize="none" placeholder="搜索..." spellcheck="false" type="text" id="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"></div></div><div class="search-pop-overlay"></div></div></div></header><a href="https://github.com/rpSebastian" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content"><div class="posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/09/16/CS294-004-强化学习介绍/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.gif"><meta itemprop="name" content="Xu Hang"><meta itemprop="description" content=""></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Xu Hang's Blog"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">CS294-004-强化学习介绍</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2019-09-16 21:49:07" itemprop="dateCreated datePublished" datetime="2019-09-16T21:49:07+08:00">2019-09-16</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-check-o"></i> </span><span class="post-meta-item-text">更新于</span> <time title="修改时间：2020-01-20 11:32:11" itemprop="dateModified" datetime="2020-01-20T11:32:11+08:00">2020-01-20</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/RL-CS294/" itemprop="url" rel="index"><span itemprop="name">RL(CS294)</span></a></span></span></div></header><div class="post-body" itemprop="articleBody"><h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><h4 id="符号"><a href="#符号" class="headerlink" title="符号"></a>符号</h4><p>$s_t$ 表示状态，$o_t$表示观测值，$a_t$表示动作，$\pi(a_t|o_t)$表示部分观测下的策略，$\pi(a_t|s_t)$表示完全观测下的策略。</p><p><img src="https://ww1.sinaimg.cn/large/006A69aEly1gb2s41hc0oj31l10axwhy.jpg" alt="image.png"></p><h4 id="模仿学习"><a href="#模仿学习" class="headerlink" title="模仿学习"></a>模仿学习</h4><p>收集训练数据$\lt o_t,a_t\gt$ ，进行监督学习得到$\pi_\theta(a_t|o_t)$</p><h4 id="奖励函数"><a href="#奖励函数" class="headerlink" title="奖励函数"></a>奖励函数</h4><p>$r(s, a)$ 定义了在某个状态下执行某个动作后得到的奖励。但强化学习期望得到的是全局的收益最大，即暂时的较小奖励可能会带来更大收益。</p><h4 id="马尔科夫链"><a href="#马尔科夫链" class="headerlink" title="马尔科夫链"></a>马尔科夫链</h4><h5 id="马尔科夫链-1"><a href="#马尔科夫链-1" class="headerlink" title="马尔科夫链"></a>马尔科夫链</h5><script type="math/tex;mode=display">\mathcal{M} = \{\mathcal{S}, \mathcal{T}\}</script><ul><li>$\mathcal{S}$ 为状态空间</li><li>$\mathcal{T}$ 为状态转移算子 $T_{i, j} = p(s_{t+1} = i | s_t=j)$, 可以将一个状态分布转移到另一个状态分布。</li><li>若$\mu_{t, j} = p(s_t = i)$ , 则 $\vec \mu_{t+1} = T\vec\mu_t$</li></ul><h5 id="马尔科夫决策链-MDP"><a href="#马尔科夫决策链-MDP" class="headerlink" title="马尔科夫决策链 MDP"></a>马尔科夫决策链 MDP</h5><script type="math/tex;mode=display">M = \{S, A, T, r\}</script><ul><li>$\mathcal{S}$ 为状态空间</li><li>$\mathcal{A}$为动作空间</li><li>$\mathcal{T}$为状态转移算子, $\mathcal{T}_{i,j,k} = p(s_{t+1}=i | s_t=j, a_t=k)$</li><li>$r$ 为奖励函数 $r: S × A \rightarrow \mathbb{R}$</li></ul><h5 id="部分观测马尔科夫决策链-POMDP"><a href="#部分观测马尔科夫决策链-POMDP" class="headerlink" title="部分观测马尔科夫决策链 POMDP"></a>部分观测马尔科夫决策链 POMDP</h5><script type="math/tex;mode=display">M = \{S, A, O, T, \varepsilon, r\}</script><ul><li>S 为状态空间</li><li>A 为动作空间</li><li>O 为观测空间</li><li>T 为状态转移算子</li><li>$\epsilon$ 为散发概率，在某状态下观测到某个现象的概率。 $p(o_t|s_t)$</li><li>r 为奖励函数 $r: S × A \rightarrow \mathbb{R}$</li></ul><h4 id="强化学习目标"><a href="#强化学习目标" class="headerlink" title="强化学习目标"></a>强化学习目标</h4><p>强化学习为一个与环境不断交互的过程，从初始状态出发，根据制定的策略选择一个动作进行执行，环境给出下一个到达的状态以及对于该状态和动作的奖励。</p><p>强化学习目标是制定一个策略，来最大化所有奖励之和的期望。</p><script type="math/tex;mode=display">\theta^*=arg\max_\theta E_{\tau\sim p_\theta(\tau)}[\sum_t r(s_t,a_t)]</script><p>一个完整的交互过程被称为 trajectory， episode， $\tau$, 完整地展开为一个序列$s_1, a_1, … , s_T, a_T$, 通过链式法则可以计算出观测到该轨迹的概率:</p><script type="math/tex;mode=display">p_\theta(\tau)=p_\theta(s_1, a_1, ..., s_T, a_T) = p(s_1) \prod_{t=1}^{T}\pi_\theta(a_t|s_t)p(s_{t+1}|s_t, a_t)</script><p>若将$s_t, a_t$视为一个整体，则马尔科夫决策链可以转化为马尔科夫链。</p><script type="math/tex;mode=display">p(s_{t+1}, a_{t+1} | s_t, a_t) = \pi_\theta(a_t|s_t) p(s_{t+1}|s_t,a_t)</script><p>故最大化奖励之和的期望可以进行转化</p><script type="math/tex;mode=display">\theta^*=arg\max_\theta \sum_{t=1}^T E_{(s_t,a_t)\sim p_\theta(s_t, a_t)}[r(s_t, a_t)]</script><p>当序列为无限时，则当$(s_t,a_t)达到稳态分布时来评估策略，此时有 $$\mu = \mathcal{T}\mu$ ，$\mu$ 是 $\mathcal{T}$ 的特征值为1对应的特征向量。</p><script type="math/tex;mode=display">\theta^*=arg\max_\theta E_{(s,a)\sim p_\theta(s, a)}[r(s, a)]</script><h3 id="强化学习算法结构"><a href="#强化学习算法结构" class="headerlink" title="强化学习算法结构"></a>强化学习算法结构</h3><p><img src="https://ww1.sinaimg.cn/large/006A69aEgy1g72qzb478ej30ek09vglu.jpg" alt="2019-09-17 19-33-51 的屏幕截图.png"></p><p>通常强化学习算法由3个部分组成</p><ul><li>运行策略进行采样，与环境交互收集数据。</li><li>分析收集到的数据，来提取将来发生的信息，估计收益。</li><li>优化策略。</li></ul><h4 id="基于策略梯度方法"><a href="#基于策略梯度方法" class="headerlink" title="基于策略梯度方法"></a>基于策略梯度方法</h4><ul><li>采样数据</li><li><p>计算收益 $J(\theta)$ = $E_\pi[\sum r_t] \approx \frac{1}{N}\sum_{i=1}^{N}\sum_tr_t^i$</p></li><li><p>优化策略 $\theta \leftarrow \theta + \alpha \bigtriangledown_\theta J(\theta)$</p></li></ul><h4 id="基于模型方法"><a href="#基于模型方法" class="headerlink" title="基于模型方法"></a>基于模型方法</h4><ul><li>采样数据</li><li>根据数据计算模型，$s_{t+1} = f_\phi(s_t, a_t)$, $r_t = g_\theta(s_t, a_t)$</li><li>反向传播优化策略，通过 $f_\phi$ 和 $r$ 计算出合适的 $\pi_\theta(s_t) =a_t$</li></ul><p><img src="https://ww1.sinaimg.cn/large/006A69aEly1gb2tuu7mnqj30z1083aan.jpg" alt="image.png"></p><p>Model-based 方法</p><ul><li>使用模型进行规划<ul><li>在连续空间中，使用最优控制等理论，进行反向传播优化得到动作。</li><li>在离散空间中，进行离散的规划，如蒙特卡洛树搜索。</li></ul></li><li>反向传播梯度到策略中<ul><li>需要一定的trick来保证实施。</li></ul></li><li>使用模型来学习值函数<ul><li>动态规划</li><li>产生模拟的经历，结合Model-Free进行学习，如Dyna</li></ul></li></ul><h3 id="值函数和Q函数"><a href="#值函数和Q函数" class="headerlink" title="值函数和Q函数"></a>值函数和Q函数</h3><script type="math/tex;mode=display">\sum_{t=1}^T E_{(s_t,a_t)\sim p_\theta(s_t, a_t)}[r(s_t, a_t)]</script><p>对收益的计算公式进行分解可以得到一个递归的条件期望表达式</p><script type="math/tex;mode=display">E_{s1\sim p(s1)}[E_{a1\sim\pi(a_1|s_1)}[r(s_1,a_1) + E_{s2\sim p(s2)}[E_{a_2\sim\pi(a_2|s_2)}[r(s_2,a_2) + ...|s2]|s_1,a_1]|s_1]]</script><p>将后面的一系列递归式子定义为Q.</p><script type="math/tex;mode=display">Q(S_1,a_1) = r(s_1,a_1) + E_{s_2\sim p(s_2 | s_1,a_1)}[E_{a_2\sim \pi(a_2|s_2)}[r(s_2,a_2) + ... | s_2] | s_1,a_1]</script><script type="math/tex;mode=display">E_{s_1 \sim p(s_1)}[E_{a_1\sim\pi(a_1|s_1)}|Q(s_1,a_1)|s_1]]</script><p>当$Q(s_1,a_1)$已知时，很容易修改$\pi(a_1|s_1)$来最大化收益。</p><h4 id="Q函数"><a href="#Q函数" class="headerlink" title="Q函数"></a>Q函数</h4><p>在$s_t$状态执行$a_t$开始期望得到的总收益。</p><script type="math/tex;mode=display">Q^\pi(s_t, a_t) = \sum_{t'=t}^T E_{\pi_\theta}[r(s_{t'}, a_{t'})|s_t,a_t]</script><h4 id="值函数"><a href="#值函数" class="headerlink" title="值函数"></a>值函数</h4><p>在$s_t$状态开始期望得到的总收益</p><script type="math/tex;mode=display">V^\pi(s_t) = \sum_{t'=t}^T E_{\pi_\theta}[r(s_{t', }, a_{t'})|s_t]</script><script type="math/tex;mode=display">V^\pi(s_t) = E_{a_t\sim\pi(a_t|s_t)}[Q^\pi(s_t,a_t)]</script><p>$s_1$ 状态的值函数即为强化学习的目标</p><script type="math/tex;mode=display">E_{s_1\sim p(s_1)}[V^\pi(s_1)]</script><h4 id="重要使用方法"><a href="#重要使用方法" class="headerlink" title="重要使用方法"></a>重要使用方法</h4><ul><li>给定策略$\pi$, 在得到 $Q^\pi(s, a)$后，可以优化策略。将每个状态的策略修改为Q值最大的动作，则该策略将不差于原策略。</li><li>使用Q函数来计算梯度，来提升较优动作的概率。如果$Q^\pi(s,a)&gt;V^\pi(s)$则该动作优于平均动作。</li></ul><h3 id="强化学习算法类型"><a href="#强化学习算法类型" class="headerlink" title="强化学习算法类型`"></a>强化学习算法类型`</h3><ul><li><p>policy gradients: 直接优化收益计算表达式。基于样本采样的近似，再计算策略表达式的梯度。</p><p><img src="https://ww1.sinaimg.cn/large/006A69aEgy1gb2uwl8l4aj30wi0eiq49.jpg" alt="image.png"></p></li><li><p>Value-based: 直接估计值函数和Q函数，通过神经网络来计算，使用argmax来优化策略。</p><p><img src="https://ww1.sinaimg.cn/large/006A69aEgy1gb2uvyxvelj30xr0fc0u0.jpg" alt="image.png"></p></li><li><p>Actor-critic: 两者的结合。在得到值函数和Q函数后，通过计算策略表达式的梯度来优化策略</p></li></ul><p><img src="https://ww1.sinaimg.cn/large/006A69aEgy1gb2uxbtqz6j30vm0f4jsl.jpg" alt="image.png"></p><ul><li><p>Model-based RL： 首先估计模型，再提升策略。方法包括使用模型进行规划, 反向传播梯度到策略，使用模型来学习价值函数，使用模型来模拟新的经历。</p><p><img src="https://ww1.sinaimg.cn/large/006A69aEgy1gb2uuupfnuj30ri0fm0tz.jpg" alt="image.png"></p></li></ul><h3 id="强化学习算法多样性原因"><a href="#强化学习算法多样性原因" class="headerlink" title="强化学习算法多样性原因"></a>强化学习算法多样性原因</h3><h4 id="不同的抉择"><a href="#不同的抉择" class="headerlink" title="不同的抉择"></a>不同的抉择</h4><ul><li>采样效率</li><li>稳定性，易用性。(收敛的概率，依赖人为调整超参数)</li></ul><h4 id="不同的前提假设"><a href="#不同的前提假设" class="headerlink" title="不同的前提假设"></a>不同的前提假设</h4><ul><li>随机或确定</li><li>连续或离散</li><li>片段或无限</li></ul><h4 id="不同的问题设定"><a href="#不同的问题设定" class="headerlink" title="不同的问题设定"></a>不同的问题设定</h4><ul><li>容易表示策略</li><li>容易表示环境</li></ul><h3 id="强化学习算法比较"><a href="#强化学习算法比较" class="headerlink" title="强化学习算法比较"></a>强化学习算法比较</h3><h4 id="样本效率"><a href="#样本效率" class="headerlink" title="样本效率"></a>样本效率</h4><p>样本效率指训练得到一个优秀的策略需要多少样本。</p><p>一个重要的抉择在于算法是否是off-policy</p><ul><li><p>off-policy: 可以通过样本来提升策略，而不需要当前策略产生新的样本。</p><blockquote><p>able to improve the policy without generating new samples from that policy.</p></blockquote></li><li><p>on-policy: 每当策略改变时，都需要使用当前策略产生新的样本。</p><blockquote><p>each time the policy is changed, even a little bit, we need to generate new samples.</p></blockquote></li></ul><p><img src="https://ww1.sinaimg.cn/large/006A69aEly1g73skv6974j30oy06i74n.jpg" alt="2019-09-18 17-20-12 的屏幕截图.png"></p><p>on-policy 策略的样本效率低，但可以通过并行算法来增加运行速率。</p><h4 id="稳定性"><a href="#稳定性" class="headerlink" title="稳定性"></a>稳定性</h4><p>稳定性指算法是否收敛，是否每一次都能收敛，收敛到什么值。</p><p>在监督学习中总是通过梯度下降来收敛到误差的较小值。</p><p>在强化学习中并不总是梯度下降。</p><ul><li>Value Function Fitting(Q-leaning): 采用的固定点迭代方法。在理想情况下最小化贝尔曼误差，但不一定保证能收敛。</li><li>Model-based RL: 模型并不是为了优化收益，更精确的模型不一定能带来收益的提高。</li><li>policy gradient： 采用了梯度下降，但是样本效率很低。</li></ul><h4 id="前提假设"><a href="#前提假设" class="headerlink" title="前提假设"></a>前提假设</h4><ul><li>全观测：价值函数拟合方法默认该假设，可以通过添加循环模块来解决。</li><li>片段学习：直接策略梯度方法和一些基于模型方法默认该假设。</li><li>连续和平滑：连续价值函数学习方法和一些基于模型方法默认该假设。</li></ul><h3 id="强化学习算法举例"><a href="#强化学习算法举例" class="headerlink" title="强化学习算法举例"></a>强化学习算法举例</h3><h4 id="价值函数拟合方法"><a href="#价值函数拟合方法" class="headerlink" title="价值函数拟合方法"></a>价值函数拟合方法</h4><ul><li>Q-learning, DQN</li><li>Temporal difference learning</li><li>Fitted value iteration</li></ul><h4 id="策略梯度方法"><a href="#策略梯度方法" class="headerlink" title="策略梯度方法"></a>策略梯度方法</h4><ul><li>REINFORCE</li><li>Natural policy gradient</li></ul><h4 id="演员表演家方法"><a href="#演员表演家方法" class="headerlink" title="演员表演家方法"></a>演员表演家方法</h4><ul><li>Asynchronous Advantage Actor-Critic</li><li>Soft Actor-Critic</li></ul><h4 id="基于模型方法-1"><a href="#基于模型方法-1" class="headerlink" title="基于模型方法"></a>基于模型方法</h4><ul><li>Dyna</li><li>Guided policy search</li></ul></div><footer class="post-footer"><div class="post-tags"><a href="/tags/RL/" rel="tag"># RL</a></div><div class="post-nav"><div class="post-nav-item"><a href="/2019/09/15/CS294-002-监督学习和模仿学习/" rel="prev" title="CS294-002-监督学习和模仿学习"><i class="fa fa-chevron-left"></i> CS294-002-监督学习和模仿学习</a></div><div class="post-nav-item"><a href="/2019/09/18/CS294-005-策略梯度简介/" rel="next" title="CS294-005-策略梯度简介">CS294-005-策略梯度简介 <i class="fa fa-chevron-right"></i></a></div></div></footer></article></div></div><script>window.addEventListener('tabs:register', () => {
    let activeClass = CONFIG.comments.activeClass;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc">文章目录</li><li class="sidebar-nav-overview">站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#定义"><span class="nav-number">1.</span> <span class="nav-text">定义</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#符号"><span class="nav-number">1.1.</span> <span class="nav-text">符号</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#模仿学习"><span class="nav-number">1.2.</span> <span class="nav-text">模仿学习</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#奖励函数"><span class="nav-number">1.3.</span> <span class="nav-text">奖励函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#马尔科夫链"><span class="nav-number">1.4.</span> <span class="nav-text">马尔科夫链</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#马尔科夫链-1"><span class="nav-number">1.4.1.</span> <span class="nav-text">马尔科夫链</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#马尔科夫决策链-MDP"><span class="nav-number">1.4.2.</span> <span class="nav-text">马尔科夫决策链 MDP</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#部分观测马尔科夫决策链-POMDP"><span class="nav-number">1.4.3.</span> <span class="nav-text">部分观测马尔科夫决策链 POMDP</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#强化学习目标"><span class="nav-number">1.5.</span> <span class="nav-text">强化学习目标</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#强化学习算法结构"><span class="nav-number">2.</span> <span class="nav-text">强化学习算法结构</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#基于策略梯度方法"><span class="nav-number">2.1.</span> <span class="nav-text">基于策略梯度方法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#基于模型方法"><span class="nav-number">2.2.</span> <span class="nav-text">基于模型方法</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#值函数和Q函数"><span class="nav-number">3.</span> <span class="nav-text">值函数和Q函数</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Q函数"><span class="nav-number">3.1.</span> <span class="nav-text">Q函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#值函数"><span class="nav-number">3.2.</span> <span class="nav-text">值函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#重要使用方法"><span class="nav-number">3.3.</span> <span class="nav-text">重要使用方法</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#强化学习算法类型"><span class="nav-number">4.</span> <span class="nav-text">强化学习算法类型`</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#强化学习算法多样性原因"><span class="nav-number">5.</span> <span class="nav-text">强化学习算法多样性原因</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#不同的抉择"><span class="nav-number">5.1.</span> <span class="nav-text">不同的抉择</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#不同的前提假设"><span class="nav-number">5.2.</span> <span class="nav-text">不同的前提假设</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#不同的问题设定"><span class="nav-number">5.3.</span> <span class="nav-text">不同的问题设定</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#强化学习算法比较"><span class="nav-number">6.</span> <span class="nav-text">强化学习算法比较</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#样本效率"><span class="nav-number">6.1.</span> <span class="nav-text">样本效率</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#稳定性"><span class="nav-number">6.2.</span> <span class="nav-text">稳定性</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#前提假设"><span class="nav-number">6.3.</span> <span class="nav-text">前提假设</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#强化学习算法举例"><span class="nav-number">7.</span> <span class="nav-text">强化学习算法举例</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#价值函数拟合方法"><span class="nav-number">7.1.</span> <span class="nav-text">价值函数拟合方法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#策略梯度方法"><span class="nav-number">7.2.</span> <span class="nav-text">策略梯度方法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#演员表演家方法"><span class="nav-number">7.3.</span> <span class="nav-text">演员表演家方法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#基于模型方法-1"><span class="nav-number">7.4.</span> <span class="nav-text">基于模型方法</span></a></li></ol></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><p class="site-author-name" itemprop="name">Xu Hang</p><div class="site-description" itemprop="description"></div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">19</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">6</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/"><span class="site-state-item-count">5</span> <span class="site-state-item-name">标签</span></a></div></nav></div></div><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i> <span>0%</span></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2020</span> <span class="with-love"><i class="fa fa-user"></i> </span><span class="author" itemprop="copyrightHolder">Xu Hang</span></div><div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v3.9.0</div><span class="post-meta-divider">|</span><div class="theme-info">主题 – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> v7.7.0</div></div></footer></div><script src="/lib/anime.min.js"></script><script src="/js/utils.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script src="/js/local-search.js"></script><script type="text/x-mathjax-config">MathJax.Ajax.config.path['mhchem'] = '//cdn.jsdelivr.net/npm/mathjax-mhchem@3';

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
        extensions: ['[mhchem]/mhchem.js'],
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });</script><script>NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);</script></body></html><!-- rebuild by neat -->