<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>CS294-001-课程综述</title>
      <link href="/2020/01/19/CS294-001-%E8%AF%BE%E7%A8%8B%E7%BB%BC%E8%BF%B0/"/>
      <url>/2020/01/19/CS294-001-%E8%AF%BE%E7%A8%8B%E7%BB%BC%E8%BF%B0/</url>
      
        <content type="html"><![CDATA[<!-- build time:Sun Jan 19 2020 21:27:23 GMT+0800 (GMT+08:00) --><h3 id="深度强化学习"><a href="#深度强化学习" class="headerlink" title="深度强化学习"></a>深度强化学习</h3><ul><li>深度学习: 提供了一种处理复杂无结构环境的方式。</li><li>强化学习: 提供了一种进行决策的数学框架。</li><li>深度强化学习：端到端的连续决策。</li></ul><h3 id="监督信号"><a href="#监督信号" class="headerlink" title="监督信号"></a>监督信号</h3><ul><li>从奖励学习<ul><li>Basic RL的目标是最大化奖励。</li></ul></li><li>从实例学习<ul><li>Behavior Cloning: 直接复制模仿观测到的行为。</li><li>Inverse RL: 从观测行为中推测奖励函数。</li></ul></li><li>通过观测世界来学习<ul><li>学习预测当前的动作会带来的结果。</li><li>无监督学习</li></ul></li><li>从其他任务中学习<ul><li>Transfer Learning: 将多个领域的知识融合起来。</li><li>Meta-leraning: 学习如何进行学习。</li></ul></li></ul><h3 id="通用人工智能"><a href="#通用人工智能" class="headerlink" title="通用人工智能"></a>通用人工智能</h3><blockquote><p>与其模拟人类大脑，不如从零开始学习。</p></blockquote><p><img src="http://ww1.sinaimg.cn/large/006A69aEgy1gb1zvx44igj31240eg11z.jpg" alt="image.png"></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> RL(CS294) </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>服务器搭建zerotier以通过外网访问，并在VPS搭建moon节点加速连接</title>
      <link href="/2020/01/18/%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%90%AD%E5%BB%BAzerotier%E4%BB%A5%E9%80%9A%E8%BF%87%E5%A4%96%E7%BD%91%E8%AE%BF%E9%97%AE%EF%BC%8C%E5%B9%B6%E5%9C%A8VPS%E6%90%AD%E5%BB%BAmoon%E8%8A%82%E7%82%B9%E5%8A%A0%E9%80%9F%E8%BF%9E%E6%8E%A5/"/>
      <url>/2020/01/18/%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%90%AD%E5%BB%BAzerotier%E4%BB%A5%E9%80%9A%E8%BF%87%E5%A4%96%E7%BD%91%E8%AE%BF%E9%97%AE%EF%BC%8C%E5%B9%B6%E5%9C%A8VPS%E6%90%AD%E5%BB%BAmoon%E8%8A%82%E7%82%B9%E5%8A%A0%E9%80%9F%E8%BF%9E%E6%8E%A5/</url>
      
        <content type="html"><![CDATA[<!-- build time:Sun Jan 19 2020 18:07:22 GMT+0800 (GMT+08:00) --><h2 id="Zerotier"><a href="#Zerotier" class="headerlink" title="Zerotier"></a>Zerotier</h2><p>Zerotier利用服务器创建了一个虚拟局域网，所有连接到虚拟局域网的服务器被分配到了一个IP，利用这个IP即可进行点对点的访问。利用Zerotier，即可在没用公网IP的情况下，在外网访问服务器。</p><h3 id="注册Zerotier并创建虚拟局域网"><a href="#注册Zerotier并创建虚拟局域网" class="headerlink" title="注册Zerotier并创建虚拟局域网"></a>注册Zerotier并创建虚拟局域网</h3><p><a href="https://xuhang.ink/2019/12/05/%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%90%AD%E5%BB%BAzerotier%E4%BB%A5%E9%80%9A%E8%BF%87%E5%A4%96%E7%BD%91%E8%AE%BF%E9%97%AE%EF%BC%8C%E5%B9%B6%E5%9C%A8VPS%E6%90%AD%E5%BB%BAmoon%E8%8A%82%E7%82%B9%E5%8A%A0%E9%80%9F%E8%BF%9E%E6%8E%A5/my.zerotier.com" target="_blank" rel="noopener">Zerotier官网地址</a>，利用邮箱注册用户并登录。</p><p>在Networks选项页中单击 “Create a Network”，即可在下方看到创建成功的网络，并会产生1个16位唯一代表这个网络，之后在加入网络时需要用到这个ID。</p><p>在网络的设置中有一项Access Control，为了安全起见，设置为PRIVATE，任何加入网络的节点，需要允许才可以访问。</p><h3 id="Ubuntu-配置-Zerotier"><a href="#Ubuntu-配置-Zerotier" class="headerlink" title="Ubuntu 配置 Zerotier"></a>Ubuntu 配置 Zerotier</h3><h4 id="安装-Zerotier"><a href="#安装-Zerotier" class="headerlink" title="安装 Zerotier"></a>安装 Zerotier</h4><blockquote><p>curl -s <a href="https://install.zerotier.com/" target="_blank" rel="noopener">https://install.zerotier.com/</a> | sudo bash</p></blockquote><h4 id="加入虚拟局域网"><a href="#加入虚拟局域网" class="headerlink" title="加入虚拟局域网"></a>加入虚拟局域网</h4><blockquote><p>sudo zerotier-cli join (network id)</p></blockquote><p>在加入虚拟网后，可以在Zerotier的网络设置页面的Member项中看到连接信息，在左侧打钩以允许访问，其中Managed IPs即为分配到的虚拟局域网中的内网IP。</p><h4 id="查看虚拟局域网情况"><a href="#查看虚拟局域网情况" class="headerlink" title="查看虚拟局域网情况"></a>查看虚拟局域网情况</h4><blockquote><p>sudo zerotier-cli listpeers</p></blockquote><p>其中Leaf是每一个连接节点，Planet是官方设置的转发根服务器，Moon为用户设置的转发服务器。</p><h3 id="VPS-搭建-Moon服务器加速连接"><a href="#VPS-搭建-Moon服务器加速连接" class="headerlink" title="VPS 搭建 Moon服务器加速连接"></a>VPS 搭建 Moon服务器加速连接</h3><p>官方设置的转发根服务器速度较慢且不稳定，可以搭建自己的Moon转发服务器来加速访问。</p><p>首先需要注册一个VPS，阿里云的服务器或者其他都可以。</p><h4 id="安装Zerotier并加入虚拟局域网"><a href="#安装Zerotier并加入虚拟局域网" class="headerlink" title="安装Zerotier并加入虚拟局域网"></a>安装Zerotier并加入虚拟局域网</h4><p>与之前相同，输入命令在VPS上安装Zerotier并加入虚拟局域网，然后在配置页面中通过。</p><h4 id="生成moon模板"><a href="#生成moon模板" class="headerlink" title="生成moon模板"></a>生成moon模板</h4><blockquote><p>cd /var/lib/zerotier-one<br>zerotier-idtool initmoon identity.public &gt; moon.json</p></blockquote><h4 id="修改moon-json"><a href="#修改moon-json" class="headerlink" title="修改moon.json"></a><strong>修改moon.json</strong></h4><p>编辑 moon.json，将 stableEndpoints 改为 “服务器IP/9993”</p><blockquote><p>“stableEndpoints”: [ “x.x.x.x/9993” ]</p></blockquote><h4 id="生成签名文件"><a href="#生成签名文件" class="headerlink" title="生成签名文件"></a>生成签名文件</h4><blockquote><p>zerotier-idtool genmoon moon.json</p></blockquote><p>之后可以看到生成了 000000xxxx.moon 文件。</p><h4 id="创建moon网络"><a href="#创建moon网络" class="headerlink" title="创建moon网络"></a>创建moon网络</h4><p>在当前目录下创建 moons.d文件，并将 .moon文件拷贝进去</p><blockquote><p>mkdir moons.d</p><p>cp *.moon moons.d</p></blockquote><h4 id="重启服务器"><a href="#重启服务器" class="headerlink" title="重启服务器"></a>重启服务器</h4><h3 id="本机加入moon节点"><a href="#本机加入moon节点" class="headerlink" title="本机加入moon节点"></a>本机加入moon节点</h3><p>在 Zerotier 配置页面中可以看到VPS对应的 10位 Address ID。</p><p>在终端中输入</p><blockquote><p>sudo zerotier-cli orbit (VPSid) (VPSid)</p></blockquote><p>在等待一段时间后，再次查看局域网情况，即可看到增加一条，ID为VPS的ID，最后为 Moon</p><h3 id="连接远程服务器"><a href="#连接远程服务器" class="headerlink" title="连接远程服务器"></a>连接远程服务器</h3><p>远程服务器的配置和本机相同，配置完后利用生成的IP即可进行连接。</p><blockquote><p>curl -s <a href="https://install.zerotier.com/" target="_blank" rel="noopener">https://install.zerotier.com/</a> | sudo bash</p><p>sudo zerotier-cli join (network id)</p><p>sudo zerotier-cli orbit (VPSid) (VPSid)</p></blockquote><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p><a href="https://www.lingbaoboy.com/2019/03/vpszerotiermoon.html" target="_blank" rel="noopener">在vps上搭建Zerotier的Moon节点</a></p><p><a href="https://blog.whsir.com/post-3685.html" target="_blank" rel="noopener">无公网IP通过ZeroTier方便实现内网穿透</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Server </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Server </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[论文阅读]DeepStack: Expert-Level Artificial Intelligence in Heads-Up No-Limit Poker</title>
      <link href="/2020/01/18/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-DeepStack-Expert-Level-Artificial-Intelligence-in-Heads-Up-No-Limit-Poker/"/>
      <url>/2020/01/18/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-DeepStack-Expert-Level-Artificial-Intelligence-in-Heads-Up-No-Limit-Poker/</url>
      
        <content type="html"><![CDATA[<!-- build time:Sun Jan 19 2020 18:07:22 GMT+0800 (GMT+08:00) --><div class="table-container"><table><thead><tr><th>标题</th><th>DeepStack: Expert-Level Artificial Intelligence in Heads-Up No-Limit Poker</th></tr></thead><tbody><tr><td>团队</td><td>Alberta</td></tr><tr><td>时间</td><td>2017</td></tr><tr><td>出处</td><td>Science</td></tr></tbody></table></div><h2 id="论文梗概"><a href="#论文梗概" class="headerlink" title="论文梗概"></a>论文梗概</h2><p>Deepstack论文于2017年发表在《science》杂志上，主要成果是训练出了达到专家水平的无限注双人德州扑克AI，在举办的比赛中打败了人类专家选手。Deepstack使用的方法是基于CFR算法进行递归思考，处理信息不对称，结合CFR-D算法使用分解来进行局部的计算，避免对完整的博弈树进行遍历，融合深度学习的方法，对一个阶段内的反事实后悔值进行拟合，称之为一种直觉。</p><h2 id="Deep-Conterfactual-Value-Network"><a href="#Deep-Conterfactual-Value-Network" class="headerlink" title="Deep Conterfactual Value Network"></a>Deep Conterfactual Value Network</h2><p>Deepstack将博弈树按照德州扑克的四个轮次，划分成了四个阶段，分别称为 pre-flop, flop, turn, river。在flop和turn阶段分别训练了一个神经网络，用了预测该阶段每颗子树的根节点CFV值，在之后每次迭代到根节点时直接使用神经网络进行预测，避免迭代全部博弈树，节省了时间。</p><p>除了flop-network和turn-network外，还针对pre-flop的最后一个阶段训练了一个辅助网络。</p><p>由于pre-flop的最后一个阶段是发牌阶段，需要发3张公共牌，可能的情况数很多，针对与具体的每一种情况，还需要调用flop-network来计算CFV值，计算量很大。辅助网络对这种情况进行优化计算，对于每一个发牌节点，直接输出加权之后的CFV值。</p><h3 id="网络输入"><a href="#网络输入" class="headerlink" title="网络输入"></a>网络输入</h3><p>神经网络的原始输入为三部分：</p><ul><li>双方玩家的总加注额与加注上限的比例</li><li>双方玩家的隐藏手牌概率分布(range)，即持有每一种手牌的概率</li><li>牌桌公共牌</li></ul><p>其中range和总加注额可以代表加注序列。</p><p>给定了隐藏手牌和公共牌后，可以组合出最强的一手牌，从而可以得到目前最强手牌的概率分布。将所有最强手牌根据牌力特征使用 k 均值进行聚类，最后得到 1000种类别的概率分布。</p><p>网络的实际输入为两部分</p><ul><li>双方玩家的总加注额与加注上限的比例</li><li>聚类之后的每种类别的概率分布</li></ul><p>由于辅助网络只有169种策略上不同的手牌，对于辅助网络不进行聚类，直接使用最强手牌组合作为输入。</p><h3 id="网络输出"><a href="#网络输出" class="headerlink" title="网络输出"></a>网络输出</h3><p>神经网络的输出为每个玩家每一种可能手牌对应的CFV值。</p><h3 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h3><p><img src="https://ww1.sinaimg.cn/large/006A69aEly1g8qwhnjzg2j30nh0ebtbo.jpg" alt="image.png"></p><p>神经网络在输入层之后，添加了7个PReLU激活的全连接线性层，得到了聚类之后每一种手牌的价值。之后有添加了一个零和神经网络，使得到到达概率和手牌价值的加权乘积为0。最后输出得到的聚类之后的每一种手牌的CFV值，映射到聚类之前的每种手牌的CFV值。</p><h3 id="网络训练"><a href="#网络训练" class="headerlink" title="网络训练"></a>网络训练</h3><p>所有的神经网络使用Torch7编写，使用Adam随机梯度下降来最小化平均Huber损失。训练采用 1000 mini-batch和 0.001 学习率，在200次epoch后减少到0.001、网络训练了350次epoch，在一个GPU上训练了2天，最后挑选出最少的验证集loss对应的epoch。</p><h3 id="数据生成"><a href="#数据生成" class="headerlink" title="数据生成"></a>数据生成</h3><p>网络输入的总加注额在一个固定分布中进行采样，持有手牌概率使用特定的随机生成算法进行生成，目的是覆盖实际re-solving使用时的各种持有手牌概率。</p><p>随机生成算法为一个迭代的过程。假设某一次处理的手牌集合为 SS , 总可能概率为 pp。若集合大小为1，则持有改手牌的概率为 $p$, 否则将集合分成两半$S1,S2$，在$0−p$ 之间生成一个随机数 p1p1, 令 $p_1=p−p_2$, 分别以$p_1,p_2$ 处理手牌集合 $S_1,S_2$。</p><p>在得到总加注额和持有手牌概率后，限定动作为fold，call，固定额bet，all-in，进行1000次的CFR+迭代，得到网络的输出CFV值。</p><p>Turn神经网络产生了一千万个训练数据，在6144个CPU核心下总计使用175核心年的计算时间进行solve。</p><p>Flop神经网络产生了一百万个训练数据，对CFR迭代进行了深度限定，结合Turn神经网络进行预测，在20个GPU下使用了半个GPU年。</p><p>辅助神经网络产生了一千万个训练数据，最后的输出值通过遍历22100种可能的发牌情况，结合Flop神经网络进行预测再计算平均值，作为网络的输出数据。</p><h3 id="网络训练精度"><a href="#网络训练精度" class="headerlink" title="网络训练精度"></a>网络训练精度</h3><div class="table-container"><table><thead><tr><th></th><th>训练集Huber loss</th><th>验证集Huber loss</th></tr></thead><tbody><tr><td>Turn神经网络</td><td>0.016</td><td>0.026</td></tr><tr><td>Flop神经网络</td><td>0.008</td><td>0.034</td></tr><tr><td>辅助神经网络</td><td>0.000053</td><td>0.000055</td></tr></tbody></table></div><h3 id="隐藏层数量"><a href="#隐藏层数量" class="headerlink" title="隐藏层数量"></a>隐藏层数量</h3><p><img src="https://ww1.sinaimg.cn/large/006A69aEly1g8t1sol48zj30fg0ahjrp.jpg" alt="image.png"></p><p>根据隐藏层数量和Huber loss进行了实验，最后在验证集误差，GPU内存，执行速度之间进行权衡，选择了7个隐藏层数量。可以预见，如果训练数据继续增大，可以选择更大的神经网络来增加网络的拟合能力。</p><h2 id="Continual-re-solving"><a href="#Continual-re-solving" class="headerlink" title="Continual re-solving"></a>Continual re-solving</h2><p>re-solving 是指对一棵子树进行迭代求出纳什均衡策略，不需要对整棵博弈树进行求解。进行re-solving的前提是需要知道到达该棵子树时，自己的range即各种手牌的概率和对方的反事实后悔值。</p><p>Continual re-solving是指不断地进行re-solving，每当需要进行决策时，进行一次re-solving，迭代出最优的策略并相应地选择动作，执行完动作后将策略抛弃不再存储。</p><h3 id="Our-range-and-Opponent-CFV"><a href="#Our-range-and-Opponent-CFV" class="headerlink" title="Our range and Opponent CFV"></a>Our range and Opponent CFV</h3><p>range 和 CFV 在存储时都是一个长度为1326的向量，表示所有私有手牌的情况下对应的值。</p><p>在游戏开始时，range为均匀分布，CFV初始化为发到每一种私有手牌时对应的价值。</p><p>根据进行决策的玩家不同，分为三种情况进行处理。</p><ul><li>自己。在求解出策略后，将range和相应动作的策略值相乘并归一化，CFV值保存为对应动作在CFR迭代时得到的CFV值。</li><li>对手。不进行任何处理。</li><li>发牌。根据不能有两张相同的手牌，将range中与公共牌相同的牌的概率变为0再归一化，CFV值为上一次求解策略时进行CFR迭代得到。</li></ul><h3 id="Opponent-Ranges-in-Re-Solving"><a href="#Opponent-Ranges-in-Re-Solving" class="headerlink" title="Opponent Ranges in Re-Solving"></a>Opponent Ranges in Re-Solving</h3><p>不保存对手的Range，而是在Re-Solving时借助Opponent CFV的限制迭代出来。与CFR-D类似，每次进行re-solve前新建一个对手决策的节点，一个决策是到达T节点，收益为Opponent CFV，另一个决策时到达F节点，即该棵子游戏的根。</p><p>对于对手Range的一个合理初始化，可以加快迭代的速度。一种方法是以权重b使用前一次计算出来的range，权重1-b使用均匀分布。另一种可能损失精度的方法时以权重b对前一次计算出来的range进行抽样，以该手牌不进行TF决策直接进行计算，以权重1-b使用均匀分布。</p><p>Deepstack只对每一轮的第一个动作使用了评估的对手Range初始化。当对手是第一个做动作时，使用了保守的方法，当自己是第一个做动作时，使用了可能损失精度的方法。b定为0.9。</p><h3 id="Sparse-Lookahead-Trees"><a href="#Sparse-Lookahead-Trees" class="headerlink" title="Sparse Lookahead Trees"></a>Sparse Lookahead Trees</h3><p>直接进行re-solving需要迭代到整棵子树的末尾，仍然需要消耗大量的时间。在每一轮结束时使用训练好的神经网络来拟合CFV值，限制了树的深度。对进行re-solving的动作进行限定，限制了树的宽度。进行re-solving的动作限制为fold， call，2-3个下注额和all-in。</p><p>通过深度和宽度的限制，re-solving的子树缩减到了107107个决策点，可以在一块GTX1080显卡的计算下使用少于5秒的时间进行求解。</p><p>对于子树的求解使用的是朴素CFR和CFR+的混合。使用了regret matching+，以及在计算平均策略和平均CFV时，忽略掉了CFR的前几次迭代。</p><h4 id="具体参数配置"><a href="#具体参数配置" class="headerlink" title="具体参数配置"></a>具体参数配置</h4><p><img src="https://ww1.sinaimg.cn/large/006A69aEly1g8twk943gjj30s406ht9j.jpg" alt="image.png"></p><p>在pre-flop阶段中，在前几次忽略的CFR迭代中，使用辅助神经网络预测最后的CFV值，之后的CFR迭代中借助flop神经网络直接计算CFV值。此外对于re-solving结果进行保存，当同样的下注序列出现时，重新使用缓存结果而不是重新计算。</p><p>在flop阶段中，借助turn神经网络预测最后的CFV值。</p><p>在turn阶段中，不适用神经网络进行预测，直接求解整棵子树，对于river阶段的动作进行了聚类抽象。</p><p>在river阶段中，直接求解整棵子树。</p><h4 id="Actions-in-sparse-lookahead-tree"><a href="#Actions-in-sparse-lookahead-tree" class="headerlink" title="Actions in sparse lookahead tree"></a>Actions in sparse lookahead tree</h4><p><img src="https://ww1.sinaimg.cn/large/006A69aEly1g8twgnm311j30s809ljt0.jpg" alt="image.png"></p><p>对不同的动作限定对第一层的CFV的影响进行了实验，以限定9个动作和4000次迭代作为基准进行比较。最后比较发现F，C，P，A兼顾了博弈树大小和最后的评估精度。</p><p><img src="https://ww1.sinaimg.cn/large/006A69aEly1g8twog2wk5j30el04yjrp.jpg" alt="image.png"></p><p>接着又使用LBR方法对于不同的抽象动作进行了比较，Default为Deepstack版本，结果显示Deepstack的策略很难被利用，且增加抽象动作对结果的影响较小。</p><h2 id="Speed-of-Play"><a href="#Speed-of-Play" class="headerlink" title="Speed of Play"></a>Speed of Play</h2><p><img src="https://ww1.sinaimg.cn/large/006A69aEly1g8twqtan5gj30ff09vaay.jpg" alt="image.png"></p><h2 id="Evaluating"><a href="#Evaluating" class="headerlink" title="Evaluating"></a>Evaluating</h2><p>整体求解的目标是逼近纳什均衡策略，即得到一个策略具有较低的可利用性。一般评价的方法有两种。</p><ul><li>对打。对打为可利用性的比较差的估计，对打成绩高并不能说明可利用性低，比如石头剪刀布中的最优策略和一直出石头策略对打，最后的结局时平局。</li><li>直接求可利用性。利用最佳响应直接求可利用性，需要对整棵博弈树进行一次完整的遍历，在对在双人无限注德州扑克中很难做到。</li></ul><h3 id="与人类玩家对打"><a href="#与人类玩家对打" class="headerlink" title="与人类玩家对打"></a>与人类玩家对打</h3><p>由于扑克牌的随机性和策略的随机性，评价策略的好坏具有很大的方差，往往需要大量的对打来得到一个稳定的结果。Deepstack使用了 AIVAT 的方法进行评价，该方法基于精心设计的结构化控制变量，可以证明在不完全信息游戏评价中进行无偏低方差的估计。AIVAT 需要在每个公共状态下获取一个拥有各种手牌的评估值，根据发牌节点和已知策略玩家选择动作的变化而导致估计值的变化，来计算控制变量。通过 AIVAT 方法获得了 85% 标准差的下降，可以在 3000局的游戏中得到统计学上的显著差异。</p><p><img src="https://ww1.sinaimg.cn/large/006A69aEly1g8u3zp10sej30kf0eeq3o.jpg" alt="image.png"></p><h3 id="Local-Best-Response"><a href="#Local-Best-Response" class="headerlink" title="Local Best Response"></a>Local Best Response</h3><p>另一种新提出来的方法为局部最佳相应(LBR)，可以找到可利用性的一个下界。LBR在每一次决策中固定了可以选择的动作。该方法可以有效地利用以前提出的基于抽象的方法。</p><p><img src="https://ww1.sinaimg.cn/large/006A69aEly1g8u3ex9821j30pc0bodht.jpg" alt="image.png"></p><p>可以看出与之前的方法相比，DeepStack很难被利用。</p><h2 id="Continual-re-solving-伪代码"><a href="#Continual-re-solving-伪代码" class="headerlink" title="Continual re-solving 伪代码"></a>Continual re-solving 伪代码</h2><p><img src="https://ww1.sinaimg.cn/large/006A69aEgy1g8u423fiodj30sa042mxx.jpg" alt="image.png"></p><p><img src="https://ww1.sinaimg.cn/large/006A69aEgy1g8u42aqeg3j30rm0df770.jpg" alt="image.png"></p><p><img src="https://ww1.sinaimg.cn/large/006A69aEgy1g8u42se03bj30se0gs422.jpg" alt="image.png"></p><p><img src="https://ww1.sinaimg.cn/large/006A69aEgy1g8u438j0omj30s30a5763.jpg" alt="image.png"></p><p><img src="https://ww1.sinaimg.cn/large/006A69aEgy1g8u43huyuij30ry08qtag.jpg" alt="image.png"></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RL </tag>
            
            <tag> Paper </tag>
            
            <tag> CFR </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[论文阅读]Deep Counterfactual Regret Minimization</title>
      <link href="/2020/01/18/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Deep-Counterfactual-Regret-Minimization/"/>
      <url>/2020/01/18/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Deep-Counterfactual-Regret-Minimization/</url>
      
        <content type="html"><![CDATA[<!-- build time:Sun Jan 19 2020 18:07:22 GMT+0800 (GMT+08:00) --><div class="table-container"><table><thead><tr><th>标题</th><th>Deep Counterfactual Regret Minimization</th></tr></thead><tbody><tr><td>团队</td><td>CMU、Facebook</td></tr><tr><td>时间</td><td>2019</td></tr><tr><td>出处</td><td>ICML</td></tr></tbody></table></div><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>朴素的CFR算法需要遍历整颗博弈树，而在大型游戏中，往往需要在使用CFR之前进行对游戏进行抽象，抽象的方法与具体领域有关，且会损失一些重要信息，一个好的抽象方法又与游戏的均衡策略有关。该论文避免对游戏进行抽象，使用深度神经网来拟合CFR在游戏中的表现。</p><p>具体而言，该论文使用一个价值神经网络来拟合每个信息集的动作反事实后悔值，一个策略神经网络来拟合最后的平均策略。在每次迭代中，首先使用k次外部采样，使用前一次的神经网络来采样对手的策略，获得自己的反事实后悔值存放在经验回放池中，采样完后训练一个新的神经网络来拟合所有的动作反事实后悔值。在每次采样对手的策略时，又将策略保存在另一个经验回放池中，用来训练最后的策略神经网络。</p><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>不完全信息博弈中，期望找到近似的均衡解，任何人不能单方面改变策略来获取更高的收益，即任何人的策略是对于他人的最佳应对策略。</p><p>CFR算法通过迭代在双人零和游戏中收敛到纳什均衡解。大型游戏中首先对游戏进行抽样，抽样方法依赖于具体的领域知识，之后再用表格式的CFR算法进行求解，由于抽样的存在，最后的解是对均衡解的粗糙逼近。</p><p>在强化学习中引入深度神经网络取得了较好效果，不需要具体领域知识就可以学习到较好的策略，但在不完全信息中很难收敛到均衡解。</p><p>NFSP(Neural Fiction Self Play, 2016)在之前在不完全游戏中的前沿方法。</p><p>DeepCFR使用深度神经网络拟合函数来逼近表格式方法，证明了能收敛到 ϵϵ 纳什均衡，在扑克变体包括双人有限注德州扑克进行实验。</p><h3 id="Notation-And-Background"><a href="#Notation-And-Background" class="headerlink" title="Notation And Background"></a>Notation And Background</h3><p>限定游戏在双人零和博弈下, $P=\{1,2\}$, $u1=−u2P=\{1,2\}$, $u1=−u2$</p><p>限定游戏为完全回忆，即如何两个节点不在同一个信息集下，这两个节点的后驱节点也互相不再同一个信息集下。</p><p>在Regret Matching中，在正后悔值求和大于0时按比例分配概率，等于0时均匀分配概率。在该论文中在等于0时固定选择反悔值最高的动作，实验发现这能更好的处理逼近误差。</p><p>实验发现交替累积玩家后悔值并计算策略，要比同时累积玩家后悔值并计算策略，能更快地收敛。</p><h3 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h3><ul><li>First-order methods(2010, 2018)，另一种收敛到纳什均衡的方法，界限优于CFR，但实际使用中CFR变体更快，更稳定，可能更容易和函数逼近方法结合。</li><li>Neural Fictious Self Play(2016)，将Fictitious Play和深度学习函数结合，收敛速度慢于CFR。</li><li>Model-free policy gradient(2019)，调整参数来最小化后悔值，达到和NFSP类似性能。</li><li>DeepStack等(2017, 2018)，在深度限制的子游戏中使用深度学习来评估价值，但在子游戏中使用表格式方法计算价值。</li><li>大规模函数逼近CFR在单一智能体设定下使用。该论文提出的是多智能体设定，且方法具有很大不同。</li><li>Regression CFR(2015)，建立回归树函数逼近CFR，手工设定大量特征，再利用线性回归逼近后悔值，手工设定特征和抽象一样依赖于领域知识。</li><li>Double Neural CFR(2018)，理论上有缺陷，只考虑小游戏。</li></ul><h3 id="Description"><a href="#Description" class="headerlink" title="Description"></a>Description</h3><p>DeepCFR的目标时逼近CFR的行为，不在每个信息集上计算和累积后悔值，而是使用深度神经网络来函数逼近，由相似的信息集进行推广。</p><p>DeepCFR在每次迭代 tt 中，根据外部采样进行 KK 次博弈树的部分遍历。在遇到每个信息集时，使用 $\theta^{t-1}$定义的价值神经网络来输出每个动作的总反事实后悔值，进而得到选择每个动作的概率，即策略 $\sigma^t$。神经网络的输出 $V(I,a\vert \theta^{t-1})$ 近似为总反事实后悔值 $R^{t−1}(I,a)$。</p><p>当遍历到叶子节点时，返回其收益。反向传播到对手和机会节点时，原样返回。反向传播到自己节点时，将所有动作的收益按照计算出来的策略 $\sigma^t$进行加权求和，同时也可以求出此次采样下每个动作的立即后悔值，将这些后悔值放入经验回放池 $M_{v,p}$中作为之后神经网络的训练样本，如果满了则使用鱼塘采样，保证每个样本被选取的概率随机。</p><p>当 $K$ 次采样完成后，使用经验回放池 $M_{v,p}$ 作为训练数据，重新训练一个新的 θtθt 定义的价值神经网络，最小化输出和立即反事实动作后悔值之间的差距。由于所有立即反事实动作后悔值的平均值与总反事实动作后悔值成比例，所以经验回放池中的数据除了鱼塘采样外不会被丢弃。</p><p>可以使用其他满足 Bregman Divergence 的损失函数。</p><p>除了价值网络外，使用一个单独的策略网络来拟合最后的平均策略，因为平均策略才收敛到纳什均衡。再维护一个经验回放池 $M_\Pi$，每次迭代中将计算得到的策略加入经验回放池，并使用迭代轮数 $t$ 进行加权。</p><p>如果迭代次数和神经网络的大小很小，则可以把每一次的价值网络保存下来。在实际每次需要进行决策时，随机挑选一个价值网络输出策略。</p><h3 id="Experimental-Setup"><a href="#Experimental-Setup" class="headerlink" title="Experimental Setup"></a>Experimental Setup</h3><p>在有限注两轮德州扑克中进行实验，具有 $10^{12}$个节点，$10^9$个信息集。有限注德州扑克具有 $10^{17}个节点，$ $10^{14}$ 个信息集。</p><p>在两个游戏中，与 NFSP 和使用抽象的方法进行比较。</p><h4 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h4><p>两个网络采用同样的结构，7层 98948个参数组成。网络的输入为信息集，由卡牌集合和投注历史组成。卡牌有3个embedding层之和进行表示：rank(1-13)，suit(1-4)，card(1-52)。投注历史用一个二进制向量表示是否投注，一个浮点数向量表示投注大小，向量的长度为 6*轮数。</p><p>每一层为全连接层，包括Relu激活函数和跳跃连接，即$x_{i+1}=Relu(Ax+x)$，网络的最后一层特征进行了归一化。</p><p>对于价值网络，输出为每个动作的后悔值。对于平均策略网络，输出为归一化的每个动作选择的概率。</p><p><img src="https://ww1.sinaimg.cn/large/006A69aEgy1g89epluhdej30if07wwg6.jpg" alt="image.png"></p><h4 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h4><p>对于每个网络的经验回放池分配了 $4∗10^7$信息集的大小。价值网络在每次迭代中随机初始化，重新开始训练，实验证明这比接着训练具有更快的收敛速度。对于两轮版有限注德州扑克，进行了4000次小批量随机梯度下降迭代，batch-size为10000，使用学习率为0.0010.001 的Adam学习器进行优化。对于有限注德州扑克，进行了32000次迭代，batch-size为20000。</p><h4 id="Linear-CFR"><a href="#Linear-CFR" class="headerlink" title="Linear CFR"></a>Linear CFR</h4><p>LInear CFR是在计算时使用迭代次数 $t$ 进行加权，虽然不会收敛到更好的结果，但可以更快地收敛。</p><p>具体而言，在经验回放池中额外保存迭代次数 $t$ ,在每 $T$ 次训练网络时按照 $2/T$进行缩放，我的理解时权重为 $2t/T$。</p><h4 id="DeepCFR-算法"><a href="#DeepCFR-算法" class="headerlink" title="DeepCFR 算法"></a>DeepCFR 算法</h4><p><img src="https://ww1.sinaimg.cn/large/006A69aEly1g89ld4tvnmj31160cj41c.jpg" alt="image.png"></p><p><img src="https://ww1.sinaimg.cn/large/006A69aEly1g89mscfraaj31190l7q87.jpg" alt="image.png"></p><h3 id="Experimental-Results"><a href="#Experimental-Results" class="headerlink" title="Experimental Results"></a>Experimental Results</h3><p>在两轮有限注德州扑克环境下实验结果，横坐标是遍历到的节点，纵坐标时可利用性，图中的收敛速度忽视了神经网络训练所花费的时间。</p><p>比较对象:</p><ul><li>Abstraction, 将 $10^9$个节点使用人为设计的特征，利用 K-means 进行聚类，然后再使用外采样线性CFR进行迭代求解。</li><li>Lossless Abstraction。仅将策略无关的节点聚类在一起，比如在顺子牌型中花色无关，该种抽样不会影响最后结果。</li><li>NFSP。深度学习拟合Fictitious Play。</li></ul><p>图像显示结果：</p><ul><li>DeepCFR和最少抽样的方法达到了相同可利用性，但是具有更快的收敛速度。</li><li>由于神经网络的训练，DeepCFR的实际运行时间可能会更长，但是具有不依赖于具体特征的优点。</li></ul><p><img src="https://ww1.sinaimg.cn/large/006A69aEly1g89mku3e81j30iu0cq404.jpg" alt="image.png"></p><p>横坐标为每次迭代中采样的数量，纵坐标为可利用率。采样的数量越少，收敛速度越慢，需要收集更多的数据来减少方差。最后每一种采样方法都能收敛到近似相同的可利用率。</p><p><img src="https://ww1.sinaimg.cn/large/006A69aEgy1g8ao4vi18kj30ck087t9m.jpg" alt="image.png"></p><p>横坐标为每次训练网络时进行小批量随机梯度下降的次数。该数值不影响收敛速度，但会影响最后收敛到的可利用率。可能原因时训练次数减少使得网络的拟合能力下降。</p><p><img src="https://ww1.sinaimg.cn/large/006A69aEgy1g8ao8kpgnsj30cl08dq3r.jpg" alt="image.png"></p><p>网络模型参数的影响。</p><p><img src="https://ww1.sinaimg.cn/large/006A69aEgy1g8aobmhw3bj30cr087t8z.jpg" alt="image.png"></p><p>显示了使用线性加权，从头训练网络，在所有后悔值小于0时随机选择动作的影响。不从头训练网络时最后收敛到的可利用率变大，可能是由于网络陷入了局部最优值。</p><p><img src="https://ww1.sinaimg.cn/large/006A69aEgy1g8aoey5eesj30el096aaw.jpg" alt="image.png"></p><p>鱼塘采样和滑动窗口对实验的影响。滑动窗口方法在内存占满后不再继续收敛。</p><p><img src="https://ww1.sinaimg.cn/large/006A69aEgy1g8aohxsfyuj30em094mxl.jpg" alt="image.png"></p><h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>将CFR算法与深度神经网络函数拟合相结合，试图在大规模不完全信息游戏中寻找纳什均衡。该方法在理论上可行，与大规模扑克游戏中达到了与使用具体领域知识方法的相似性能。</p><p>将DeepCFR扩展到更大的游戏中，可能需要结合其他方法，如可扩展的采样策略，减少采样的方差。</p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RL </tag>
            
            <tag> Paper </tag>
            
            <tag> CFR </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>服务器搭建jupyter notebook</title>
      <link href="/2020/01/18/%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%90%AD%E5%BB%BAjupyter-notebook/"/>
      <url>/2020/01/18/%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%90%AD%E5%BB%BAjupyter-notebook/</url>
      
        <content type="html"><![CDATA[<!-- build time:Sun Jan 19 2020 18:07:22 GMT+0800 (GMT+08:00) --><h3 id="anaconda-创建环境并激活"><a href="#anaconda-创建环境并激活" class="headerlink" title="anaconda 创建环境并激活"></a>anaconda 创建环境并激活</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conda create -n envName python=3.7</span><br><span class="line">conda activate envName</span><br></pre></td></tr></table></figure><h3 id="生成密码"><a href="#生成密码" class="headerlink" title="生成密码"></a>生成密码</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jupyter notebook password</span><br></pre></td></tr></table></figure><p>输入两次密码后，加密值保存在 .jupyter/jupyter_notebook_config.json</p><h3 id="修改配置文件"><a href="#修改配置文件" class="headerlink" title="修改配置文件"></a>修改配置文件</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim ~/.jupyter/jupyter_notebook_config.json</span><br></pre></td></tr></table></figure><p>修改为</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="string">"NotebookApp"</span>: &#123;</span><br><span class="line">    <span class="string">"password"</span>: <span class="string">"sha1: ***"</span>,</span><br><span class="line">    <span class="string">"ip"</span>: <span class="string">" Server IP"</span>,</span><br><span class="line">    <span class="string">"open_browser"</span>: <span class="literal">false</span>,</span><br><span class="line">    <span class="string">"port"</span>: 9000</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="后台运行jupyter"><a href="#后台运行jupyter" class="headerlink" title="后台运行jupyter"></a>后台运行jupyter</h3><p>在终端中输入</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup jupyter notebook &gt;/tmp/tmp.file 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure><ul><li>nohup 表示将任务挂起</li><li>最后的&amp; 表示在后台运行，只加该符号而不加nohup，任务会在终端关闭后停止</li><li>2&gt;&amp;1 表示将错误输出流写入到标准输出流。</li><li>>/tmp/tmp.file 表示将标准输出流写入到文件</li></ul><p>可以在浏览器通过服务器IP+端口号进行访问。</p><h3 id="添加anaconda内核"><a href="#添加anaconda内核" class="headerlink" title="添加anaconda内核"></a>添加anaconda内核</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conda install ipykernel </span><br><span class="line">python -m ipykernel install --user --name envName --display-name <span class="string">"envName"</span></span><br></pre></td></tr></table></figure><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p><a href="https://jupyter-notebook.readthedocs.io/en/latest/public_server.html#notebook-server-security" target="_blank" rel="noopener">https://jupyter-notebook.readthedocs.io/en/latest/public_server.html#notebook-server-security</a></p><p><a href="https://www.cnblogs.com/yinzm/p/7881328.html" target="_blank" rel="noopener">https://www.cnblogs.com/yinzm/p/7881328.html</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> server </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>[论文阅读]Monte Carlo Sampling for Regret Minimization in Extensive Games</title>
      <link href="/2019/10/19/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Monte-Carlo-Sampling-for-Regret-Minimization-in-Extensive-Games/"/>
      <url>/2019/10/19/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Monte-Carlo-Sampling-for-Regret-Minimization-in-Extensive-Games/</url>
      
        <content type="html"><![CDATA[<!-- build time:Sun Jan 19 2020 18:07:22 GMT+0800 (GMT+08:00) --><div class="table-container"><table><thead><tr><th>标题</th><th>Monte Carlo Sampling for Regret Minimization in Extensive Games</th></tr></thead><tbody><tr><td>团队</td><td>Alberta、CMU、Yahoo</td></tr><tr><td>时间</td><td>2009</td></tr><tr><td>出处</td><td>nips</td></tr></tbody></table></div><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>CFR算法通过最小化反事实后悔值来最小化总后悔值，使得平均策略收敛于纳什均衡。当时的CFR算法具有以下缺点:</p><ul><li>多数实现针对卡牌游戏进行优化，对发牌进行抽样，这是一种不通用的方法。</li><li>CFR在迭代时需要已知对手的策略，使得该方法不适用于在线学习。</li></ul><p>该论文提出了基于采样的CFR算法，使用给出了通用形式，又详细介绍了两种方法: 结果采样(采样每一个对局)和外采样(采样对手和机会节点的动作)。基于采样的CFR算法在期望上与原CFR相同，但是方差较大，通过最小化后悔值的界限体现出来。结果采样中对手的策略被抵消掉，所以可以用于在线学习，结果采样的总迭代时间减少了。</p><h2 id="蒙特卡洛CFR"><a href="#蒙特卡洛CFR" class="headerlink" title="蒙特卡洛CFR"></a>蒙特卡洛CFR</h2><h3 id="通用方法"><a href="#通用方法" class="headerlink" title="通用方法"></a>通用方法</h3><p>基于采样的CFR的基本思路是将所有叶子节点划分若干块，每次以某种概率采样某一块，计算该块内的所有叶子节点对其到根路径上的节点的贡献，对于计算的贡献值为原有的方法除以选取该块的概率，从而保证在期望上是相同的。</p><p>将所有叶子节点划分为若干个集合 $Q = \{ Q_1, …, Q_r\} $, 选取某个集合的概率为$q_j$, 保证 $\sum_{j=1}^{r} q_j = 1$， 所有集合的并覆盖叶子节点。</p><p>定义一个叶子节点在每一次迭代中被选取的概率$q(z) = \sum_{j:z\in Q_j}$, 使用求和符号的意思时一个叶子节点可能被划分在多个集合中，一般来说都不会重复划分叶子节点。</p><p>在提出了CFR的论文中，CFR的定义为</p><script type="math/tex;mode=display">v(\sigma, I) = \sum_{h\in I, z\in Z}\pi^\sigma_{-i}(h)\pi^\sigma(h, z)u_i(z)</script><p>将该定义转化为直接枚举每个叶子节点，则可以确定其对应的$h$节点, 定义为 $z[I]$，则CFR的定义可以表示为</p><script type="math/tex;mode=display">v(\sigma, I) = \sum_{z\in Z_I}\pi^\sigma_{-i}(z[I])\pi^\sigma(z[I], z)u_i(z)</script><p>在一次采样中，每个节点的采样反事实价值为</p><script type="math/tex;mode=display">\tilde{v} _i (\sigma, I | j) = \sum_{z\in Q_j \cap Z_I} \frac{1}{q(z)}u_i(z)\pi^\sigma_{-i}(z[I])\pi^\sigma(z[I], z)</script><p>在多次采样下， 采样反事实后悔值的期望值与反事实后悔值相同。</p><p>同样地，可以定义每个节点采取某个动作的立即反事实后悔值。</p><script type="math/tex;mode=display">\tilde{r}(I, a) = \tilde{v} _i (\sigma^t_{(I\rightarrow a)}, I) - \tilde{v}_i(\sigma^t, I)</script><h3 id="结果采样CFR"><a href="#结果采样CFR" class="headerlink" title="结果采样CFR"></a>结果采样CFR</h3><p>结果采样CFR中对于每一条到叶子节点的路径，对应于一个子集合，即每个子集合包含的节点数量为1，选取子集合的概率为这条路径上所有策略的乘积。即对于一个策略组合$\sigma^\prime$ , $q(z) = \pi^{\sigma^\prime}(z)$。</p><p>则对于该叶子节点，计算其对路径上每一个节点的所有动作的立即反事实后悔值的贡献。</p><script type="math/tex;mode=display">r(I, a) =\left\{\begin{aligned}w_I *(1 - \sigma(a | z[I])) && if (z[I]a) \in z \\-w_I *\sigma(a|z[I])&&otherwise\end{aligned}\;\;\;where \;w_I = \frac{u_i(z)\pi^\sigma_{-i}(z)\pi_i^\sigma(z[i]a, z)}{\pi^{\sigma^\prime}(z)}\right.</script><p>如果叶子节点的采样概率与对手的策略相同，则可以把$\pi_{-i}^\sigma(z)$抵消掉，此时式子的计算不再依赖于对手的概率，所以该算法可以推广为在线学习算法。</p><h3 id="外采样CFR"><a href="#外采样CFR" class="headerlink" title="外采样CFR"></a>外采样CFR</h3><p>外采样CFR对于对手策略和机会策略进行采样，即对于一个策略组合$\sigma ^ \prime$, $q(z) = \pi^{\sigma^\prime(z)}_{-i}$。实际每次计算中，对于博弈树进行遍历，遇到对手策略或机会策略节点，根据实际策略采样一个动作，选择这个动作的边向下遍历，遇到自己策略的节点时，则选择所有动作的边向下遍历。则实际遍历到的是整颗博弈树的一颗子树，该棵子树中包含多个block，一个block中的所有叶子节点的 $\pi_{-i}$ 值相同。</p><p>则对于每个叶子节点，同样计算对所有自己决策节点的每一个动作的贡献。</p><script type="math/tex;mode=display">\sum_{z\in Q \cap Z_I} u_i(z) (\pi_i^\sigma(z[I]a, z) - \pi_i^\sigma(z[I], z))</script><p>其中 $q(z)$ 与 $\pi_{-i}(z[I])$ 和 $\pi_{-i}(z[I], z)$ 进行抵消。</p><h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p>在四个游戏中进行实验。</p><ul><li>Goofspiel. 玩家每人拥有 1-K 十三张牌，公共牌也为1-K十三张牌，每次亮出一张公共牌，每个玩家出一张自己的牌进行竞标，牌最大者获胜，拥有公共牌对应的分数，然后每个玩家弃掉竞标的牌，13轮后拥有最多分数玩家获胜。实际实验中使用变形，每个玩家不知道竞标的牌的大小。</li><li>One-Card Poker，每人从牌堆中抽一张牌，加价或不加价一轮，比牌大小。实际实验中牌堆大小为500.</li><li>Princess and Monster， 在一个黑暗空间中怪兽和公主进行移动，当两者的距离小于一定范围时公主被抓住。实际实验中在3*3地图中随机初始位置，逃离者的收益为没有被抓住的步数。</li><li>Latent Tic-Tac-Toe，井字棋变形，每一步双方选择下一步棋的位置，棋子的显示延迟一回合，即当对方下完下一步棋后，才能看到我方当前回合下的棋，如果下棋发生冲突则算输。</li></ul><p><img src="https://ww1.sinaimg.cn/large/006A69aEly1g86pqegxi1j30ux08fdhh.jpg" alt="image.png"></p><p>采用四种算法进行评估。</p><ul><li>朴素CFR。</li><li>带剪枝的CFR。当某个节点按照策略选取的概率为0时，不再搜索该棵子树。</li><li>结果CFR。增加随机选取节点。以 $\epsilon$ 概率随机采样， $1 - \epsilon$ 概率按照策略采样，结果发现 $\epsilon = 0.6$ 时较优。</li><li>外部采样。</li></ul><p><img src="https://ww1.sinaimg.cn/large/006A69aEgy1g86pxxk6uvj30w80nwgqh.jpg" alt="image.png"></p><p>横坐标为搜索时遍历节点数，不采用迭代次数的原因时不同算法的每次迭代时间不一样，纵坐标为计算策略的可利用性。</p><p>实验结果分析:</p><ul><li>MCCFR变体的性能显著由于朴素CFR。</li><li>剪枝对于CFR性能的体现较明显，在后期每次迭代遍历到的节点数大大减小。</li><li>外部采样在三个游戏中优于结果采样，算法的性能可能和游戏的特性有关。</li></ul><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>定义了基于采样的CFR算法，并提出了两种采样模式，外采样(每次迭代采样一条路径)，外部采样(每次采样对手的chance的一个固定策略)。对与朴素CFR提出了更紧密的边界，也对两种采样方法提出了边界。</p><p>未来工作:</p><ul><li>检验游戏特性对算法的影响。</li><li>外采样在在线学习中的应用。</li><li>采用不完全回忆来抽象动作。</li></ul><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RL </tag>
            
            <tag> Paper </tag>
            
            <tag> CFR </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[论文阅读]Regret Minimization in Games with Incomplete Information</title>
      <link href="/2019/10/15/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Regret-Minimization-in-Games-with-Incomplete-Information/"/>
      <url>/2019/10/15/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Regret-Minimization-in-Games-with-Incomplete-Information/</url>
      
        <content type="html"><![CDATA[<!-- build time:Sun Jan 19 2020 18:07:22 GMT+0800 (GMT+08:00) --><div class="table-container"><table><thead><tr><th>标题</th><th>Regret Minimization in Games with Incomplete Information</th></tr></thead><tbody><tr><td>团队</td><td>Alberta</td></tr><tr><td>时间</td><td>2008</td></tr><tr><td>出处</td><td>nips</td></tr></tbody></table></div><h3 id="论文梗概"><a href="#论文梗概" class="headerlink" title="论文梗概"></a>论文梗概</h3><p>该论文的作者为阿尔伯塔大学的团队，该团队在扑克博弈方面有着深厚的积累，之前发表的DeepStack论文也是出自该作者，之后也要抽时间读一下那篇论文。</p><p>该论文主要提出了CFR算法，为双人博弈中寻找纳什均衡解提供了一种通过自我博弈迭代的方法，提供了理论上界证明，在双人有限注德州扑克对弈中进行了实验。CFR算法十分重要，之后的发展大多是在此基础上进行改进。</p><h3 id="背景知识"><a href="#背景知识" class="headerlink" title="背景知识"></a>背景知识</h3><p>对于扑克这一类游戏，可以归纳为不完全信息的扩展式博弈。扩展式博弈是指多个玩家轮流进行决策，而不是同时进行决策。不完全信息是指，与围棋、象棋不同，每个玩家对于场面上的信息不是完全掌握，具体在扑克中就是不知道对方的手牌。所以可以将信息分为私有信息和公共信息，私有信息就是玩家的手牌，公共信息就是公共牌和双方玩家的动作序列。</p><p>针对于扩展式博弈轮流进行决策的特性，可以建立出一棵博弈树，博弈树的每条边对应于玩家的一种决策。对于非玩家进行决策的内容，将其抽象为一个玩家，称为chance，可以理解成运气成分。</p><p><img src="https://ww1.sinaimg.cn/large/006A69aEly1g7ysspbhu0j30hq0d575c.jpg" alt="2019-10-15 12-59-16 的屏幕截图.png"></p><p>在博弈树中一个重要概念就是信息集，我的理解是对于每个人而言，将所有其具有同样信息的节点聚合起来，称为信息集。就扑克而言，同一个信息集的两个状态，具有相同的初始手牌和双方相同的动作序列，唯一的不同在于对手的手牌不同，但这一点对于本方玩家是不知道的，所以具有相同信息集。信息集的重要意义在于可以为这些状态制定一个统一的策略，这也符合人类制定策略的直觉。即当我们掌握相同的信息时，制定的策略一般都是相同的。</p><p>策略就是对于玩家在某个信息集下，执行每一种动作的可能，也就是对于可行动作的概率分布。讲所有玩家的策略组合在一起，就是一个策略组合，当一个策略组合确定下来后，就可以求出博弈树下到达每一个节点的概率了。</p><p>进行一场博弈肯定会有输赢，博弈树中的所有节点就是终止节点，可以根据游戏规则计算出输赢的金钱，也就是该节点的收益。当确定了概率组合之后，也就可以算出每个玩家的总体期望收益了。</p><p>在制定博弈策略时，两个重要策略是最佳应对策略和纳什均衡策略。</p><p>最佳应对策略是指假设对手的策略固定，那么找一个策略使得我能赢的期望收益最大，这个策略就是最佳应对策略。</p><p>纳什均衡策略是指双方的策略达到了一种动态均衡，互相为对方的最佳应对策略，假如某一方改变策略，必定会损失金钱。</p><p>评价一个策略的好坏的一个指标是可利用性。可利用性是指对方采用最佳应对策略与我的策略进行对打，对方的期望收益是多少。可利用性越低，说明我们在实际中输的可能性越小。当可利用性为0时，就说明我的策略是那是纳什均衡策略，无论对方的策略是什么，我都能保证我不会输钱。</p><p>由于双人扑克博弈时零和博弈，所以在纳什均衡解下双方的金币为0。</p><p>下面用一些数学符号来定义这些概念。</p><div class="table-container"><table><thead><tr><th>符号</th><th>意义</th></tr></thead><tbody><tr><td>$N = {0, 1, … n - 1}$</td><td>玩家集合</td></tr><tr><td>$H $</td><td>历史动作序列，也可以看成状态，博弈树上的每一个点</td></tr><tr><td>$Z,\; Z \subseteq H$</td><td>叶子节点集合</td></tr><tr><td>$P(h) \in N \cup \{c\}$</td><td>定义了每一个状态的行动者，可能为玩家或者chance</td></tr><tr><td>$f_c(a \vert h)$</td><td>在状态h下，行动玩家为 $P(h)=c$ , 执行动作 $a$ 的概率</td></tr><tr><td>$I_i \subseteq \{h\in H:p(h)=i\}$</td><td>信息集，为所有决策玩家为i的决策点的一个划分，信息集之间互不相交。</td></tr><tr><td>$u_i(z)$</td><td>为叶子节点 $z$ 对于玩家 $i$ 的收益</td></tr><tr><td>$\triangle_{u, i} = max_zu_i(z) - min_z u_i(z)$</td><td>玩家i的收益范围</td></tr><tr><td>$\sigma_i(a\vert I)$</td><td>玩家i的策略</td></tr><tr><td>$\pi^\sigma(h)$</td><td>在策略组合$\sigma$ 下到达状态h的概率</td></tr><tr><td>$\pi_i^\sigma(h)$</td><td>在策略组合$\sigma$下玩家i的策略对到达状态h的概率的贡献</td></tr><tr><td>$\pi_{-i}^\sigma(h)$</td><td>在策略组合$\sigma$下其他玩家的策略对到达状态h的概率的贡献</td></tr><tr><td>$\pi^\sigma(I) = \sum_{h\in I}\pi^\sigma(h)$</td><td>在策略组合$\sigma$ 下到达信息集I的概率</td></tr><tr><td>$u_i(\sigma)=\sum_{h\in Z}u_i(h)\pi^\sigma(h)$</td><td>策略组合下玩家i的期望收益</td></tr><tr><td>$u_1(\sigma^\star) = \max_{\sigma_1’\in\Sigma_1}u_1(\sigma_1’, \sigma_2)$</td><td>对于$\sigma_2$的最佳应对策略</td></tr></tbody></table></div><h3 id="后悔值最小化"><a href="#后悔值最小化" class="headerlink" title="后悔值最小化"></a>后悔值最小化</h3><p>后悔值最小化是一个在线方法，用来求解纳什均衡的通用方法，在每一次迭代中，寻找一个最佳策略，使得用这个策略去和前几轮的的对手打，获得的平均收益最大。</p><p>定义平均总后悔值为</p><script type="math/tex;mode=display">R_i^T = \frac{1}{T}\max_{\sigma_i^*\in\Sigma_i}\sum_{t=1}^T (u_i(\sigma_i^*,\sigma_{-i}^t) - u_i(\sigma^t))</script><p>再定义平均策略</p><script type="math/tex;mode=display">\overline{\sigma}_i^t(a|I) = \frac{\sum_{t=1}^T\pi_i^{\sigma^t}(I)\sigma^t(a|I)}{\sum_{t=1}^T\pi_i^{\sigma^t}(I)}</script><blockquote><p>定理: 当平均总后悔值小于 $\epsilon$ ，平均策略为 $2\epsilon$ 纳什均衡策略。</p></blockquote><p>所以，如果能最小化平均总后悔值，就等于找到求解纳什均衡的一种方法，</p><h3 id="反事实后悔值"><a href="#反事实后悔值" class="headerlink" title="反事实后悔值"></a>反事实后悔值</h3><p>定义反事实收益为在达到信息集I的前提下，所有玩家使用策略组合$\sigma$ ，玩家i使用固定到达信息机 $I$ 的策略下的 期望收益。</p><script type="math/tex;mode=display">u_i(\sigma, I) = \frac{\sum_{h\in I, z\in Z}\pi^\sigma_{-i}(h)\pi^\sigma(h, z)u_i(z)}{\pi_{-i}^\sigma(I)}</script><p>定义 $\sigma\vert_{I \to a}$ 表示玩家$i$ 在信息机$I$ 下改变策略选择动作 $a$，其他玩家不变。则立刻反事实后悔值为</p><script type="math/tex;mode=display">R_{i, imm}^T(I) = \frac{1}{T}\max_{a\in A(I)} \sum_{t=1}^T \pi_{-i}^{\sigma^t}(I)(u_i(\sigma^t\vert_{I\to a}, I) - u_i(\sigma^t, I))</script><script type="math/tex;mode=display">R_{i, imm}^{T, +} = max(R_{i, imm}^T, 0)</script><blockquote><p>定理: $R_i^T \le \sum_{I \in I_i} R_{i, imm}^{T, +}(I)$</p></blockquote><p>由该定理可知最小化立刻反事实后悔值之和，就可以最小化平均总后悔值，也就是在逼近纳什均衡策略。</p><p>立刻反事实后悔值的好处是，可以通过策略的迭代来最小化该后悔值。</p><p>对于所有信息集的每一个动作，维护</p><script type="math/tex;mode=display">R_i^T(I, a) = \frac{1}{T}  \sum_{t=1}^T \pi_{-i}^{\sigma^t}(I)(u_i(\sigma^t\vert_{I\to a}, a) - u_i(\sigma^t, I))</script><p>则策略迭代方式为根据该后悔值的大小按比例分配，若所有后悔值的大小都小于0，则随机挑选动作，具体而言</p><script type="math/tex;mode=display">\sigma_i^{T+1}(a \vert I) = \left\{\begin{aligned}&\frac{R_i^{T,+}(I, a)}{\sum_{a\in A(I)}R_i^{T, +}(I, a)}  & &if \; \Sigma_{a\in A(i)} R_i^{T, +}(I, a) > 0\\&\frac{1}{\vert A(I) \vert}  &  & otherwise \end{aligned}\right.</script><blockquote><p>定理: $R_{i, imm} ^T \le \triangle_{u, i}\sqrt{\vert A_i} \vert / \sqrt{T}$, $R_i^T \le \triangle_{u, i}\vert I_i \vert\sqrt{\vert A_i} \vert / \sqrt{T}$</p></blockquote><h3 id="双人德州扑克应用"><a href="#双人德州扑克应用" class="headerlink" title="双人德州扑克应用"></a>双人德州扑克应用</h3><p>分为两个步骤进行，首先对游戏进行抽象，将游戏状态进行缩减，再使用CFR算法求解。</p><p>在进行抽象时，不改变下注结构，而是对卡牌进行抽象。将卡牌按照牌力大小的平方分成10组，使用平方的原因时当牌力越大时，人们对于自己可能会赢越自信。</p><h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><h4 id="计算方法"><a href="#计算方法" class="headerlink" title="计算方法"></a>计算方法</h4><p>在德州扑克中，双方的博弈胜负结果用 mbb/h 表示，mbb 表示大盲下注的千分之一，h表示每一手，整体含义是平均每局能赢钱数与大盲下注量的比例。</p><p>一般来说小盲下注量为50， 大盲下注量为100，则在双人中每把牌都fold的结果是 750 mbb/h。如果双方对战结果是 10mbb/h, 那么在统计上来说打一百万场才有 95%的胜率。</p><p>可利用性可以认为是与纳什均衡之间的差距，但是在大博弈树下直接计算十分困难，一般采用和较高水准的对手进行多次博弈，计算mbb/h来衡量。虽然策略之间不存在传递性，但是可以大概衡量策略的好坏。</p><p>在抽象游戏下，计算可利用性的复杂度降低，但是计算出来的可利用性并不等于原空间下的可利用性。</p><p>该论文 通过 $2*10^9$ 次迭代，在抽象空间中达到了 2.2 mbb/h。</p><h4 id="收敛速度"><a href="#收敛速度" class="headerlink" title="收敛速度"></a>收敛速度</h4><p><img src="https://ww1.sinaimg.cn/large/006A69aEgy1g7ywj2njd2j30ve0e0aby.jpg" alt="2019-10-15 15-09-24 的屏幕截图.png"></p><p>对于抽象组数进行了比较，抽象组数可以表示状态空间/信息集的大小，认为收敛所需的迭代次数与状态空间成线性关系，为之前的定理提供依据。</p><h4 id="对战比较"><a href="#对战比较" class="headerlink" title="对战比较"></a>对战比较</h4><p><img src="https://ww1.sinaimg.cn/large/006A69aEgy1g7ywmhii7xj30pg08habl.jpg" alt="2019-10-15 15-12-29 的屏幕截图.png"></p><p>将CFR和当时流行的方法进行对战。</p><h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><p>提出了一种在扩展式博弈中的新的后悔值——反事实后悔值。通过最小化反事实后悔值，可以最小化总后悔值，并且提出了一种通用的方法来有效的最小化反事实后悔值。在扑克中验证了这种方法，表明这种方法可以在$10^{12}$的状态空间下计算出近似纳什均衡。</p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RL </tag>
            
            <tag> Paper </tag>
            
            <tag> CFR </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LHY-ML-Week1</title>
      <link href="/2019/09/29/LHY-ML-Week1/"/>
      <url>/2019/09/29/LHY-ML-Week1/</url>
      
        <content type="html"><![CDATA[<!-- build time:Sun Jan 19 2020 18:07:22 GMT+0800 (GMT+08:00) --><h2 id="机器学习的介绍"><a href="#机器学习的介绍" class="headerlink" title="机器学习的介绍"></a>机器学习的介绍</h2><h3 id="机器学习目标"><a href="#机器学习目标" class="headerlink" title="机器学习目标"></a>机器学习目标</h3><p>寻找一个通用函数，对于给定的数据，输出符合要求的数据。</p><h3 id="Framework"><a href="#Framework" class="headerlink" title="Framework"></a>Framework</h3><p>在一个函数集中，利用一系列的数据，找出最适合的函数。</p><h3 id="Relation"><a href="#Relation" class="headerlink" title="Relation"></a>Relation</h3><p><img src="https://ww1.sinaimg.cn/large/006A69aEgy1g7gkoxn1fyj310b0qfh1w.jpg" alt="2019-09-29 18-38-50 的屏幕截图.png"></p><h4 id="Scenario"><a href="#Scenario" class="headerlink" title="Scenario"></a>Scenario</h4><p>机器学习根据使用场景不同可以分为监督学习，半监督学习, 迁移学习，无监督学习和强化学习。</p><p>不同场景的区别在于数据的获取难度，不同的数据决定使用场景。</p><ul><li>监督学习中所有数据给定标签。</li><li>半监督学习中部分数据给定标签。</li><li>迁移学习中具有大量相似问题的数据。</li><li>无监督学习中所有数据没有标签。</li><li>强化学习中训练数据只有好坏的评估机制。</li></ul><h4 id="Task"><a href="#Task" class="headerlink" title="Task"></a>Task</h4><p>在一个场景中根据输出的不同要求，分为回归任务，分类任务和结构化学习。</p><ul><li>回归任务输出的是连续性数据。</li><li>分类任务输出的是离散性数据。</li><li>结构化学习任务输出的是多样化的数据，如目标检测是需要同时输出目标所在位置和目标的类别。</li></ul><h4 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h4><p>对于一个具体的任务，需要对数据进行拟合一个函数，根据该函数的不同可以分为线性模型和非线性模型，非线性模型又包括深度学习，支持向量机，决策树，K-近邻等。</p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> LHY-ML </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ML </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CS294-005-策略梯度简介</title>
      <link href="/2019/09/18/CS294-005-%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%E7%AE%80%E4%BB%8B/"/>
      <url>/2019/09/18/CS294-005-%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%E7%AE%80%E4%BB%8B/</url>
      
        <content type="html"><![CDATA[<!-- build time:Mon Jan 20 2020 17:26:51 GMT+0800 (GMT+08:00) --><h3 id="直接策略梯度推导"><a href="#直接策略梯度推导" class="headerlink" title="直接策略梯度推导"></a>直接策略梯度推导</h3><p>强化学习的目标为找出最优的策略，使得期望收益最大。</p><script type="math/tex;mode=display">\theta^* = arg\max_\theta J(\theta) = arg \max_\theta E_{\tau\sim p_\theta(\tau)}[r(\tau)]</script><p>对$J(\theta)$进行求导可得</p><script type="math/tex;mode=display">\begin{aligned}\bigtriangledown_\theta J(\theta) &=\bigtriangledown_\theta  E_{\tau\sim p_\theta(\tau)}[r(\tau)]\\&= \bigtriangledown_\theta\int p_\theta(\tau)r(\tau)d\tau\\&=\int \bigtriangledown_\theta p_\theta(\tau) r(\tau)d\tau \\&=\int p_\theta(\tau)\bigtriangledown_\theta log\,p_\theta(\tau) r(\tau)d\tau\\&= E_{\tau\sim p_\theta(\tau)}[\bigtriangledown_\theta log\,p_\theta(\tau)r(\tau)]\end{aligned}</script><p>由$p_\theta(\tau)$展开式可得</p><script type="math/tex;mode=display">\begin{aligned}p_\theta(\tau) &= p(s_1) \prod_{t=1}^T \pi_\theta(a_t|s_t)p(s_{t+1}|s_t, a_t) \\log\,p_\theta(\tau) &= log \,p(s_1) + \sum_{t=1}^T log\,\pi_\theta(a_t|s_t) + log\,p(s_{t+1}|s_t,a_t)\\\bigtriangledown_\theta log\,p_\theta(\tau) &=  \sum_{t=1}^T \bigtriangledown_\theta log \pi_\theta(a_t|s_t)\end{aligned}</script><p>故化简后得到的$J(\theta)$的梯度表达式为</p><script type="math/tex;mode=display">\bigtriangledown_\theta J(\theta) = E_{\tau\sim p_\theta(\tau)}[(\sum_{t=1}^T \bigtriangledown_\theta log\,\pi_\theta(a_t|s_t))](\sum_{t=1}^T r(s_t, a_t))</script><p>根据采样值来逼近期望值可以得到</p><script type="math/tex;mode=display">\bigtriangledown_\theta J(\theta) \approx \frac{1}{N}\sum_{i=1}^N[(\sum_{t=1}^T \bigtriangledown_\theta log\,\pi_\theta(a_{i,t}|s_{i,t}))](\sum_{t=1}^T r(s_{i,t}, a_{i,t}))</script><p>接着使用梯度下降法来修改策略</p><script type="math/tex;mode=display">\theta \leftarrow \theta + \alpha \bigtriangledown_\theta J(\theta)</script><h3 id="REINFORCE"><a href="#REINFORCE" class="headerlink" title="REINFORCE"></a>REINFORCE</h3><h4 id="algorithm"><a href="#algorithm" class="headerlink" title="algorithm"></a>algorithm</h4><ul><li>根据策略 $\pi_\theta(a_t,s_t)$ 进行采样 $\tau^i$</li><li>计算策略梯度 $\bigtriangledown_\theta J(\theta) \approx \frac{1}{N}\sum_i<a href="\sum_{t=1}^T r(s_t^i, a_t^i">(\sum_{t=1}^T \bigtriangledown_\theta log\,\pi_\theta(a_t^i|s_t^i))</a>)$</li><li>通过梯度来优化策略 $\theta \leftarrow \theta + \alpha \bigtriangledown_\theta J(\theta)$</li></ul><p>对于局部观测情况同样适用。</p><p>该算法可以认为是形式化了试错学习。对于正确的轨迹具有较高的回报值，更倾向于提高策略概率，对于错误的轨迹回报值低，则倾向于降低策略概率。</p><p><img src="https://ww1.sinaimg.cn/large/006A69aEly1gb2zaoi34pj30k40a942g.jpg" alt="image.png"></p><h4 id="存在问题"><a href="#存在问题" class="headerlink" title="存在问题"></a>存在问题</h4><p>本质问题是方差问题。</p><p>正常而言对奖励增加一个常数，不会对算法带来影响，但对于REINFORCE算法有很大的影响。REINFORCE算法对奖励值很敏感，根据奖励值的正负和大小比例来调整策略。</p><p><img src="https://ww1.sinaimg.cn/large/006A69aEly1gb2zgajfxgj31ca0e940k.jpg" alt="image.png"></p><h3 id="降低方差"><a href="#降低方差" class="headerlink" title="降低方差"></a>降低方差</h3><h4 id="因果关系"><a href="#因果关系" class="headerlink" title="因果关系"></a>因果关系</h4><p>若 $ t &lt; t’$， 则 $t’$ 时刻的策略不会影响 $t$ 时刻的策略。</p><script type="math/tex;mode=display">\begin{aligned}\bigtriangledown_\theta J(\theta) \approx &\frac{1}{N}\sum_{i=1}^N[(\sum_{t=1}^T \bigtriangledown_\theta log\,\pi_\theta(a_{i,t}|s_{i,t}))](\sum_{t=1}^T r(s_{i,t}, a_{i,t})) \\= &\frac{1}{N}\sum_{i=1}^N\sum_{t=1}^T \bigtriangledown_\theta log\,\pi_\theta(a_{i,t}|s_{i,t})(\sum_{t'=1}^T r(s_{i,t'}, a_{i,t'})) \\\rightarrow &\frac{1}{N}\sum_{i=1}^N\sum_{t=1}^T \bigtriangledown_\theta log\,\pi_\theta(a_{i,t}|s_{i,t})(\sum_{t'=t}^T r(s_{i,t'}, a_{i,t'})) \\\end{aligned}</script><p><strong>reward to go</strong>: $ \hat{Q}_{i,t} = \sum_{t’=t}^T r(s_{i,t’}, a_{i,t’}))$</p><p>一般而言，在固定区间的情况下最优策略是时变策略，即在不同的时间段，面临相同的状态时，会执行不同的动作。</p><p>而通常的做法是将策略限制为时间无关的策略。</p><p><strong>降低方差的原因</strong>: 减少了数值大小。</p><h4 id="添加奖励基准"><a href="#添加奖励基准" class="headerlink" title="添加奖励基准"></a>添加奖励基准</h4><script type="math/tex;mode=display">\begin{aligned}\bigtriangledown_\theta J(\theta) \approx &\frac{1}{N}\sum_{i=1}^N \bigtriangledown_\theta log \pi_\theta(\tau)r(\tau) \\=&\frac{1}{N}\sum_{i=1}^N \bigtriangledown_\theta log\pi_\theta(\tau)[r(\tau) - b]\end{aligned}</script><p>一般的做法是令 $b = \frac{1}{N}\sum_{i=1}^{N}r(\tau)$，添加的该项不会改变期望值，但会降低方差。</p><script type="math/tex;mode=display">E_{\tau\sim p_\theta(\tau)}[\bigtriangledown_\theta log \;p_\theta(\tau)b] = \int p_\theta(\tau)\bigtriangledown_\theta log\;p_\theta(\tau)b \;d\tau=\int\bigtriangledown_\theta p_\theta(\tau)b\;d\tau = b\bigtriangledown_\theta\int p_\theta(\tau)d\tau = 0</script><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> RL(CS294) </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CS294-004-强化学习介绍</title>
      <link href="/2019/09/16/CS294-004-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%BB%8B%E7%BB%8D/"/>
      <url>/2019/09/16/CS294-004-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%BB%8B%E7%BB%8D/</url>
      
        <content type="html"><![CDATA[<!-- build time:Mon Jan 20 2020 11:32:24 GMT+0800 (GMT+08:00) --><h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><h4 id="符号"><a href="#符号" class="headerlink" title="符号"></a>符号</h4><p>$s_t$ 表示状态，$o_t$表示观测值，$a_t$表示动作，$\pi(a_t|o_t)$表示部分观测下的策略，$\pi(a_t|s_t)$表示完全观测下的策略。</p><p><img src="https://ww1.sinaimg.cn/large/006A69aEly1gb2s41hc0oj31l10axwhy.jpg" alt="image.png"></p><h4 id="模仿学习"><a href="#模仿学习" class="headerlink" title="模仿学习"></a>模仿学习</h4><p>收集训练数据$\lt o_t,a_t\gt$ ，进行监督学习得到$\pi_\theta(a_t|o_t)$</p><h4 id="奖励函数"><a href="#奖励函数" class="headerlink" title="奖励函数"></a>奖励函数</h4><p>$r(s, a)$ 定义了在某个状态下执行某个动作后得到的奖励。但强化学习期望得到的是全局的收益最大，即暂时的较小奖励可能会带来更大收益。</p><h4 id="马尔科夫链"><a href="#马尔科夫链" class="headerlink" title="马尔科夫链"></a>马尔科夫链</h4><h5 id="马尔科夫链-1"><a href="#马尔科夫链-1" class="headerlink" title="马尔科夫链"></a>马尔科夫链</h5><script type="math/tex;mode=display">\mathcal{M} = \{\mathcal{S}, \mathcal{T}\}</script><ul><li>$\mathcal{S}$ 为状态空间</li><li>$\mathcal{T}$ 为状态转移算子 $T_{i, j} = p(s_{t+1} = i | s_t=j)$, 可以将一个状态分布转移到另一个状态分布。</li><li>若$\mu_{t, j} = p(s_t = i)$ , 则 $\vec \mu_{t+1} = T\vec\mu_t$</li></ul><h5 id="马尔科夫决策链-MDP"><a href="#马尔科夫决策链-MDP" class="headerlink" title="马尔科夫决策链 MDP"></a>马尔科夫决策链 MDP</h5><script type="math/tex;mode=display">M = \{S, A, T, r\}</script><ul><li>$\mathcal{S}$ 为状态空间</li><li>$\mathcal{A}$为动作空间</li><li>$\mathcal{T}$为状态转移算子, $\mathcal{T}_{i,j,k} = p(s_{t+1}=i | s_t=j, a_t=k)$</li><li>$r$ 为奖励函数 $r: S × A \rightarrow \mathbb{R}$</li></ul><h5 id="部分观测马尔科夫决策链-POMDP"><a href="#部分观测马尔科夫决策链-POMDP" class="headerlink" title="部分观测马尔科夫决策链 POMDP"></a>部分观测马尔科夫决策链 POMDP</h5><script type="math/tex;mode=display">M = \{S, A, O, T, \varepsilon, r\}</script><ul><li>S 为状态空间</li><li>A 为动作空间</li><li>O 为观测空间</li><li>T 为状态转移算子</li><li>$\epsilon$ 为散发概率，在某状态下观测到某个现象的概率。 $p(o_t|s_t)$</li><li>r 为奖励函数 $r: S × A \rightarrow \mathbb{R}$</li></ul><h4 id="强化学习目标"><a href="#强化学习目标" class="headerlink" title="强化学习目标"></a>强化学习目标</h4><p>强化学习为一个与环境不断交互的过程，从初始状态出发，根据制定的策略选择一个动作进行执行，环境给出下一个到达的状态以及对于该状态和动作的奖励。</p><p>强化学习目标是制定一个策略，来最大化所有奖励之和的期望。</p><script type="math/tex;mode=display">\theta^*=arg\max_\theta E_{\tau\sim p_\theta(\tau)}[\sum_t r(s_t,a_t)]</script><p>一个完整的交互过程被称为 trajectory， episode， $\tau$, 完整地展开为一个序列$s_1, a_1, … , s_T, a_T$, 通过链式法则可以计算出观测到该轨迹的概率:</p><script type="math/tex;mode=display">p_\theta(\tau)=p_\theta(s_1, a_1, ..., s_T, a_T) = p(s_1) \prod_{t=1}^{T}\pi_\theta(a_t|s_t)p(s_{t+1}|s_t, a_t)</script><p>若将$s_t, a_t$视为一个整体，则马尔科夫决策链可以转化为马尔科夫链。</p><script type="math/tex;mode=display">p(s_{t+1}, a_{t+1} | s_t, a_t) = \pi_\theta(a_t|s_t) p(s_{t+1}|s_t,a_t)</script><p>故最大化奖励之和的期望可以进行转化</p><script type="math/tex;mode=display">\theta^*=arg\max_\theta \sum_{t=1}^T E_{(s_t,a_t)\sim p_\theta(s_t, a_t)}[r(s_t, a_t)]</script><p>当序列为无限时，则当$(s_t,a_t)达到稳态分布时来评估策略，此时有 $$\mu = \mathcal{T}\mu$ ，$\mu$ 是 $\mathcal{T}$ 的特征值为1对应的特征向量。</p><script type="math/tex;mode=display">\theta^*=arg\max_\theta E_{(s,a)\sim p_\theta(s, a)}[r(s, a)]</script><h3 id="强化学习算法结构"><a href="#强化学习算法结构" class="headerlink" title="强化学习算法结构"></a>强化学习算法结构</h3><p><img src="https://ww1.sinaimg.cn/large/006A69aEgy1g72qzb478ej30ek09vglu.jpg" alt="2019-09-17 19-33-51 的屏幕截图.png"></p><p>通常强化学习算法由3个部分组成</p><ul><li>运行策略进行采样，与环境交互收集数据。</li><li>分析收集到的数据，来提取将来发生的信息，估计收益。</li><li>优化策略。</li></ul><h4 id="基于策略梯度方法"><a href="#基于策略梯度方法" class="headerlink" title="基于策略梯度方法"></a>基于策略梯度方法</h4><ul><li>采样数据</li><li><p>计算收益 $J(\theta)$ = $E_\pi[\sum r_t] \approx \frac{1}{N}\sum_{i=1}^{N}\sum_tr_t^i$</p></li><li><p>优化策略 $\theta \leftarrow \theta + \alpha \bigtriangledown_\theta J(\theta)$</p></li></ul><h4 id="基于模型方法"><a href="#基于模型方法" class="headerlink" title="基于模型方法"></a>基于模型方法</h4><ul><li>采样数据</li><li>根据数据计算模型，$s_{t+1} = f_\phi(s_t, a_t)$, $r_t = g_\theta(s_t, a_t)$</li><li>反向传播优化策略，通过 $f_\phi$ 和 $r$ 计算出合适的 $\pi_\theta(s_t) =a_t$</li></ul><p><img src="https://ww1.sinaimg.cn/large/006A69aEly1gb2tuu7mnqj30z1083aan.jpg" alt="image.png"></p><p>Model-based 方法</p><ul><li>使用模型进行规划<ul><li>在连续空间中，使用最优控制等理论，进行反向传播优化得到动作。</li><li>在离散空间中，进行离散的规划，如蒙特卡洛树搜索。</li></ul></li><li>反向传播梯度到策略中<ul><li>需要一定的trick来保证实施。</li></ul></li><li>使用模型来学习值函数<ul><li>动态规划</li><li>产生模拟的经历，结合Model-Free进行学习，如Dyna</li></ul></li></ul><h3 id="值函数和Q函数"><a href="#值函数和Q函数" class="headerlink" title="值函数和Q函数"></a>值函数和Q函数</h3><script type="math/tex;mode=display">\sum_{t=1}^T E_{(s_t,a_t)\sim p_\theta(s_t, a_t)}[r(s_t, a_t)]</script><p>对收益的计算公式进行分解可以得到一个递归的条件期望表达式</p><script type="math/tex;mode=display">E_{s1\sim p(s1)}[E_{a1\sim\pi(a_1|s_1)}[r(s_1,a_1) + E_{s2\sim p(s2)}[E_{a_2\sim\pi(a_2|s_2)}[r(s_2,a_2) + ...|s2]|s_1,a_1]|s_1]]</script><p>将后面的一系列递归式子定义为Q.</p><script type="math/tex;mode=display">Q(S_1,a_1) = r(s_1,a_1) + E_{s_2\sim p(s_2 | s_1,a_1)}[E_{a_2\sim \pi(a_2|s_2)}[r(s_2,a_2) + ... | s_2] | s_1,a_1]</script><script type="math/tex;mode=display">E_{s_1 \sim p(s_1)}[E_{a_1\sim\pi(a_1|s_1)}|Q(s_1,a_1)|s_1]]</script><p>当$Q(s_1,a_1)$已知时，很容易修改$\pi(a_1|s_1)$来最大化收益。</p><h4 id="Q函数"><a href="#Q函数" class="headerlink" title="Q函数"></a>Q函数</h4><p>在$s_t$状态执行$a_t$开始期望得到的总收益。</p><script type="math/tex;mode=display">Q^\pi(s_t, a_t) = \sum_{t'=t}^T E_{\pi_\theta}[r(s_{t'}, a_{t'})|s_t,a_t]</script><h4 id="值函数"><a href="#值函数" class="headerlink" title="值函数"></a>值函数</h4><p>在$s_t$状态开始期望得到的总收益</p><script type="math/tex;mode=display">V^\pi(s_t) = \sum_{t'=t}^T E_{\pi_\theta}[r(s_{t', }, a_{t'})|s_t]</script><script type="math/tex;mode=display">V^\pi(s_t) = E_{a_t\sim\pi(a_t|s_t)}[Q^\pi(s_t,a_t)]</script><p>$s_1$ 状态的值函数即为强化学习的目标</p><script type="math/tex;mode=display">E_{s_1\sim p(s_1)}[V^\pi(s_1)]</script><h4 id="重要使用方法"><a href="#重要使用方法" class="headerlink" title="重要使用方法"></a>重要使用方法</h4><ul><li>给定策略$\pi$, 在得到 $Q^\pi(s, a)$后，可以优化策略。将每个状态的策略修改为Q值最大的动作，则该策略将不差于原策略。</li><li>使用Q函数来计算梯度，来提升较优动作的概率。如果$Q^\pi(s,a)&gt;V^\pi(s)$则该动作优于平均动作。</li></ul><h3 id="强化学习算法类型"><a href="#强化学习算法类型" class="headerlink" title="强化学习算法类型`"></a>强化学习算法类型`</h3><ul><li><p>policy gradients: 直接优化收益计算表达式。基于样本采样的近似，再计算策略表达式的梯度。</p><p><img src="https://ww1.sinaimg.cn/large/006A69aEgy1gb2uwl8l4aj30wi0eiq49.jpg" alt="image.png"></p></li><li><p>Value-based: 直接估计值函数和Q函数，通过神经网络来计算，使用argmax来优化策略。</p><p><img src="https://ww1.sinaimg.cn/large/006A69aEgy1gb2uvyxvelj30xr0fc0u0.jpg" alt="image.png"></p></li><li><p>Actor-critic: 两者的结合。在得到值函数和Q函数后，通过计算策略表达式的梯度来优化策略</p></li></ul><p><img src="https://ww1.sinaimg.cn/large/006A69aEgy1gb2uxbtqz6j30vm0f4jsl.jpg" alt="image.png"></p><ul><li><p>Model-based RL： 首先估计模型，再提升策略。方法包括使用模型进行规划, 反向传播梯度到策略，使用模型来学习价值函数，使用模型来模拟新的经历。</p><p><img src="https://ww1.sinaimg.cn/large/006A69aEgy1gb2uuupfnuj30ri0fm0tz.jpg" alt="image.png"></p></li></ul><h3 id="强化学习算法多样性原因"><a href="#强化学习算法多样性原因" class="headerlink" title="强化学习算法多样性原因"></a>强化学习算法多样性原因</h3><h4 id="不同的抉择"><a href="#不同的抉择" class="headerlink" title="不同的抉择"></a>不同的抉择</h4><ul><li>采样效率</li><li>稳定性，易用性。(收敛的概率，依赖人为调整超参数)</li></ul><h4 id="不同的前提假设"><a href="#不同的前提假设" class="headerlink" title="不同的前提假设"></a>不同的前提假设</h4><ul><li>随机或确定</li><li>连续或离散</li><li>片段或无限</li></ul><h4 id="不同的问题设定"><a href="#不同的问题设定" class="headerlink" title="不同的问题设定"></a>不同的问题设定</h4><ul><li>容易表示策略</li><li>容易表示环境</li></ul><h3 id="强化学习算法比较"><a href="#强化学习算法比较" class="headerlink" title="强化学习算法比较"></a>强化学习算法比较</h3><h4 id="样本效率"><a href="#样本效率" class="headerlink" title="样本效率"></a>样本效率</h4><p>样本效率指训练得到一个优秀的策略需要多少样本。</p><p>一个重要的抉择在于算法是否是off-policy</p><ul><li><p>off-policy: 可以通过样本来提升策略，而不需要当前策略产生新的样本。</p><blockquote><p>able to improve the policy without generating new samples from that policy.</p></blockquote></li><li><p>on-policy: 每当策略改变时，都需要使用当前策略产生新的样本。</p><blockquote><p>each time the policy is changed, even a little bit, we need to generate new samples.</p></blockquote></li></ul><p><img src="https://ww1.sinaimg.cn/large/006A69aEly1g73skv6974j30oy06i74n.jpg" alt="2019-09-18 17-20-12 的屏幕截图.png"></p><p>on-policy 策略的样本效率低，但可以通过并行算法来增加运行速率。</p><h4 id="稳定性"><a href="#稳定性" class="headerlink" title="稳定性"></a>稳定性</h4><p>稳定性指算法是否收敛，是否每一次都能收敛，收敛到什么值。</p><p>在监督学习中总是通过梯度下降来收敛到误差的较小值。</p><p>在强化学习中并不总是梯度下降。</p><ul><li>Value Function Fitting(Q-leaning): 采用的固定点迭代方法。在理想情况下最小化贝尔曼误差，但不一定保证能收敛。</li><li>Model-based RL: 模型并不是为了优化收益，更精确的模型不一定能带来收益的提高。</li><li>policy gradient： 采用了梯度下降，但是样本效率很低。</li></ul><h4 id="前提假设"><a href="#前提假设" class="headerlink" title="前提假设"></a>前提假设</h4><ul><li>全观测：价值函数拟合方法默认该假设，可以通过添加循环模块来解决。</li><li>片段学习：直接策略梯度方法和一些基于模型方法默认该假设。</li><li>连续和平滑：连续价值函数学习方法和一些基于模型方法默认该假设。</li></ul><h3 id="强化学习算法举例"><a href="#强化学习算法举例" class="headerlink" title="强化学习算法举例"></a>强化学习算法举例</h3><h4 id="价值函数拟合方法"><a href="#价值函数拟合方法" class="headerlink" title="价值函数拟合方法"></a>价值函数拟合方法</h4><ul><li>Q-learning, DQN</li><li>Temporal difference learning</li><li>Fitted value iteration</li></ul><h4 id="策略梯度方法"><a href="#策略梯度方法" class="headerlink" title="策略梯度方法"></a>策略梯度方法</h4><ul><li>REINFORCE</li><li>Natural policy gradient</li></ul><h4 id="演员表演家方法"><a href="#演员表演家方法" class="headerlink" title="演员表演家方法"></a>演员表演家方法</h4><ul><li>Asynchronous Advantage Actor-Critic</li><li>Soft Actor-Critic</li></ul><h4 id="基于模型方法-1"><a href="#基于模型方法-1" class="headerlink" title="基于模型方法"></a>基于模型方法</h4><ul><li>Dyna</li><li>Guided policy search</li></ul><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> RL(CS294) </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CS294-002-监督学习和模仿学习</title>
      <link href="/2019/09/15/CS294-002-%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E5%92%8C%E6%A8%A1%E4%BB%BF%E5%AD%A6%E4%B9%A0/"/>
      <url>/2019/09/15/CS294-002-%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E5%92%8C%E6%A8%A1%E4%BB%BF%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<!-- build time:Sun Jan 19 2020 21:27:23 GMT+0800 (GMT+08:00) --><h3 id="模仿学习"><a href="#模仿学习" class="headerlink" title="模仿学习"></a>模仿学习</h3><h4 id="Behavior-Cloning"><a href="#Behavior-Cloning" class="headerlink" title="Behavior Cloning"></a>Behavior Cloning</h4><p>收集数据 $o_t, a_t$ 进行监督学习得到 $\pi_\theta(a_t|o_t)$</p><h4 id="不足原因"><a href="#不足原因" class="headerlink" title="不足原因"></a>不足原因</h4><ul><li>可以会碰到没见过的数据</li><li>给定的行为不一定是正确的</li><li>在序列决策中，一点细小误差将会逐渐积累成大误差。随着误差的变大，会遇到更多没见过的状态，又会导致更大的误差。</li></ul><p><img src="https://ww1.sinaimg.cn/large/006A69aEly1gb23cjbnkmj30zl0kc0w1.jpg" alt></p><h4 id="本质问题"><a href="#本质问题" class="headerlink" title="本质问题"></a>本质问题</h4><p>Drift Problem, 分布不匹配。训练数据的分布与实际遇到的分布不同。</p><p>模型训练效果不佳，对于没见到过的数据预测能力较差。</p><ul><li>非马尔科夫行为：倾向于过去所做出的决策，决策还依赖于过去。</li><li>多模型行为：面对同一个状态表现出不同的行为。</li></ul><h3 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h3><p>解决方法包括人为调整，获取实际轨迹的分布，修改模型和修改数据等。DAgger为修改数据，解决非马尔科夫行为和多模型行为，从而提高模型的训练效果。</p><h4 id="人为调整"><a href="#人为调整" class="headerlink" title="人为调整"></a>人为调整</h4><p>在自动驾驶中，使用左中右三个摄像头，神经网络的输入为摄像头拍摄到的图片，输出为方向盘转动的角度。对于左摄像头得到的图片，标记的转动角度比正常更朝右一些，右摄像头相反，中间的摄像头不变。这种做法的好处是，当实际测试时决策错误时，汽车的行驶方向发生偏差，所遇到的情况可能仍处在训练集中，则有机会进行校正，从而回到正常的行驶线路上来。<br><img src="https://ww1.sinaimg.cn/large/006A69aEly1gb24jmvyznj30lr0lmdjm.jpg" alt="image.png"></p><h4 id="获取实际轨迹的分布"><a href="#获取实际轨迹的分布" class="headerlink" title="获取实际轨迹的分布"></a>获取实际轨迹的分布</h4><p>对于可能出现的各种情况，除了正常状态外，还对各种异常状况进行标记，作为数据的一部分。如果能得到所有轨迹的分布，则很容易纠正实际出现的误差。</p><p>实现起来很困难。获取大量数据并标记、不现实。一种可行的方法是首先建模环境，接着通过plan模拟实际出现的各种情况。</p><p><img src="https://ww1.sinaimg.cn/large/006A69aEly1gb24t37szaj310x0hftf1.jpg" alt="image.png"></p><h4 id="Dataset-Aggregation"><a href="#Dataset-Aggregation" class="headerlink" title="Dataset Aggregation"></a>Dataset Aggregation</h4><p>将测试集不断加入到训练集中，使得真实遇到的数据的分布等同于训练数据的分布 —&gt; Online Learning</p><p>重复以下过程：</p><ul><li>使用训练集$D$训练策略，得到 $\pi_\theta(a_t|o_t)$</li><li>运行策略得到数据集$D_\pi$</li><li>对数据集$D_\pi$进行人工标记$a_t$</li><li>$D \leftarrow D\bigcup D_\pi$</li></ul><h4 id="解决非马尔科夫行为"><a href="#解决非马尔科夫行为" class="headerlink" title="解决非马尔科夫行为"></a>解决非马尔科夫行为</h4><p>使用所有的观测值作为输入。使用共享的CNN编码特征，再利用RNN进行序列处理。RNN为模型添加了记忆功能。</p><h4 id="解决多模型行为"><a href="#解决多模型行为" class="headerlink" title="解决多模型行为"></a>解决多模型行为</h4><p>多模型行为: 输出决策不是单一固定的。比如遇到障碍，可以从左边绕，也可以从右边绕。</p><p>多模型行为可以分为离散决策和连续决策。解决该问题的出发点时使得模型可以对于同一个状态，输出多个结果。</p><h5 id="离散决策"><a href="#离散决策" class="headerlink" title="离散决策"></a>离散决策</h5><p>在模型最后添加softmax层，输出不同的决策概率。</p><h5 id="连续决策"><a href="#连续决策" class="headerlink" title="连续决策"></a>连续决策</h5><ul><li><p>输出混合的高斯分布。混合密度网络： N个高斯分布之和, $\pi(a|o) =\sum w_i N(\mu_i,\Sigma_i)$</p></li><li><p>隐变量模型。对于输入添加噪声扰动$N(0, 1)$。简单地加入噪声对网络的影响较小。相关方法包括Normalizing flow / realNVP, stein variational gradient descent。</p></li><li><p>自动回归离散化。 对高维的连续变量进行离散化，一次离散化一个维度。训练多个网络，每一次输出减少一个连续变量维度，增加一个相对应的离散变量维度。缺点为较难实现、需要进行架构设计、离散化存在误差。</p></li></ul><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>模仿学习指采用观测值和行为值作为训练数据进行监督学习。</p><p>由于分布偏移问题，模仿学习的效果有时表现不佳。</p><p>改进模仿学习的部分方法:</p><ul><li>人为调整</li><li>从稳定的片分布中采样</li><li>添加更多在线数据，Dagger</li><li>更好的模型</li></ul><p>模仿学习的问题：</p><ul><li>需要人为提供数据。深度学习在数据量大时性能较好。</li><li>人类在某些情况下较难提供正确的判断。如电机电压大小，多臂机器人。</li><li>人类可以自动学习，机器也许也能做到类似的事情。<ul><li>可以有无线的数据。</li><li>连续地自我提升。</li></ul></li></ul><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> RL(CS294) </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>sarsa(λ)推导</title>
      <link href="/2019/09/04/sarsa-%CE%BB-%E6%8E%A8%E5%AF%BC/"/>
      <url>/2019/09/04/sarsa-%CE%BB-%E6%8E%A8%E5%AF%BC/</url>
      
        <content type="html"><![CDATA[<!-- build time:Sun Jan 19 2020 18:07:22 GMT+0800 (GMT+08:00) --><p>在阅读David Silver中提到了 Forward View Sarsa(λ) 和 Backward View Sarsa(λ)， 下面对这两种方法之间的转化进行推导，并比较两者之间的关系。</p><h3 id="Forward-View-Sarsa-λ"><a href="#Forward-View-Sarsa-λ" class="headerlink" title="Forward View Sarsa(λ)"></a>Forward View Sarsa(λ)</h3><p>假设一个片段为</p><script type="math/tex;mode=display">S_t, A_t, R_{t+1}, S_{t+1},A_{t+1},R_{t+2},...,R_{T-1},S_{T-1},A_{T-1},R_{T},S_T</script><p>在估计 $S_t, A_t$ 的价值时 Sarsa 采用的是</p><script type="math/tex;mode=display">R_{t+1} + \gamma Q(S_{t+1},A_{t+1})</script><p>n-step Sarsa采用的是</p><script type="math/tex;mode=display">q_t^{(n)} = R_{t+1} + \gamma R_{t+2} + ... + \gamma^{n-1}R_{t+n} + \gamma^n Q(S_{t+n}, A_{t+n})</script><p>对该片段展开得</p><script type="math/tex;mode=display">\begin{aligned}q_t^{(1)} & = R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) \\q_t^{(2)} & = R_{t+1} + \gamma R_{t+2} + \gamma^2 Q(S_{t+2}, A_{t+2}) \\...\\q_t^{(T-t -1)}&=R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + ... + \gamma^{T-t-2}R_{T-1} +\gamma^{T-t-1}Q(S_{T-1},A_{T-1}) \\q_t^{T-t} &=R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3}  + ... + \gamma^{T-t-2}R_{T-1} + \gamma^{T-t-1}R_T\end{aligned}</script><p>Sarsa(λ) 对 所有的 $q_t^{n}$ 进行加权</p><script type="math/tex;mode=display">q_t^\lambda = (1-\lambda)q_t^{(1)} + (1-\lambda)\lambda q_t^{(2)} + (1-\lambda)\lambda^2 q_t^{(3)} + ... + (1-\lambda)\lambda^{T-t-2}q_t^{T-t-1} + \lambda^{T-t-1}q_t^{T-t}</script><p>对该片段，并合并同类项得</p><script type="math/tex;mode=display">\begin{aligned}q_t^\lambda = &[(1-\lambda)(1 + \lambda+\lambda^2+...+\lambda^{T-t-2} )+\lambda^{T-t-1}] R_{t+1} + \\&[(1-\lambda)(\lambda + \lambda^2 + ...+ \lambda^{T-t-2}) + \lambda^{T-t-1}]\gamma R_{t+2} + \\& ...\\&[(1-\lambda)(\lambda^{T-t-2}) + \lambda^{T-t-1}]\gamma^{T-t-2}R_{T-1}  + \\&\lambda^{T-t-1}\gamma^{T-t-1}R_{T} +\\&(1-\lambda)\gamma Q(S_{t+1},A_{t+1}) + \\&(1-\lambda)\lambda\gamma^2Q(S_{t+2}, A_{t+2}) + \\&... \\&(1-\lambda)\lambda^{T-t-2}\gamma^{T-t-1}Q(S_{T-1}, A_{T-1})\end{aligned}</script><p>利用等比数列公式化简可得</p><script type="math/tex;mode=display">\begin{aligned}q_t^\lambda = &R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) +\\&\lambda\gamma (R_{t+2} + \gamma Q(S_{t+2}, A_{t+2} - Q(S_{t+1}, A_{t+1})) +\\&...\\& \lambda^{T-t-2}\gamma^{T-t-2}(R_{T-1}+\gamma Q(S_{T-1}, A_{T-1}) -Q(S_{T-2}, A_{T-2}) +\\&\lambda^{T-t-1}\gamma^{T-t-1}R_{T}\end{aligned}</script><p>对于$S_t, A_t$ 的TD更新误差为</p><script type="math/tex;mode=display">\delta_t = R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)</script><p>则对于$S_t, A_t$ 的 Sarsa(λ)的更新误差为</p><script type="math/tex;mode=display">\begin{aligned}\Delta_t &= q_t^\lambda  - Q(S_t, A_t) \\&= \delta_t + \gamma\lambda \delta_{t+1} + ... + \lambda^{T-t-2}\gamma^{T-t-2}\delta_{T-2} + \gamma^{T-t-1}\gamma^{T-t-1}\delta_{T-1}\end{aligned}</script><h3 id="Backward-View-Sarsa-λ"><a href="#Backward-View-Sarsa-λ" class="headerlink" title="Backward View Sarsa(λ)"></a>Backward View Sarsa(λ)</h3><p>从上述误差计算公式可以看出Forward View Sarsa(λ)不需要在计算完整个片段之后再计算，可以和TD一样，每计算完一次误差，对前面所有经过的状态的误差进行更新。</p><p>对于每一个误差的权重，初始值为1，每经过一次，衰减$\gamma\lambda$，即Eligibility_trace的作用。</p><p><img src="https://ww1.sinaimg.cn/large/006A69aEly1g5scilvdj6j30i60aatbw.jpg" alt="image"></p><h3 id="Backward-View-和-Forward-View-比较"><a href="#Backward-View-和-Forward-View-比较" class="headerlink" title="Backward View 和 Forward View 比较"></a>Backward View 和 Forward View 比较</h3><p>在off-line中，即优化的策略和选取下一个动作的策略是不同时，Backward View 和 Forward View是相同的。</p><p>而在on-line中，两者有轻微的区别。因为对于前面到达的状态的估计值是不完全的，当且仅当整个片段完成后，对一个状态的更新才完成。若在片段中可能会使用不完全的估计值来选取下一个动作。</p><p>当 λ = 0 时，则 Sarsa(λ) 和 TD 完全相同，Backward View 和 Forward View也完全相同。</p><p>当 λ = 1时， 则 Forward View Sarsa(λ) 和 MC 相同。</p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> RL(David Silver) </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>007-Policy Gradient</title>
      <link href="/2019/08/28/007-Policy-Gradient/"/>
      <url>/2019/08/28/007-Policy-Gradient/</url>
      
        <content type="html"><![CDATA[<!-- build time:Sun Jan 19 2020 18:07:22 GMT+0800 (GMT+08:00) --><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><h4 id="Advantage"><a href="#Advantage" class="headerlink" title="Advantage"></a>Advantage</h4><p>Advantages:</p><ul><li>Better convergence properties</li><li>Effective in high-dimensional or continuous action spaces</li><li>Can learn stochastic policies</li></ul><p>Disadvantages:</p><ul><li>Typically converge to a local rather than global optimum</li><li>Evaluating a policy is typically inefficient and high variance</li></ul><h4 id="policy-objective-function"><a href="#policy-objective-function" class="headerlink" title="policy objective function"></a>policy objective function</h4><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> RL(David Silver) </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>006-Value Function Approximation</title>
      <link href="/2019/08/28/006-Value-Function-Approximation/"/>
      <url>/2019/08/28/006-Value-Function-Approximation/</url>
      
        <content type="html"><![CDATA[<!-- build time:Sun Jan 19 2020 18:07:22 GMT+0800 (GMT+08:00) --><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><ul><li>Estimate value function with function approximation<script type="math/tex;mode=display">\hat v(s, w) ≈ v_\pi(s) \\ \hat q(s, a, w) ≈ q_\pi(s, a)</script></li><li>Generalise from seen states to unseen states<br>Update parameter w using MC or TD learning</li></ul><p><img src="https://ww1.sinaimg.cn/large/006A69aEly1g5zeudlp2lj30o80cuq3m.jpg" alt></p><h4 id="Objective-Function"><a href="#Objective-Function" class="headerlink" title="Objective Function"></a>Objective Function</h4><script type="math/tex;mode=display">J(w) = E_\pi[(v_\pi(S) - \hat v(S, w))^2]</script><h4 id="Least-Squares-Prediction"><a href="#Least-Squares-Prediction" class="headerlink" title="Least Squares Prediction"></a>Least Squares Prediction</h4><p>experice</p><script type="math/tex;mode=display">D = \{\lt s_1, v_1^\pi\gt, \lt s_2, v_2^\pi\gt, ... , \lt s_T, v_T^\pi\gt\}</script><p>loss</p><script type="math/tex;mode=display">LS(w) = \sum_{t=1}^T(v_t^\pi - \hat v(s_t, w))^2</script><h3 id="Deep-Q-Network"><a href="#Deep-Q-Network" class="headerlink" title="Deep Q-Network"></a>Deep Q-Network</h3><ul><li>DQN uses experience replay and fixed Q-targets</li><li>Take action $a_t$ according to $\epsilon$-greedy policy</li><li>Store transition $(s_t, a_t, r_{t+1}, s_{t+1})$ in replay memory D</li><li>Sample random mini-batch of transitions $(s, a, r, s’)$ from D</li><li>Compute Q-learning targets w.r.t. old, fixed parameters $w-$</li><li>Optimise MSE between Q-network and Q-learning targets<script type="math/tex;mode=display">L_i(w_i) = E_{s, a, r, s' \sim D_i}[(r + \gamma \max_{a'}Q(s', a'; w_i^-)-Q(s, a; w_i))^2]</script></li><li>Using variant of stochastic gradient descent</li></ul><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> RL(David Silver) </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>005-Model Free Control</title>
      <link href="/2019/08/28/005-Model-Free-Control/"/>
      <url>/2019/08/28/005-Model-Free-Control/</url>
      
        <content type="html"><![CDATA[<!-- build time:Sun Jan 19 2020 18:07:22 GMT+0800 (GMT+08:00) --><p>optimise the valuefunction of an unknown MDP</p><h3 id="On-policy-and-Off-policy"><a href="#On-policy-and-Off-policy" class="headerlink" title="On-policy and Off-policy"></a>On-policy and Off-policy</h3><ul><li>On-policy learning<ul><li>Learn on the job</li><li>Learn about policy π from experience sampled from π</li></ul></li><li>Off-policy learning<ul><li>Look over someone’s shoulder</li><li>Learn about policy π from experience sampled from µ</li></ul></li></ul><h3 id="On-policy-Monte-Carlo-Learning"><a href="#On-policy-Monte-Carlo-Learning" class="headerlink" title="On-policy Monte-Carlo Learning"></a>On-policy Monte-Carlo Learning</h3><h4 id="epsilon-Greedy-Exploration"><a href="#epsilon-Greedy-Exploration" class="headerlink" title="$\epsilon$-Greedy Exploration"></a>$\epsilon$-Greedy Exploration</h4><script type="math/tex;mode=display">\pi(a|s)=\left\{\begin{aligned}&\epsilon / m + 1 - \epsilon \;& if a^* = \arg\max_{a\in A} Q(s, a)\\&\epsilon / m    & otherwise\end{aligned}\right.</script><h4 id="Monte-Carlo-Policy-Iteration"><a href="#Monte-Carlo-Policy-Iteration" class="headerlink" title="Monte-Carlo Policy Iteration"></a>Monte-Carlo Policy Iteration</h4><ul><li>Policy Evaluation: Monte-Carlo policy Evalutaion, $Q=q_\pi$</li><li>Pollicy Improvement: $\epsilon$-greedy policy improvement</li></ul><h4 id="GLIE-Monte-Carlo-Control-Greedy-in-the-Limit-with-Infinite-Exploration"><a href="#GLIE-Monte-Carlo-Control-Greedy-in-the-Limit-with-Infinite-Exploration" class="headerlink" title="GLIE Monte-Carlo Control(Greedy in the Limit with Infinite Exploration)"></a>GLIE Monte-Carlo Control(Greedy in the Limit with Infinite Exploration)</h4><ul><li>对于使用$\pi$的第k次采样序列: $\{S_1, A_1, R_2, …, S_T\}$</li><li>更新每一个状态和动作<script type="math/tex;mode=display">\begin{aligned}N(S_t, A_t) &\leftarrow N(S_t, A_t) + 1 \\Q(S_t, A_r) &\leftarrow Q(S_t, A_t) + \frac{1}{N(S_t, A_t)}(G_t - Q(S_t, A_t))\end{aligned}</script></li><li>优化策略<script type="math/tex;mode=display">\begin{aligned}\epsilon &\leftarrow 1 / k \\\pi & \leftarrow \epsilon-greedy(Q)\end{aligned}</script></li></ul><h3 id="On-Policy-Temporal-Difference-Learning"><a href="#On-Policy-Temporal-Difference-Learning" class="headerlink" title="On-Policy Temporal-Difference Learning"></a>On-Policy Temporal-Difference Learning</h3><h4 id="Sarsa"><a href="#Sarsa" class="headerlink" title="Sarsa"></a>Sarsa</h4><script type="math/tex;mode=display">Q(S, A) \leftarrow Q(S, A) + \alpha(R + \gamma Q(S', A') - Q(S, A)</script><p><img src="https://ww1.sinaimg.cn/large/006A69aEly1g5zckgpzehj30j1078whp.jpg" alt></p><h4 id="n-step-Sarsa"><a href="#n-step-Sarsa" class="headerlink" title="n-step Sarsa"></a>n-step Sarsa</h4><p>Q-return</p><script type="math/tex;mode=display">q_t(n) = R_{t+1} + \gamma R_{t+2} + ... + \gamma ^ {n - 1} R_{t + n} + \gamma^n Q(S_{t+n})</script><p>n-step Sarsa</p><script type="math/tex;mode=display">Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha(q_t^{(n)} - Q(S_t, A_t))</script><h4 id="Forward-View-Sarsa-lambda"><a href="#Forward-View-Sarsa-lambda" class="headerlink" title="Forward View Sarsa($\lambda$)"></a>Forward View Sarsa($\lambda$)</h4><script type="math/tex;mode=display">q_t^\lambda = (1 - \lambda)\sum_{n=1}^\infty \lambda^{n-1}q_t^{(n)}</script><p>Sarsa($\lambda$)</p><script type="math/tex;mode=display">Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha(q_t^{\lambda} - Q(S_t, A_t))</script><h4 id="Backward-View-Sarsa-lambda"><a href="#Backward-View-Sarsa-lambda" class="headerlink" title="Backward View Sarsa($\lambda$)"></a>Backward View Sarsa($\lambda$)</h4><p>Eligible Trace</p><script type="math/tex;mode=display">E_t(s, a) = \gamma\lambda E_{t-1}(s, a) + I(S_t=s, A_t=a)</script><p>Update for every state s and action a</p><script type="math/tex;mode=display">\begin{aligned}\delta_t &= R_{t+1}+\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t) \\ Q(s, a) &\leftarrow Q(s, a) + \alpha \delta_t E_t(s, a)\end{aligned}</script><h4 id="Backward-View-Sarsa-lambda-Algorithm"><a href="#Backward-View-Sarsa-lambda-Algorithm" class="headerlink" title="Backward View Sarsa($\lambda$) Algorithm"></a>Backward View Sarsa($\lambda$) Algorithm</h4><p><img src="https://ww1.sinaimg.cn/large/006A69aEly1g5scilvdj6j30i60aatbw.jpg" alt="image"></p><h3 id="Off-Policy-Learning"><a href="#Off-Policy-Learning" class="headerlink" title="Off-Policy Learning"></a>Off-Policy Learning</h3><ul><li>evaluate target policy $\pi(a|s)$ to compute $v_\pi(s)$ or $q_\pi(s, a)$</li><li>While following behaviour policy $\mu(a|s)$<script type="math/tex;mode=display">\{S_1, A_1, R_2, ..., S_T \} ∼ \mu</script></li></ul><h4 id="Importance-Sampling-for-off-policy-Monte-Carlo"><a href="#Importance-Sampling-for-off-policy-Monte-Carlo" class="headerlink" title="Importance Sampling for off-policy Monte-Carlo"></a>Importance Sampling for off-policy Monte-Carlo</h4><script type="math/tex;mode=display">G_t^{\pi/\mu}=\frac{\pi(A_t|S_t)\pi(A_{t+1}|S_{t+1})}{\mu(A_t|S_t)\mu(A_{t+1}|S_{t+1})}...\frac{\pi(A_T|S_T)}{\mu(A_T|S_T)}G_t</script><script type="math/tex;mode=display">V(S_t) \leftarrow V(S_t) + \alpha(G_t^{\pi/\mu}-V(S_t))</script><p>Importance sampling can dramatically increase variance</p><h4 id="Importance-Sampling-for-off-policy-TD"><a href="#Importance-Sampling-for-off-policy-TD" class="headerlink" title="Importance Sampling for off-policy TD"></a>Importance Sampling for off-policy TD</h4><script type="math/tex;mode=display">V(S_t) \leftarrow V(S_t) + \alpha(\frac{\pi(A_t|S_t)}{\mu(A_t|S_t)}(R_{t+1}+\gamma V(S_{t+1})-V(S_t))</script><h4 id="Q-learning"><a href="#Q-learning" class="headerlink" title="Q-learning"></a>Q-learning</h4><ul><li>Next action is chosen using behavior policy $A_{t+1} \sim \mu(·|S_t) $</li><li>Consider Alternative successor action $A’ \sim \pi(·|S_t)$</li></ul><script type="math/tex;mode=display">Q(S_t, A_t) \leftarrow Q(S_t, A_r) + \alpha(R_{t+1} + \gamma Q(S_{t+1}, A') - Q(S_t, A_t))</script><h4 id="Off-policy-Q-learning"><a href="#Off-policy-Q-learning" class="headerlink" title="Off-policy Q-learning"></a>Off-policy Q-learning</h4><script type="math/tex;mode=display">Q(S, A) \leftarrow Q(S, A) + \alpha(R + \gamma \max_a'Q(S', a') - Q(S, A))</script><p><img src="https://ww1.sinaimg.cn/large/006A69aEly1g5zcice89cj30j406lwgt.jpg" alt></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> RL(David Silver) </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>004-Model Free Prediction</title>
      <link href="/2019/08/28/004-Model-Free-Prediction/"/>
      <url>/2019/08/28/004-Model-Free-Prediction/</url>
      
        <content type="html"><![CDATA[<!-- build time:Sun Jan 19 2020 18:07:22 GMT+0800 (GMT+08:00) --><p>Estimate the value function of an unknown MDP</p><h3 id="Monte-Carlo-Reinforcement-Learning"><a href="#Monte-Carlo-Reinforcement-Learning" class="headerlink" title="Monte-Carlo Reinforcement Learning"></a>Monte-Carlo Reinforcement Learning</h3><ul><li>Learn directly from episodes of experiecnce</li><li>model-free</li><li>complete episodes: no bootstraping</li><li>simplest possible idea: value = mean return</li></ul><h4 id="First-visit-Monte-Carlo"><a href="#First-visit-Monte-Carlo" class="headerlink" title="First visit Monte-Carlo"></a>First visit Monte-Carlo</h4><ul><li>evaluate state s</li><li><strong>first</strong> time-step t that state is visited in an episode</li><li>$N(s) \leftarrow N(s) + 1$</li><li>$S(s) \leftarrow S(s) + G_t$</li><li>$V(s) = S(s) / N(s)$</li></ul><h4 id="Every-visit-Monte-Carlo"><a href="#Every-visit-Monte-Carlo" class="headerlink" title="Every-visit Monte-Carlo"></a>Every-visit Monte-Carlo</h4><ul><li>evaluate state s</li><li><strong>every</strong> time-step t that state is visited in an episode</li><li>$N(s) \leftarrow N(s) + 1$</li><li>$S(s) \leftarrow S(s) + G_t$</li><li>$V(s)\;=\;S(s) / N(s)$</li></ul><h4 id="Incremental-Mean"><a href="#Incremental-Mean" class="headerlink" title="Incremental Mean"></a>Incremental Mean</h4><script type="math/tex;mode=display">\begin{aligned}\mu_k &= \frac{1}{k}\sum_{j=1}^k x_j \\      &= \frac{1}{k}(x_k + (k-1)\mu_{k-1}) \\      &= \mu_{k-1} + \frac{1}{k}(x_k - \mu_{k-1})\end{aligned}</script><h4 id="Incremental-Monte-Carlo-Updates"><a href="#Incremental-Monte-Carlo-Updates" class="headerlink" title="Incremental Monte-Carlo Updates"></a>Incremental Monte-Carlo Updates</h4><p>Update $V(s)$ incrementally after episode $S_1, A_1, R_1, …, S_T$</p><script type="math/tex;mode=display">\begin{aligned}&N(S_t) \leftarrow N(S_t) + 1 \\&V(S_t) \leftarrow V(S_t) + \frac{1}{N(S_t)}(G_t - V(S_t)\end{aligned}</script><p>forget old episodes</p><script type="math/tex;mode=display">V(S_t) \leftarrow V(S_t) + \alpha(G_t - V(S_t))</script><h3 id="Timeporal-Difference-Learning"><a href="#Timeporal-Difference-Learning" class="headerlink" title="Timeporal Difference Learning"></a>Timeporal Difference Learning</h3><ul><li>Learn directly from episodes of experiecnce</li><li>model-free</li><li>incomplete episodes: by bootstraping</li><li>update a guess towards a guess</li></ul><h4 id="TD-0"><a href="#TD-0" class="headerlink" title="TD(0)"></a>TD(0)</h4><script type="math/tex;mode=display">\begin{aligned}\delta &= R_{t+1}+\gamma V(S_{t+1}) - V(S_t)\\V(S_t) &\leftarrow V(S_t) + \alpha * \delta\end{aligned}</script><h4 id="Forward-View-TD-lambda"><a href="#Forward-View-TD-lambda" class="headerlink" title="Forward View TD($\lambda$)"></a>Forward View TD($\lambda$)</h4><p>Compelete Episodes</p><p>N-step Return</p><script type="math/tex;mode=display">G_t^{(n)} = R_{t+1} + \gamma R_{t+2} + ... + \gamma ^{n-1}R_{t+n}+\gamma^n V(S_{t+n})</script><p>$\lambda$-Return</p><script type="math/tex;mode=display">G_t^{\lambda} = (1-\lambda)\sum_{n=1}^{\infin}\lambda^{n-1}G_t^{(n)}</script><p>Forward view TD($\lambda$)</p><script type="math/tex;mode=display">V(S_t) \leftarrow V(S_t) + \alpha(G_t^\lambda - V(S_t))</script><h4 id="Backward-View-TD-lambda"><a href="#Backward-View-TD-lambda" class="headerlink" title="Backward View TD($\lambda$)"></a>Backward View TD($\lambda$)</h4><ul><li>Forward view provides theory</li><li>Backward view provides mechanism</li><li>Update online, every step, from incomplete sequences</li></ul><p>Eligibility trace</p><script type="math/tex;mode=display">\begin{aligned}E_0(s) &= 0 \\E_t(s) &= \gamma\lambda E_{t-1}(s)+I(S_t=s) \end{aligned}</script><p>Backward View TD($\lambda$)</p><ul><li>Keep an eligibility trace for every state s</li><li>Update value V(s) for every state s<script type="math/tex;mode=display">\begin{aligned}\delta_t &= R_{t+1}+\gamma V(S_{t+1}) - V(S_t) \\ V(S) &\leftarrow V(S) + \alpha\delta_t E_t(S)\end{aligned}</script></li></ul><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> RL(David Silver) </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>003-Planning by Dynamic Programming</title>
      <link href="/2019/08/28/003-Planning-by-Dynamic-Programming/"/>
      <url>/2019/08/28/003-Planning-by-Dynamic-Programming/</url>
      
        <content type="html"><![CDATA[<!-- build time:Sun Jan 19 2020 18:07:22 GMT+0800 (GMT+08:00) --><h3 id="Planning"><a href="#Planning" class="headerlink" title="Planning"></a>Planning</h3><p>Dynamic programming assumes full knowledge of the MDP</p><ul><li>Prediction<ul><li>Input: MDP $\lt S, A, P, R, \gamma\gt$ policy $\pi$</li><li>Input: MRP $\lt S, P^\pi, R^\pi, \gamma\gt$</li><li>Output: value function $V_\pi$</li></ul></li><li>Control<ul><li>Input: MDP $ \lt S, A, P, R, \gamma\gt $</li><li>Output: optimal value function $V_*$</li><li>Output: optimal policy $\pi_*$</li></ul></li></ul><h3 id="Policy-Evaluation"><a href="#Policy-Evaluation" class="headerlink" title="Policy Evaluation"></a>Policy Evaluation</h3><ul><li>Problem: evaluate a given policy $\pi$</li><li>Solution: iterative application of Bellman expectation backup</li><li>$V_1 \rightarrow V_2 \rightarrow … \rightarrow V_\pi$</li><li>synchronout backups<ul><li>At each iteration $k+1$</li><li>For all states $s\in S$</li><li>Update $V_{k+1}(s)$ from $V_k(s’)$</li></ul></li></ul><h3 id="Policy-Iteration"><a href="#Policy-Iteration" class="headerlink" title="Policy Iteration"></a>Policy Iteration</h3><ul><li>Evaluate the policy $\pi$</li><li>imporve the policy by acting greedily with respect to $V_\pi$<script type="math/tex;mode=display">\pi' = greedy(V_\pi)</script>this process of policy iteration always converges to $\pi_*$</li></ul><h3 id="Value-Iteration"><a href="#Value-Iteration" class="headerlink" title="Value Iteration"></a>Value Iteration</h3><ul><li>Problem: find optimal policy $\pi$</li><li>Solution: iterative application of Bellman opimality backup</li><li>$V_1 \rightarrow V_2 \rightarrow … \rightarrow V_*$</li><li>sychronous backup<ul><li>At each iteration $k+1$</li><li>For all state $s\in S$</li><li>Update $V_{k+1}(s)$ from $V_k(s’)$</li></ul></li></ul><h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><div class="table-container"><table><thead><tr><th>Problem</th><th>Bellman Equation</th><th>Algorithm</th></tr></thead><tbody><tr><td>Prediction</td><td>Bellman Expectation Equation</td><td>Iterative Policy Evaluation</td></tr><tr><td>Control</td><td>Bellman Expection Equation + Greedy Policy Improvement</td><td>Policy Iteration</td></tr><tr><td>Control</td><td>Bllman Optimaliry Equation</td><td>Value Iteration</td></tr></tbody></table></div><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> RL(David Silver) </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>002-Markov Decision Process</title>
      <link href="/2019/08/28/002-Markov-Decision-Process/"/>
      <url>/2019/08/28/002-Markov-Decision-Process/</url>
      
        <content type="html"><![CDATA[<!-- build time:Sun Jan 19 2020 18:07:22 GMT+0800 (GMT+08:00) --><h3 id="Markov-Process"><a href="#Markov-Process" class="headerlink" title="Markov Process"></a>Markov Process</h3><h4 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h4><p>Markov decision process formally describe an environment for reinforcement learning.</p><p>The environment is fully observable</p><p>All RL problems can be formalised as MDPs</p><h4 id="Markov-Property"><a href="#Markov-Property" class="headerlink" title="Markov Property"></a>Markov Property</h4><p>The future is independent of the past given the present.</p><script type="math/tex;mode=display">P[S_{t+1} | S_t] = P[S_{t+1} | S1, ... , S_t]</script><h4 id="Markov-Process-1"><a href="#Markov-Process-1" class="headerlink" title="Markov Process"></a>Markov Process</h4><p>Markov Process is a tuple $ \lt S, P\gt $</p><ul><li>S is a finite set of states</li><li>P is a state transition probability matrix</li></ul><h3 id="Markov-Reward-Process"><a href="#Markov-Reward-Process" class="headerlink" title="Markov Reward Process"></a>Markov Reward Process</h3><p>A Markov reward process is a Markov chain with values</p><p>A Markov Process is a tuple $ \lt S, P, R, \gamma \gt $</p><ul><li>S is a finite set of states</li><li>P is a state transition probability matrix $P_{ss’} = P[S_{t+1} =s’ | S_t = s]$</li><li>R is a reward function $R_s=E[R_{t+1} | S_t=s]$</li><li>$\gamma$ is a discount factor</li></ul><h4 id="Return"><a href="#Return" class="headerlink" title="Return"></a>Return</h4><p>return is the total discounted reward from time-step t</p><script type="math/tex;mode=display">G_t = R_{t+1} + \gamma R_{t+2} + ... = \sum_{k=0}^{\infin}\gamma^k R_{t+k+1}</script><p>Discount Reason:</p><ul><li>数值上方便</li><li>避免计算循环</li><li>未来的不确定性</li><li>人类更偏向于立即收益</li></ul><h4 id="Value-function"><a href="#Value-function" class="headerlink" title="Value function"></a>Value function</h4><p>the expected return stating from state s.</p><script type="math/tex;mode=display">v(s)=E[G_t|S_t=s] \\v(s) = R_s + \gamma * \sum_{s'\in S}P_{ss'}v(s')</script><h3 id="Markov-Decision-Process"><a href="#Markov-Decision-Process" class="headerlink" title="Markov Decision Process"></a>Markov Decision Process</h3><p>A markov decision process is a Markov reward process with decisions. It is an environment in which all states are Markov.</p><p>A markov decision processs is a tuple $ \lt S, A, P, R, \gamma\gt $</p><ul><li>S is a finite set of states</li><li>A is a finite set of actions</li><li>P is a state transition probability matrix $P_{ss’}^a = P[S_{t+1} =s’ | S_t = s, A_t=a]$</li><li>R is a reward function $R_s^a=E[R_{t+1} | S_t=s, A_t=a]$</li><li>$\gamma$ is a discount factor</li></ul><h4 id="Policy"><a href="#Policy" class="headerlink" title="Policy"></a>Policy</h4><p>A policy $\pi$ is a distribution over actions givens states.</p><script type="math/tex;mode=display">\pi(a|s) = P[A_t=a|S_t=s]</script><p>Given an MDP, M = $ \lt S, A, P, R, \gamma\gt $ and a policy $\pi$</p><p>The state sequence is a Markov process $ \lt S, P^\pi\gt $</p><p>The state and reward sequence is a Markov reward process $ \lt S, P^\pi, R^\pi, \gamma\gt $</p><script type="math/tex;mode=display">P_{s, s'}^{\pi} = \sum_{a\in A}\pi(a|s)P_{ss'}^a \\R_s^{\pi} = \sum_{a\in A}\pi(a|s)R_s^a</script><h4 id="Value-function-1"><a href="#Value-function-1" class="headerlink" title="Value function"></a>Value function</h4><p>The state-value function of an MDP is the expected return starting from state S and then following policy $\pi$</p><script type="math/tex;mode=display">V\pi(s) = E_\pi[G_t|S_t=s]</script><p>The action-value function of an MDP is the expected return staring from state S, taking action a and then following policy $\pi$</p><script type="math/tex;mode=display">q_\pi(s, a) = E\pi[G_t|S_t=s, A_t=a]</script><p>Bellman Expectation Equation:</p><script type="math/tex;mode=display">V_\pi(s) = \sum_{a\in A}\pi(a|s)q_\pi(s,a)=\sum_{a\in A}\pi(a|s)[R_s^a+\gamma \sum_{s'\in S}P_{ss'}^a V_\pi(s')]</script><script type="math/tex;mode=display">q_{\pi}(s, a) = R_s^a + \gamma \sum_{s'\in S}P_{ss'}^a V_\pi(s') = R_s^a + \gamma \sum_{s'\in S}P_{ss'}^a\sum_{a'\in A}\pi(a'|s')q_\pi(s', a')</script><h4 id="Optimal-Value-Function"><a href="#Optimal-Value-Function" class="headerlink" title="Optimal Value Function"></a>Optimal Value Function</h4><p>The optimal value function is the maximum value function over all policies.</p><script type="math/tex;mode=display">V_*(s) = \max_\pi V_\pi(s) \\ q_*(s, a) = \max_\pi q\pi(s, a)</script><p>Bellman Optimality Equation</p><script type="math/tex;mode=display">V_*(s) = \max_a q_*(s,a) = \max_a R_s^a + \gamma \sum_{s'\in S}P_{ss'}^a V_*(s') \\q_*(s,a) = R_s^a + \gamma\sum_{s'\in S} P_{ss'}^a \max_{a'} q_*(s', a')</script><p>methods</p><ul><li>Value iteration</li><li>policy iteration</li><li>Q-learning</li><li>Sarsa</li></ul><h3 id="Partially-Observable-MDPs"><a href="#Partially-Observable-MDPs" class="headerlink" title="Partially Observable MDPs"></a>Partially Observable MDPs</h3><p>A Partially Observable MDPs is a MDP with hidden states. It is a hidden Markov model with actions.</p><p>A PDMDP is a tuple ‘$ \lt S, A, O, P, R, Z, \gamma\gt $</p><ul><li>S is a finite set of states</li><li>A is a finite set of actions</li><li>O is a finit set of observations</li><li>P is a state transition probability matrix $P_{ss’}^a = P[S_{t+1} =s’ | S_t = s, A_t=a]$</li><li>R is a reward function $R_s^a=E[R_{t+1} | S_t=s, A_t=a]$</li><li>Z is an observation function, $Z_{s’o}^a=P[O_{t+1}=o|S_{t+1} =s’, A_t=a$</li><li>$\gamma$ is a discount factor</li></ul><h4 id="Belief-States"><a href="#Belief-States" class="headerlink" title="Belief States"></a>Belief States</h4><p>A belief state b(h) is a probability distribution over states, conditioned on the history h</p><script type="math/tex;mode=display">b(h) = (P[S_t=s^1|H_t=h), ... , P(S_t=s^n | H_t=h)</script><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> RL(David Silver) </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>001-Introduction to Reinforment Learning</title>
      <link href="/2019/08/28/001-Introduction-to-Reinforment-Learning/"/>
      <url>/2019/08/28/001-Introduction-to-Reinforment-Learning/</url>
      
        <content type="html"><![CDATA[<!-- build time:Sun Jan 19 2020 18:07:22 GMT+0800 (GMT+08:00) --><h3 id="RL-Problem"><a href="#RL-Problem" class="headerlink" title="RL Problem"></a>RL Problem</h3><p>Reinforcement Learning is based on the <strong>reward hypothesis</strong></p><blockquote><p>All goals can be described by the maximisation of expected cumulative reward</p></blockquote><h4 id="History-and-state"><a href="#History-and-state" class="headerlink" title="History and state"></a>History and state</h4><p>The history is the sequence of observations, actions, rewards</p><script type="math/tex;mode=display">H_t = O_1, R_1, A_1,...,A_{t-1},O_t,R_t</script><p>State is the information used to determine what happens next. State is a function of the history.</p><script type="math/tex;mode=display">S_t = f(H_t)</script><ul><li>Environment State</li><li>Agent State</li></ul><ul><li>Full Observability —&gt; ES = AS —&gt; Markov decision process</li><li>Partial Observability —&gt; ES ≠ AS —&gt; Partially observable MDP</li></ul><h3 id="RL-Agent"><a href="#RL-Agent" class="headerlink" title="RL Agent"></a>RL Agent</h3><p>Three Compoents:</p><h4 id="Pollcy"><a href="#Pollcy" class="headerlink" title="Pollcy"></a>Pollcy</h4><ul><li>agent’s behavior function</li><li>map from state to action<script type="math/tex;mode=display">a = \pi(s) \\\pi(a|s)=P[A_t = a | S_t = s]</script></li></ul><h4 id="Value-function"><a href="#Value-function" class="headerlink" title="Value function"></a>Value function</h4><ul><li>how good is each state and/or action</li><li>used to evaluate state<script type="math/tex;mode=display">V_\pi(s)=E\pi[R_{t+1}+\gamma R_{t+2}+\gamma ^2R_{t+3}+...|S_t=s]</script></li></ul><h4 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h4><ul><li>agent’s representation of the environment</li><li>predict what the environment will do next</li><li>P predict the next state</li><li>R predict the next reward<script type="math/tex;mode=display">P_{ss'}^a=P[S_{t+1}=s'|S_t=s,A_t=a] \\R_s^a=E[R_{t+1}|S_t=s,A_t=a]</script><h4 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h4><img src="https://ww1.sinaimg.cn/large/006A69aEly1g5nyslmwfvj30gs0exq52.jpg" alt="image"></li></ul><h3 id="Problems-with-RL"><a href="#Problems-with-RL" class="headerlink" title="Problems with RL"></a>Problems with RL</h3><h4 id="Learning-and-Planning"><a href="#Learning-and-Planning" class="headerlink" title="Learning and Planning"></a>Learning and Planning</h4><ul><li>Reinforcement Learning<ul><li>Environment is unknown</li><li>agent interacts with the environment</li><li>agent imporves its policy</li></ul></li><li>Planning<ul><li>model of the environment is known</li><li>agent performs computation with its model</li><li>agent imporves its policy</li></ul></li></ul><h4 id="Exploration-and-Exploitation"><a href="#Exploration-and-Exploitation" class="headerlink" title="Exploration and Exploitation"></a>Exploration and Exploitation</h4><ul><li>Exploration: finds more information about the environment</li><li>Exploitation: exploit known information to maximise reward</li></ul><h4 id="Prediction-and-Control"><a href="#Prediction-and-Control" class="headerlink" title="Prediction and Control"></a>Prediction and Control</h4><ul><li>Prediction: evaluate the future / Given the policy<ul><li>Policy —&gt; Value function</li></ul></li><li>Control: optimise the future / Find the best policy<ul><li>Optimal Value function and Optimal Policy</li></ul></li></ul><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> RL(David Silver) </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RL </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
