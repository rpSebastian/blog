---
title: CS294-002-监督学习和模仿学习
mathjax: true
date: 2019-09-15 16:41:00
categories: RL(CS294)
tags: RL
---

### 模仿学习

#### Behavior Cloning

收集数据 $o_t, a_t$ 进行监督学习得到 $\pi_\theta(a_t|o_t)$

#### 不足原因

* 可以会碰到没见过的数据
* 给定的行为不一定是正确的
* 在序列决策中，一点细小误差将会逐渐积累成大误差。随着误差的变大，会遇到更多没见过的状态，又会导致更大的误差。

![](https://ww1.sinaimg.cn/large/006A69aEly1gb23cjbnkmj30zl0kc0w1.jpg)

#### 本质问题

Drift Problem, 分布不匹配。训练数据的分布与实际遇到的分布不同。

模型训练效果不佳，对于没见到过的数据预测能力较差。

- 非马尔科夫行为：倾向于过去所做出的决策，决策还依赖于过去。
- 多模型行为：面对同一个状态表现出不同的行为。

### 解决方法

解决方法包括人为调整，获取实际轨迹的分布，修改模型和修改数据等。DAgger为修改数据，解决非马尔科夫行为和多模型行为，从而提高模型的训练效果。

#### 人为调整
在自动驾驶中，使用左中右三个摄像头，神经网络的输入为摄像头拍摄到的图片，输出为方向盘转动的角度。对于左摄像头得到的图片，标记的转动角度比正常更朝右一些，右摄像头相反，中间的摄像头不变。这种做法的好处是，当实际测试时决策错误时，汽车的行驶方向发生偏差，所遇到的情况可能仍处在训练集中，则有机会进行校正，从而回到正常的行驶线路上来。
![image.png](https://ww1.sinaimg.cn/large/006A69aEly1gb24jmvyznj30lr0lmdjm.jpg)

#### 获取实际轨迹的分布

对于可能出现的各种情况，除了正常状态外，还对各种异常状况进行标记，作为数据的一部分。如果能得到所有轨迹的分布，则很容易纠正实际出现的误差。

实现起来很困难。获取大量数据并标记、不现实。一种可行的方法是首先建模环境，接着通过plan模拟实际出现的各种情况。

![image.png](https://ww1.sinaimg.cn/large/006A69aEly1gb24t37szaj310x0hftf1.jpg)

#### Dataset Aggregation

将测试集不断加入到训练集中，使得真实遇到的数据的分布等同于训练数据的分布 --> Online Learning

重复以下过程：

* 使用训练集$D$训练策略，得到 $\pi_\theta(a_t|o_t)$
* 运行策略得到数据集$D_\pi$
* 对数据集$D_\pi$进行人工标记$a_t$
* $D \leftarrow D\bigcup D_\pi$

#### 解决非马尔科夫行为

使用所有的观测值作为输入。使用共享的CNN编码特征，再利用RNN进行序列处理。RNN为模型添加了记忆功能。

#### 解决多模型行为

多模型行为: 输出决策不是单一固定的。比如遇到障碍，可以从左边绕，也可以从右边绕。

多模型行为可以分为离散决策和连续决策。解决该问题的出发点时使得模型可以对于同一个状态，输出多个结果。

##### 离散决策

在模型最后添加softmax层，输出不同的决策概率。

##### 连续决策 

* 输出混合的高斯分布。混合密度网络： N个高斯分布之和, $\pi(a|o) =\sum w_i N(\mu_i,\Sigma_i)$

* 隐变量模型。对于输入添加噪声扰动$N(0, 1)$。简单地加入噪声对网络的影响较小。相关方法包括Normalizing flow / realNVP, stein variational gradient descent。  

* 自动回归离散化。  对高维的连续变量进行离散化，一次离散化一个维度。训练多个网络，每一次输出减少一个连续变量维度，增加一个相对应的离散变量维度。缺点为较难实现、需要进行架构设计、离散化存在误差。

#### 总结

模仿学习指采用观测值和行为值作为训练数据进行监督学习。

由于分布偏移问题，模仿学习的效果有时表现不佳。

改进模仿学习的部分方法:

* 人为调整
* 从稳定的片分布中采样
* 添加更多在线数据，Dagger
* 更好的模型

模仿学习的问题：

* 需要人为提供数据。深度学习在数据量大时性能较好。
* 人类在某些情况下较难提供正确的判断。如电机电压大小，多臂机器人。
* 人类可以自动学习，机器也许也能做到类似的事情。
  * 可以有无线的数据。
  * 连续地自我提升。