---
title: CS294-002-监督学习和模仿学习
mathjax: true
date: 2019-09-15 16:41:00
categories: RL(CS294)
tags: RL
---

### 模仿学习

#### Behavior Cloning

收集数据 $o_t, a_t$ 进行监督学习得到 $\pi_\theta(a_t|o_t)$

不足原因：

* 可以会碰到没见过的数据
*  给定的行为不一定时正确的
* 在序列决策中，一点细小误差将会逐渐积累成大误差

#### Dataset Aggregation

将测试集不断加入到训练集中 ---> Online Learning

重复以下过程：

* 使用训练集$D$训练策略，得到 $\pi_\theta(a_t|o_t)$
* 运行策略得到数据集$D_\pi$
* 对数据集$D_\pi$进行人工标记$a_t$
* $D \leftarrow D\bigcup D_\pi$

#### 分布不匹配

在序列决策中，一点细小误差逐渐积累成大误差。

模型训练效果不佳导致。

模型训练效果不佳原因：

* 非马尔科夫行为：倾向于过去所做出的决策，决策还依赖于过去。
* 多模型行为：面对同一个状态表现出不同的行为。

解决方法：

* 非马尔科夫行为：使用所有的观测值作为输入。使用共享的CNN编码特征，再利用RNN进行序列处理。
* 多模型行为: 输出决策不是单一固定的。比如遇到障碍，可以从左边绕，也可以从右边绕。
  * 离散决策。在模型最后添加softmax层，对不同的决策输出概率。
  * 连续决策。 

多模型行为连续决策解决方法：

* 输出混合的高斯分布

  混合密度网络： N个高斯分布之和, $\pi(a|o) =\sum w_i N(\mu_i,\Sigma_i)$

* 隐变量模型

  对于输入添加噪声扰动$N(0, 1)$

*  自动回归离散化

  对高维的连续变量进行离散化，一次离散化一个维度。

#### 总结

模仿学习指采用观测值和行为值作为训练数据进行监督学习。

由于分布偏移问题，模仿学习的效果有时表现不佳。

改进模仿学习的部分方法:

* 认为调整
* 从稳定的片段分布中采样
* 添加更多在线数据，Dagger
* 更好的模型

