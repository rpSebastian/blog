---
title: '[论文阅读]Monte Carlo Sampling for Regret Minimization in Extensive Games'
mathjax: true
date: 2019-10-19 19:37:56
categories: Paper
tags: 
    - Paper
    - CFR
---

| 标题 | Monte Carlo Sampling for Regret Minimization in Extensive Games |
| ---- | ------------------------------------------------------------ |
| 团队 | Alberta、CMU、Yahoo                                          |
| 时间 | 2009                                                         |
| 出处 | nips                                                         |

## Introduction

CFR算法通过最小化反事实后悔值来最小化总后悔值，使得平均策略收敛于纳什均衡。当时的CFR算法具有以下缺点:

* 多数实现针对卡牌游戏进行优化，对发牌进行抽样，这是一种不通用的方法。
* CFR在迭代时需要已知对手的策略，使得该方法不适用于在线学习。

该论文提出了基于采样的CFR算法，使用给出了通用形式，又详细介绍了两种方法: 结果采样(采样每一个对局)和外采样(采样对手和机会节点的动作)。基于采样的CFR算法在期望上与原CFR相同，但是方差较大，通过最小化后悔值的界限体现出来。结果采样中对手的策略被抵消掉，所以可以用于在线学习，结果采样的总迭代时间减少了。

## 蒙特卡洛CFR

### 通用方法

基于采样的CFR的基本思路是将所有叶子节点划分若干块，每次以某种概率采样某一块，计算该块内的所有叶子节点对其到根路径上的节点的贡献，对于计算的贡献值为原有的方法除以选取该块的概率，从而保证在期望上是相同的。

将所有叶子节点划分为若干个集合 $Q = \{ Q_1, ..., Q_r\} $, 选取某个集合的概率为$q_j$, 保证 $\sum_{j=1}^{r} q_j = 1$， 所有集合的并覆盖叶子节点。

定义一个叶子节点在每一次迭代中被选取的概率$q(z) = \sum_{j:z\in Q_j}$, 使用求和符号的意思时一个叶子节点可能被划分在多个集合中，一般来说都不会重复划分叶子节点。

在提出了CFR的论文中，CFR的定义为
$$
v(\sigma, I) = \sum_{h\in I, z\in Z}\pi^\sigma_{-i}(h)\pi^\sigma(h, z)u_i(z)
$$

将该定义转化为直接枚举每个叶子节点，则可以确定其对应的$h$节点, 定义为 $z[I]$，则CFR的定义可以表示为
$$
v(\sigma, I) = \sum_{z\in Z_I}\pi^\sigma_{-i}(z[I])\pi^\sigma(z[I], z)u_i(z)
$$
在一次采样中，每个节点的采样反事实价值为

$$
\tilde{v} _i (\sigma, I | j) = \sum_{z\in Q_j \cap Z_I} \frac{1}{q(z)}u_i(z)\pi^\sigma_{-i}(z[I])\pi^\sigma(z[I], z)
$$

在多次采样下， 采样反事实后悔值的期望值与反事实后悔值相同。

同样地，可以定义每个节点采取某个动作的立即反事实后悔值。
$$
\tilde{r}(I, a) = \tilde{v} _i (\sigma^t_{(I\rightarrow a)}, I) - \tilde{v}_i(\sigma^t, I)
$$

### 结果采样CFR

结果采样CFR中对于每一条到叶子节点的路径，对应于一个子集合，即每个子集合包含的节点数量为1，选取子集合的概率为这条路径上所有策略的乘积。即对于一个策略组合$\sigma^\prime$ , $q(z) = \pi^{\sigma^\prime}(z)$。

则对于该叶子节点，计算其对路径上每一个节点的所有动作的立即反事实后悔值的贡献。

$$
r(I, a) =\left\{
\begin{aligned}
w_I *(1 - \sigma(a | z[I])) && if (z[I]a) \in z \\
-w_I *\sigma(a|z[I])&&otherwise
\end{aligned}
\;\;\;where \;w_I = \frac{u_i(z)\pi^\sigma_{-i}(z)\pi_i^\sigma(z[i]a, z)}{\pi^{\sigma^\prime}(z)}
\right.
$$

如果叶子节点的采样概率与对手的策略相同，则可以把$\pi_{-i}^\sigma(z)$抵消掉，此时式子的计算不再依赖于对手的概率，所以该算法可以推广为在线学习算法。

### 外采样CFR

外采样CFR对于对手策略和机会策略进行采样，即对于一个策略组合$\sigma ^ \prime$, $q(z) = \pi^{\sigma^\prime(z)}_{-i}$。实际每次计算中，对于博弈树进行遍历，遇到对手策略或机会策略节点，根据实际策略采样一个动作，选择这个动作的边向下遍历，遇到自己策略的节点时，则选择所有动作的边向下遍历。则实际遍历到的是整颗博弈树的一颗子树，该棵子树中包含多个block，一个block中的所有叶子节点的 $\pi_{-i}$ 值相同。

则对于每个叶子节点，同样计算对所有自己决策节点的每一个动作的贡献。

$$
\sum_{z\in Q \cap Z_I} u_i(z) (\pi_i^\sigma(z[I]a, z) - \pi_i^\sigma(z[I], z))
$$

其中 $q(z)$ 与 $\pi_{-i}(z[I])$ 和 $\pi_{-i}(z[I], z)$ 进行抵消。 

## 实验结果

在四个游戏中进行实验。

* Goofspiel. 玩家每人拥有 1-K 十三张牌，公共牌也为1-K十三张牌，每次亮出一张公共牌，每个玩家出一张自己的牌进行竞标，牌最大者获胜，拥有公共牌对应的分数，然后每个玩家弃掉竞标的牌，13轮后拥有最多分数玩家获胜。实际实验中使用变形，每个玩家不知道竞标的牌的大小。
* One-Card Poker，每人从牌堆中抽一张牌，加价或不加价一轮，比牌大小。实际实验中牌堆大小为500.
* Princess and Monster， 在一个黑暗空间中怪兽和公主进行移动，当两者的距离小于一定范围时公主被抓住。实际实验中在3*3地图中随机初始位置，逃离者的收益为没有被抓住的步数。
* Latent Tic-Tac-Toe，井字棋变形，每一步双方选择下一步棋的位置，棋子的显示延迟一回合，即当对方下完下一步棋后，才能看到我方当前回合下的棋，如果下棋发生冲突则算输。

![image.png](https://ww1.sinaimg.cn/large/006A69aEly1g86pqegxi1j30ux08fdhh.jpg)

采用四种算法进行评估。

* 朴素CFR。
* 带剪枝的CFR。当某个节点按照策略选取的概率为0时，不再搜索该棵子树。
* 结果CFR。增加随机选取节点。以 $\epsilon$ 概率随机采样， $1 - \epsilon$ 概率按照策略采样，结果发现 $\epsilon = 0.6$ 时较优。
* 外部采样。

![image.png](https://ww1.sinaimg.cn/large/006A69aEgy1g86pxxk6uvj30w80nwgqh.jpg)

横坐标为搜索时遍历节点数，不采用迭代次数的原因时不同算法的每次迭代时间不一样，纵坐标为计算策略的可利用性。

实验结果分析: 

* MCCFR变体的性能显著由于朴素CFR。
* 剪枝对于CFR性能的体现较明显，在后期每次迭代遍历到的节点数大大减小。
* 外部采样在三个游戏中优于结果采样，算法的性能可能和游戏的特性有关。

## 结论

定义了基于采样的CFR算法，并提出了两种采样模式，外采样(每次迭代采样一条路径)，外部采样(每次采样对手的chance的一个固定策略)。对与朴素CFR提出了更紧密的边界，也对两种采样方法提出了边界。

未来工作:

* 检验游戏特性对算法的影响。
* 外采样在在线学习中的应用。
* 采用不完全回忆来抽象动作。

