---
title: '[论文阅读]-Equilibrium Finding in Matrix Games Via Greedy Regret Minimization'
mathjax: true
date: 2022-04-02 11:31:50
categories:
    - Paper
tags:
    - Paper
    - CFR
---

| 标题 | Equilibrium Finding in Matrix Games Via Greedy Regret Minimization |
| ---- | -------------------------------------------------------- |
| 团队 | Facebook                                                  |
| 时间 | 2022                                                     |
| 出处 | AAAI                                                     |

## 摘要

我们在矩阵式博弈中扩展了传统遗憾值最小化框架，在运行时观测到的遗憾值来贪心加权每次迭代。理论上我们的方法保持了之前的收敛速度。在大规模随机矩阵式博弈和Diplomacy的矩阵式子博弈中，贪心加权超过之前的算法，无论是否使用采样，有时超过了几个数量级。

## 介绍

计算博弈论的一个核心目标是构造能够有效收敛到均衡解的算法。近年来，通过自我博弈逼近均衡解的遗憾值最小化算法在扑克领域取得了巨大成功。遗憾值最小化算法是目前在大规模博弈中计算均衡解的最前沿方法，特别是在有大量动作的游戏，或者对收益矩阵的查询过于昂贵，使得无法通过线性规划计算出精确的解。

然而，过去在游戏中进行自我博弈的遗憾值最小化算法通常将每个玩家当成独立的在线学习者，导致错过了加速收敛的机会。特别是在在线学习的设定下，每次迭代的权重相同。在这篇文章中，我们展示通过贪心加权每次迭代来**最小化势能函数**，即衡量与实际均衡距离的函数，可以在实验中保持相同最坏情况下的遗憾边界的同时加速收敛速度。之前的算法如CFR+、Linear CFR通过非均匀加权迭代来获得更快的性能。然而在之前的算法，迭代根据固定的、预先设定的调度算法进行加权。相比而言，我们介绍**贪心加权**，第一个均衡求解遗憾值最小化算法，在运行时利用可用信息来动态加权。我们在多种环境中进行试验。我们发现无论是否采样，贪心加权都显著提升了收敛速度。最后我们发现在常和博弈中贪心加权发现的均衡解通常具有更高的社会福利。

## 背景知识

对于任意的策略序列 $\pi^1, ...,\pi^T$，玩家$i$为没有执行动作 $a_i'$的的加权**外遗憾值**为

$$
R^{E, T} *i(a_i ') = \sum*{t=1}^T w_t\left(u_i(a_i ', \pi_{-i}^t ) - u_i(\pi^t)\right)
$$

然后可以定义玩家$ i  $整体平均外遗憾值

$$
\bar{R}*i^{E, T} = \max*{a_i '\in A_i}\frac{R_i^{E, T}(a_i')}{\sum_{i=1}^T w_t}
$$

类似地，我们可以定义玩家 $i$为在实际执行动作 $a_i^A$时没有交替执行动作 $a_i'$的加权**内遗憾值**为

$$
R_i^{I, T}(a_i^A, a_i') = \sum_{t=1}^T \mathbb{1}[a_i^t = a_i^A] w_t\left( u_i(a_i', a_{-i}^t) - u_i(a^t)\right)
$$

然后整体平均内遗憾值为

$$
\bar{R}*i^{I, T} = \max*{a_i', a_I^A \in A_i} \frac{R_i^{I, T}(a_i^A, a_i'）}{\sum_{t=1}^T w_t}
$$

定义所有玩家平均外遗憾向量为  $\bar{\mathbf{R}}^{E, T}$，平均内遗憾值为$\bar{\mathbf{R}}^{I, T}$。$R_+$为$\max(0, R)$。

过去结论证明，最小化所有玩家的平均外遗憾值得到粗相关均衡，最小化所有玩家的平均内遗憾值得到相关均衡。存在许多著名算法来同时最小化内遗憾值和外遗憾值。在我们外遗憾值最小化实验中，我们使用Blackwell的遗憾值最小化，要求每个玩家根据正遗憾值来选择下个动作。即每个玩家在时间点 $t+1$选择动作$a_i$的概率为

$$
\Pr(a_i^{t+1} = a_i) = \frac{R_{i, +}^{E, t}(a_i)}{\sum_{a_i' \in A_i} R_{i, +}^{E, t}(a_i')}
$$

除了当所有的遗憾值都为非正时，均匀选择动作。

对于内遗憾值最小化，我们主要使用Blackwell的遗憾值最小化的扩展，称为遗憾值匹配算法。遗憾值匹配算法额外添加了一个固定的惯性参数 $\alpha$，因此总是保持一个正概率，以概率1总遗憾值消失。

$$
\operatorname{Pr}\left(a_{i}^{t+1}=a_{i}\right)= \begin{cases}\frac{\alpha}{\alpha+\sum_{a_{i}^{\prime} \in A_{i}} R_{i,+}^{I, t}\left(a_{i}^{t}, a_{i}^{\prime}\right)}, & \text { if } a_{i}=a_{i}^{t} \\ \frac{R_{i,+}^{I, t}\left(a_{i}^{t}, a_{i}\right)}{\alpha+\sum_{a_{i}^{\prime} \in A_{i}} R_{i,+}^{I, t}\left(a_{i}^{t}, a_{i}^{\prime}\right)}, & \text { otherwise }\end{cases}
$$

许多理论证明使用势能函数的检验，定义为正遗憾值的平方的和。遗憾值匹配算法最小化的函数为：

$$
\phi(\bar{\mathbf{R}}^{E, T})=\sum_{i\in P}\sum_{a_i' \in A_i}\left(\bar R_{i, +}^{E, T}(a_i')\right)^2
$$

$$
\phi(\bar{\mathbf{R}}^{I, T})=\sum_{i\in P}\sum_{a_i^A, a_i^B\in A_i}\left(\bar R_{i, +}^{I, T}(a_i^A, a_i^B)\right)^2
$$

外遗憾值匹配保证 $\phi(\bar{\mathbf{R}}^{E, T}) \le \frac{|P|\Delta^2|A|}{T}$，从而保证 $\max_{i\in P}\max{a_i \in A_i} \bar{R}_i^{E, T}(a_i) \le \frac{\Delta\sqrt{|P||A|}}{\sqrt{T}}$。如果所有玩家的平均遗憾值被 $\epsilon$约束，则平均策略是 $O(\epsilon)-$纳什均衡。

## 贪心权重

![](https://s2.loli.net/2022/04/02/UNgmO7P1T8rJjZq.png)

将均衡解集合表示为$E$，在第$t$次迭代，令$\bar{\pi}^{t-1}$表示加权平均策略，$\pi^t$表示第$t$次迭代使用的策略。遗憾值用$\bar{\mathbf{R}}^{t-1}$和$\mathbf{r}^t$表示。朴素遗憾最小化算法给予每次迭代的权重为 $\frac{1}{t}$，然而其他的加权方式可能使得平均策略更加接近均衡解。相比而言，贪心加权在$\bar{\pi}^{t-1}$和$\pi^t$之间选择合适权重来最小化衡量新的平均策略$\bar{\pi}_*^t$与纳什均衡解距离的势能函数。贪心加权在每次迭代中选择权重，来贪心最小化$ \phi_i^{E, T}  $或$\phi_i^{I, T}$。

![](https://s2.loli.net/2022/04/02/Yw2HhBJkZsxO3m1.png)

我们在算法1中描述了完整的贪心加权程序。在每次迭代中用$\mathbf{R}$来表示所有玩家的遗憾值向量，用来确定每次的策略$\pi$，策略$\pi$得到的瞬时遗憾值用$\mathbf{r}$表示。接着计算最小化势能函数的$\mathbf{r}$的权重$w$。遗憾值向量和平均策略使用相同权重$w$更新。为了减少数值不稳定性和移除，衰减已计算的遗憾值和平均策略组合是有益的。在实际实验中，将先前的迭代衰减$\frac{1}{w}$并加权1。

与许多贪心算法类似，贪心加权不能保证比朴素遗憾值匹配算法收敛更快。在两人零和博弈的实验中观测到设置权重下限$\frac{w_{sum}}{2t}$有助于加速收敛。在所有其他设定中，没有观测到这一现象。

计算最优权重是线性搜索的过程。该过程可以用二项搜索或直接检验$\mathcal{O}(|\mathcal{P}||\mathcal{A}|)$个点来近似。这种方法在$\mathbf{r}$计算很昂贵时很有用（比如使用神经网络价值函数）。引文线性搜索只需要一次$\mathbf{r}$的评估，计算$\phi(\mathbf{R} + w\mathbf{r})$时一个简单的代数函数，不需要额外的奖励函数。

贪心加权算法保持了原先的遗憾值最小化算法的收敛性。

## 实验结果

在随机产生的矩阵博弈和7人游戏Diplomacy的子博弈中，将贪心加权算法和领域的最前沿的遗憾值最小化算法进行比较。在实验中评估了以下遗憾值最小化算法：

*   Regret Matching (RM)。

*   Regret Matching+ (RM+).

*   linear RM。

此外先前的工作发现对遗憾值最小化算法的一些额外修改，能够显著提高在两人零和博弈中的收敛性。

*   交替更新。

*   Optimism。**临时**修改猜测遗憾值，使得最新的迭代轮次被计算两次。

### 每次迭代的纯策略和混合策略

为了解释结果，有必要讨论在两人零和设定下遗憾值匹配经常使用的一个技巧。在计算纳什均衡时，可以在每次迭代时模拟每个玩家执行混合策略，而不是采样一个单独动作。这种修改带来显著的收敛速度提升。然而，使用混合策略带来了一些重要的计算缺陷，因为每次迭代的计算复杂度变成了由$O(|P||A|)$变成了$O(|P||A|^{|P|})$。

