---
title: '[论文阅读]Deep Counterfactual Regret Minimization'
mathjax: true
date: 2020-01-18 17:02:10
categories:
	- Paper
tags:
	- RL
	- Paper
	- CFR
---

| 标题 | Deep Counterfactual Regret Minimization |
| ---- | --------------------------------------- |
| 团队 | CMU、Facebook                           |
| 时间 | 2019                                    |
| 出处 | ICML                                    |

### Abstract

朴素的CFR算法需要遍历整颗博弈树，而在大型游戏中，往往需要在使用CFR之前进行对游戏进行抽象，抽象的方法与具体领域有关，且会损失一些重要信息，一个好的抽象方法又与游戏的均衡策略有关。该论文避免对游戏进行抽象，使用深度神经网来拟合CFR在游戏中的表现。

具体而言，该论文使用一个价值神经网络来拟合每个信息集的动作反事实后悔值，一个策略神经网络来拟合最后的平均策略。在每次迭代中，首先使用k次外部采样，使用前一次的神经网络来采样对手的策略，获得自己的反事实后悔值存放在经验回放池中，采样完后训练一个新的神经网络来拟合所有的动作反事实后悔值。在每次采样对手的策略时，又将策略保存在另一个经验回放池中，用来训练最后的策略神经网络。

### Introduction

不完全信息博弈中，期望找到近似的均衡解，任何人不能单方面改变策略来获取更高的收益，即任何人的策略是对于他人的最佳应对策略。

CFR算法通过迭代在双人零和游戏中收敛到纳什均衡解。大型游戏中首先对游戏进行抽样，抽样方法依赖于具体的领域知识，之后再用表格式的CFR算法进行求解，由于抽样的存在，最后的解是对均衡解的粗糙逼近。

在强化学习中引入深度神经网络取得了较好效果，不需要具体领域知识就可以学习到较好的策略，但在不完全信息中很难收敛到均衡解。

NFSP(Neural Fiction Self Play, 2016)在之前在不完全游戏中的前沿方法。

DeepCFR使用深度神经网络拟合函数来逼近表格式方法，证明了能收敛到 ϵϵ 纳什均衡，在扑克变体包括双人有限注德州扑克进行实验。

### Notation And Background

限定游戏在双人零和博弈下, $P=\{1,2\}$, $u1=−u2P=\{1,2\}$, $u1=−u2$

限定游戏为完全回忆，即如何两个节点不在同一个信息集下，这两个节点的后驱节点也互相不再同一个信息集下。

在Regret Matching中，在正后悔值求和大于0时按比例分配概率，等于0时均匀分配概率。在该论文中在等于0时固定选择反悔值最高的动作，实验发现这能更好的处理逼近误差。

实验发现交替累积玩家后悔值并计算策略，要比同时累积玩家后悔值并计算策略，能更快地收敛。

### Related Work

- First-order methods(2010, 2018)，另一种收敛到纳什均衡的方法，界限优于CFR，但实际使用中CFR变体更快，更稳定，可能更容易和函数逼近方法结合。
- Neural Fictious Self Play(2016)，将Fictitious Play和深度学习函数结合，收敛速度慢于CFR。
- Model-free policy gradient(2019)，调整参数来最小化后悔值，达到和NFSP类似性能。
- DeepStack等(2017, 2018)，在深度限制的子游戏中使用深度学习来评估价值，但在子游戏中使用表格式方法计算价值。
- 大规模函数逼近CFR在单一智能体设定下使用。该论文提出的是多智能体设定，且方法具有很大不同。
- Regression CFR(2015)，建立回归树函数逼近CFR，手工设定大量特征，再利用线性回归逼近后悔值，手工设定特征和抽象一样依赖于领域知识。
- Double Neural CFR(2018)，理论上有缺陷，只考虑小游戏。

### Description

DeepCFR的目标时逼近CFR的行为，不在每个信息集上计算和累积后悔值，而是使用深度神经网络来函数逼近，由相似的信息集进行推广。

DeepCFR在每次迭代 tt 中，根据外部采样进行 KK 次博弈树的部分遍历。在遇到每个信息集时，使用 $\theta^{t-1}$定义的价值神经网络来输出每个动作的总反事实后悔值，进而得到选择每个动作的概率，即策略 $\sigma^t$。神经网络的输出 $V(I,a\vert \theta^{t-1})$ 近似为总反事实后悔值 $R^{t−1}(I,a)$。

当遍历到叶子节点时，返回其收益。反向传播到对手和机会节点时，原样返回。反向传播到自己节点时，将所有动作的收益按照计算出来的策略 $\sigma^t$进行加权求和，同时也可以求出此次采样下每个动作的立即后悔值，将这些后悔值放入经验回放池 $M_{v,p}$中作为之后神经网络的训练样本，如果满了则使用鱼塘采样，保证每个样本被选取的概率随机。

当 $K$ 次采样完成后，使用经验回放池 $M_{v,p}$ 作为训练数据，重新训练一个新的 θtθt 定义的价值神经网络，最小化输出和立即反事实动作后悔值之间的差距。由于所有立即反事实动作后悔值的平均值与总反事实动作后悔值成比例，所以经验回放池中的数据除了鱼塘采样外不会被丢弃。

可以使用其他满足 Bregman Divergence 的损失函数。

除了价值网络外，使用一个单独的策略网络来拟合最后的平均策略，因为平均策略才收敛到纳什均衡。再维护一个经验回放池 $M_\Pi$，每次迭代中将计算得到的策略加入经验回放池，并使用迭代轮数 $t$ 进行加权。

如果迭代次数和神经网络的大小很小，则可以把每一次的价值网络保存下来。在实际每次需要进行决策时，随机挑选一个价值网络输出策略。

### Experimental Setup

在有限注两轮德州扑克中进行实验，具有 $10^{12}$个节点，$10^9$个信息集。有限注德州扑克具有 $10^{17}个节点，$ $10^{14}$ 个信息集。

在两个游戏中，与 NFSP 和使用抽象的方法进行比较。

#### 网络结构

两个网络采用同样的结构，7层 98948个参数组成。网络的输入为信息集，由卡牌集合和投注历史组成。卡牌有3个embedding层之和进行表示：rank(1-13)，suit(1-4)，card(1-52)。投注历史用一个二进制向量表示是否投注，一个浮点数向量表示投注大小，向量的长度为 6*轮数。

每一层为全连接层，包括Relu激活函数和跳跃连接，即$x_{i+1}=Relu(Ax+x)$，网络的最后一层特征进行了归一化。

对于价值网络，输出为每个动作的后悔值。对于平均策略网络，输出为归一化的每个动作选择的概率。

![image.png](https://ww1.sinaimg.cn/large/006A69aEgy1g89epluhdej30if07wwg6.jpg)

#### 模型训练

对于每个网络的经验回放池分配了 $4∗10^7$信息集的大小。价值网络在每次迭代中随机初始化，重新开始训练，实验证明这比接着训练具有更快的收敛速度。对于两轮版有限注德州扑克，进行了4000次小批量随机梯度下降迭代，batch-size为10000，使用学习率为0.0010.001 的Adam学习器进行优化。对于有限注德州扑克，进行了32000次迭代，batch-size为20000。

#### Linear CFR

LInear CFR是在计算时使用迭代次数 $t$ 进行加权，虽然不会收敛到更好的结果，但可以更快地收敛。

具体而言，在经验回放池中额外保存迭代次数 $t$ ,在每 $T$ 次训练网络时按照 $2/T$进行缩放，我的理解时权重为 $2t/T$。

#### DeepCFR 算法

![image.png](https://ww1.sinaimg.cn/large/006A69aEly1g89ld4tvnmj31160cj41c.jpg)

![image.png](https://ww1.sinaimg.cn/large/006A69aEly1g89mscfraaj31190l7q87.jpg)

### Experimental Results

在两轮有限注德州扑克环境下实验结果，横坐标是遍历到的节点，纵坐标时可利用性，图中的收敛速度忽视了神经网络训练所花费的时间。

比较对象:

- Abstraction, 将 $10^9$个节点使用人为设计的特征，利用 K-means 进行聚类，然后再使用外采样线性CFR进行迭代求解。
- Lossless Abstraction。仅将策略无关的节点聚类在一起，比如在顺子牌型中花色无关，该种抽样不会影响最后结果。
- NFSP。深度学习拟合Fictitious Play。

图像显示结果：

- DeepCFR和最少抽样的方法达到了相同可利用性，但是具有更快的收敛速度。
- 由于神经网络的训练，DeepCFR的实际运行时间可能会更长，但是具有不依赖于具体特征的优点。

![image.png](https://ww1.sinaimg.cn/large/006A69aEly1g89mku3e81j30iu0cq404.jpg)

横坐标为每次迭代中采样的数量，纵坐标为可利用率。采样的数量越少，收敛速度越慢，需要收集更多的数据来减少方差。最后每一种采样方法都能收敛到近似相同的可利用率。

![image.png](https://ww1.sinaimg.cn/large/006A69aEgy1g8ao4vi18kj30ck087t9m.jpg)

横坐标为每次训练网络时进行小批量随机梯度下降的次数。该数值不影响收敛速度，但会影响最后收敛到的可利用率。可能原因时训练次数减少使得网络的拟合能力下降。

![image.png](https://ww1.sinaimg.cn/large/006A69aEgy1g8ao8kpgnsj30cl08dq3r.jpg)

网络模型参数的影响。

![image.png](https://ww1.sinaimg.cn/large/006A69aEgy1g8aobmhw3bj30cr087t8z.jpg)

显示了使用线性加权，从头训练网络，在所有后悔值小于0时随机选择动作的影响。不从头训练网络时最后收敛到的可利用率变大，可能是由于网络陷入了局部最优值。

![image.png](https://ww1.sinaimg.cn/large/006A69aEgy1g8aoey5eesj30el096aaw.jpg)

鱼塘采样和滑动窗口对实验的影响。滑动窗口方法在内存占满后不再继续收敛。

![image.png](https://ww1.sinaimg.cn/large/006A69aEgy1g8aohxsfyuj30em094mxl.jpg)

### Conclusion

将CFR算法与深度神经网络函数拟合相结合，试图在大规模不完全信息游戏中寻找纳什均衡。该方法在理论上可行，与大规模扑克游戏中达到了与使用具体领域知识方法的相似性能。

将DeepCFR扩展到更大的游戏中，可能需要结合其他方法，如可扩展的采样策略，减少采样的方差。