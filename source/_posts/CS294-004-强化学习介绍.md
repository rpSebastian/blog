---
title: CS294-004-强化学习介绍
mathjax: true
date: 2019-09-16 21:49:07
categories: RL(CS294)
tags: RL
---

### 定义

#### 符号

$s_t$ 表示状态，$o_t$表示观测值，$a_t$表示动作，$\pi(a_t|o_t)$表示部分观测下的策略，$\pi(a_t|s_t)$表示完全观测下的策略。

#### 模仿学习

收集训练数据$\lt o_t,a_t\gt$ ，进行监督学习得到$\pi_\theta(a_t|o_t)$

#### 奖励函数

$r(s, a)$ 定义了在某个状态下执行某个动作后得到的奖励。但强化学习期望得到的是全局的收益最大，即暂时的较小奖励可能会带来更大收益。

#### 马尔科夫链

##### 马尔科夫链

$$
M = \{S, T\}
$$

* S 为状态空间
* T为状态转移算子 $T_{i, j} = p(s_{t+1} = i | s_t=j)$, 可以将一个状态分布转移到另一个状态分布。 
* 若$\mu_{t, j} = p(s_t = i)$ , 则 $\vec \mu_{t+1} = T\vec\mu_t$

##### 马尔科夫决策链 MDP

$$
M = \{S, A, T, r\}
$$

* S为状态空间
* A为动作空间
* T为状态转移算子
* r为奖励函数 $r: S × A \rightarrow \mathbb{R}$ 

##### 部分观测马尔科夫决策链 POMDP

$$
M = \{S, A, O, T, \epsilon, r\}
$$

* S 为状态空间
* A 为动作空间
* O 为观测空间
* T 为状态转移算子
* $\epsilon$ 为散发概率，在某状态下观测到某个现象的概率。 $p(o_t|s_t)$
* r 为奖励函数

#### 强化学习目标

强化学习为一个与环境不断交互的过程，从初始状态出发，根据制定的策略选择一个动作进行执行，环境给出下一个到达的状态以及对于该状态和动作的奖励。

强化学习目标是制定一个策略，来最大化所有奖励之和的期望。
$$
\theta^*=arg\max_\theta E_{\tau\sim p_\theta(\tau)}[\sum_t r(s_t,a_t)]
$$
一个完整的交互过程被称为 trajectory， episode， $\tau$, 完整地展开为一个序列$s_1, a_1, ... , s_T,  a_T$
$$
p_\theta(\tau)=p_\theta(s_1, a_1, ..., s_T, a_T) = p(s_1) \prod_{t=1}^{T}\pi_\theta(a_t|s_t)p(s_{t+1}|s_t, a_t)
$$
若将$s_t, a_t$视为一个整体，则马尔科夫决策链可以转化为马尔科夫链。
$$
p(s_{t+1}, a_{t+1} | s_t, a_t) = \pi_\theta(a_t|s_t) p(s_{t+1}|s_t,a_t)
$$
故最大化奖励之和的期望可以进行转化
$$
\theta^*=arg\max_\theta \sum_{t=1}^T E_{(s_t,a_t)\sim p_\theta(s_t, a_t)}[r(s_t, a_t)]
$$
当序列为无限时，则当$(s_t,a_t)达到稳态分布时来评估策略，此时有 $$\mu = T\mu$
$$
\theta^*=arg\max_\theta E_{(s,a)\sim p_\theta(s, a)}[r(s, a)]
$$

### 强化学习算法结构

![2019-09-17 19-33-51 的屏幕截图.png](https://ww1.sinaimg.cn/large/006A69aEgy1g72qzb478ej30ek09vglu.jpg)

通常强化学习算法由3个部分组成

* 运行策略进行采样，与环境交互收集数据。
* 分析收集到的数据，来提取将来发生的信息，估计收益。
* 优化策略。

#### 基于策略梯度方法

* 采样数据
* 计算收益 $J(\theta)$ = $E_\pi[\sum r_t]$

*  优化策略 $\theta \leftarrow \theta + \alpha \bigtriangledown_\theta J(\theta)$

#### 基于模型方法

* 采样数据
* 根据数据计算模型，$s_{t+1} = f_\phi(s_t, a_t)$, $r_t = g_\theta(s_t, a_t)$
* 反向传播优化策略

### 值函数和Q函数

$$
\sum_{t=1}^T E_{(s_t,a_t)\sim p_\theta(s_t, a_t)}[r(s_t, a_t)]
$$

对收益的计算公式进行分解可以得到一个递归的条件期望表达式
$$
E_{s1\sim p(s1)}[E_{a1\sim\pi(a_1|s_1)}[r(s_1,a_1) + E_{s2\sim p(s2)}[E_{a_2\sim\pi(a_2|s_2)}[r(s_2,a_2) + ...|s2]|s_1,a_1]|s_1]
$$

#### Q函数

在$s_t$状态执行$a_t$开始期望得到的总收益。
$$
Q^\pi(s_t, a_t) = \sum_{t'=t}^T E_{\pi_\theta}[r(s_{t'}, a_{t'})|s_t,a_t]
$$

#### 值函数

在$s_t$状态开始期望得到的总收益
$$
V^\pi(s_t) = \sum_{t'=t}^T E_{\pi_\theta}[r(s_{t', }, a_{t'})|s_t]
$$

$$
V^\pi(s_t) = E_{a_t\sim\pi(a_t|s_t)}[Q^\pi(s_t,a_t)]
$$

$s_1$ 状态的值函数即为强化学习的目标
$$
E_{s_1\sim p(s_1)}[V^\pi(s_1)]
$$

#### 重要使用方法

*  给定策略$\pi$, 在得到 $Q^\pi(s, a)$后，可以优化策略。将每个状态的策略修改为Q值最大的动作，则该策略讲不差于原策略。
* 使用Q函数来计算梯度，来提升较优动作的概率。如果$Q^\pi(s,a)>V^\pi(s)$则该动作优于平均动作。

### 强化学习算法类型

* policy gradients: 直接优化收益计算表达式。基于样本采样的近似，再计算策略表达式的梯度。
* Value-based: 直接估计值函数和Q函数，通过神经网络来计算，使用argmax来优化策略。
* Actor-critic: 两者的结合。在得到值函数和Q函数后，通过计算策略表达式的梯度来优化策略
* Model-based RL： 首先估计模型，再提升策略。方法包括使用模型进行规划, 反向传播梯度到策略，使用模型来学习价值函数，使用模型来模拟新的经历。

### 强化学习算法多样性原因

#### 不同的抉择

* 采样效率
* 稳定性

#### 不同的前提假设

* 随机或确定
* 连续或离散
* 片段或无限

#### 在不同的设定下

* 容易表示策略
* 容易表示环境

### 强化学习算法比较

#### 样本效率

样本效率指训练得到一个优秀的策略需要多少样本。

一个重要的抉择在于算法是否是off-policy

* off-policy: 可以通过样本来提升策略，而不需要当前策略产生新的样本。

  > able to improve the policy without generating new samples from that policy.

* on-policy: 每当策略改变时，都需要使用当前策略产生新的样本。

  > each time the policy is changed, even a little bit, we need to generate new samples.

![2019-09-18 17-20-12 的屏幕截图.png](https://ww1.sinaimg.cn/large/006A69aEly1g73skv6974j30oy06i74n.jpg)

on-policy 策略的样本效率低，但可以通过并行算法来增加运行速率。

#### 稳定性

稳定性指算法是否收敛，是否每一次都能收敛，收敛到什么值。

在监督学习中总是通过梯度下降来收敛到误差的较小值。

在强化学习中并不总是梯度下降。

* Q-leaning: 采用的固定点迭代方法。
* Model-based RL: 模型并不是为了优化收益，更精确的模型不一定能带来收益的提高。
* policy gradient： 采用了梯度下降，但是样本效率很低。
* Value function fitting:  在理想情况下最小化贝尔曼误差，但不一定保证只能收敛。

#### 前提假设

* 全观测：价值函数拟合方法默认该假设，可以通过添加循环模块来解决。
* 片段学习：直接策略梯度方法和一些基于模型方法默认该假设。
* 连续和平滑：连续价值函数学习方法和一些基于模型方法默认该假设。

### 强化学习算法举例

#### 价值函数拟合方法

* Q-learning, DQN
* Temporal difference learning
* Fitted value iteration

#### 策略梯度方法

* REINFORCE 
* Natural policy gradient

#### 演员表演家方法

* Asynchronous Advantage Actor-Critic
* Soft Actor-Critic

#### 基于模型方法

* Dyna
* Guided policy search



