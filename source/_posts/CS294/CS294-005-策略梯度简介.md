---
title: CS294-005-策略梯度简介
mathjax: true
date: 2019-09-18 21:24:53
categories: RL(CS294)
tags: RL
---

### 直接策略梯度推导 

强化学习的目标为找出最优的策略，使得期望收益最大。
$$
\theta^* = arg\max_\theta J(\theta) = arg \max_\theta E_{\tau\sim p_\theta(\tau)}[r(\tau)]
$$
对$J(\theta)$进行求导可得
$$
\begin{aligned}
\bigtriangledown_\theta J(\theta) &=\bigtriangledown_\theta  E_{\tau\sim p_\theta(\tau)}[r(\tau)]\\
&= \bigtriangledown_\theta\int p_\theta(\tau)r(\tau)d\tau\\
&=\int \bigtriangledown_\theta p_\theta(\tau) r(\tau)d\tau \\
&=\int p_\theta(\tau)\bigtriangledown_\theta log\,p_\theta(\tau) r(\tau)d\tau\\
&= E_{\tau\sim p_\theta(\tau)}[\bigtriangledown_\theta log\,p_\theta(\tau)r(\tau)]
\end{aligned}
$$
由$p_\theta(\tau)$展开式可得
$$
\begin{aligned}
p_\theta(\tau) &= p(s_1) \prod_{t=1}^T \pi_\theta(a_t|s_t)p(s_{t+1}|s_t, a_t) \\
log\,p_\theta(\tau) &= log \,p(s_1) + \sum_{t=1}^T log\,\pi_\theta(a_t|s_t) + log\,p(s_{t+1}|s_t,a_t)\\
\bigtriangledown_\theta log\,p_\theta(\tau) &=  \sum_{t=1}^T \bigtriangledown_\theta log \pi_\theta(a_t|s_t)
\end{aligned}
$$
故化简后得到的$J(\theta)$的梯度表达式为
$$
\bigtriangledown_\theta J(\theta) = E_{\tau\sim p_\theta(\tau)}[(\sum_{t=1}^T \bigtriangledown_\theta log\,\pi_\theta(a_t|s_t))](\sum_{t=1}^T r(s_t, a_t))
$$
根据采样值来逼近期望值可以得到
$$
\bigtriangledown_\theta J(\theta) \approx \frac{1}{N}\sum_{i=1}^N[(\sum_{t=1}^T \bigtriangledown_\theta log\,\pi_\theta(a_{i,t}|s_{i,t}))](\sum_{t=1}^T r(s_{i,t}, a_{i,t}))
$$

接着使用梯度下降法来修改策略
$$
\theta \leftarrow \theta + \alpha \bigtriangledown_\theta J(\theta)
$$

### REINFORCE

#### algorithm

* 根据策略 $\pi_\theta(a_t,s_t)$ 进行采样 $\tau^i$
* 计算策略梯度 $\bigtriangledown_\theta J(\theta) \approx \frac{1}{N}\sum_i[(\sum_{t=1}^T \bigtriangledown_\theta log\,\pi_\theta(a_t^i|s_t^i))](\sum_{t=1}^T r(s_t^i, a_t^i))$
* 通过梯度来优化策略 $\theta \leftarrow \theta + \alpha \bigtriangledown_\theta J(\theta)$

对于局部观测情况同样适用。

该算法可以认为是形式化了试错学习。对于正确的轨迹具有较高的回报值，更倾向于提高策略概率，对于错误的轨迹回报值低，则倾向于降低策略概率。

![image.png](https://ww1.sinaimg.cn/large/006A69aEly1gb2zaoi34pj30k40a942g.jpg) 

#### 存在问题

本质问题是方差问题。

正常而言对奖励增加一个常数，不会对算法带来影响，但对于REINFORCE算法有很大的影响。REINFORCE算法对奖励值很敏感，根据奖励值的正负和大小比例来调整策略。

![image.png](https://ww1.sinaimg.cn/large/006A69aEly1gb2zgajfxgj31ca0e940k.jpg)

### 降低方差

#### 因果关系

若 $ t < t'$， 则 $t'$ 时刻的策略不会影响 $t$ 时刻的策略。
$$
\begin{aligned}
\bigtriangledown_\theta J(\theta) \approx &\frac{1}{N}\sum_{i=1}^N[(\sum_{t=1}^T \bigtriangledown_\theta log\,\pi_\theta(a_{i,t}|s_{i,t}))](\sum_{t=1}^T r(s_{i,t}, a_{i,t})) \\
= &\frac{1}{N}\sum_{i=1}^N\sum_{t=1}^T \bigtriangledown_\theta log\,\pi_\theta(a_{i,t}|s_{i,t})(\sum_{t'=1}^T r(s_{i,t'}, a_{i,t'})) \\
\rightarrow &\frac{1}{N}\sum_{i=1}^N\sum_{t=1}^T \bigtriangledown_\theta log\,\pi_\theta(a_{i,t}|s_{i,t})(\sum_{t'=t}^T r(s_{i,t'}, a_{i,t'})) \\
\end{aligned}
$$
**reward to go**: $ \hat{Q}_{i,t} = \sum_{t'=t}^T r(s_{i,t'}, a_{i,t'}))$

一般而言，在固定区间的情况下最优策略是时变策略，即在不同的时间段，面临相同的状态时，会执行不同的动作。

而通常的做法是将策略限制为时间无关的策略。

**降低方差的原因**: 减少了数值大小。

#### 添加奖励基准

$$
\begin{aligned}
\bigtriangledown_\theta J(\theta) \approx &\frac{1}{N}\sum_{i=1}^N \bigtriangledown_\theta log \pi_\theta(\tau)r(\tau) \\
=&\frac{1}{N}\sum_{i=1}^N \bigtriangledown_\theta log\pi_\theta(\tau)[r(\tau) - b]
\end{aligned}
$$

一般的做法是令 $b = \frac{1}{N}\sum_{i=1}^{N}r(\tau)$，添加的该项不会改变期望值，但会降低方差。
$$
E_{\tau\sim p_\theta(\tau)}[\bigtriangledown_\theta log \;p_\theta(\tau)b] = \int p_\theta(\tau)\bigtriangledown_\theta log\;p_\theta(\tau)b \;d\tau=\int\bigtriangledown_\theta p_\theta(\tau)b\;d\tau = b\bigtriangledown_\theta\int p_\theta(\tau)d\tau = 0
$$
