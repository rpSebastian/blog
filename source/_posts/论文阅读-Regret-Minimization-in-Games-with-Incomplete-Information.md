---
title: '[论文阅读]Regret Minimization in Games with Incomplete Information'
mathjax: true
date: 2019-10-15 12:27:36
categories: Paper
tags: 
    - RL
    - Paper
    - CFR
---

| 标题 | Regret Minimization in Games with Incomplete Information |
| ---- | -------------------------------------------------------- |
| 团队 | Alberta                                                  |
| 时间 | 2008                                                     |
| 出处 | nips                                                     |

###  论文梗概

该论文的作者为阿尔伯塔大学的团队，该团队在扑克博弈方面有着深厚的积累，之前发表的DeepStack论文也是出自该作者，之后也要抽时间读一下那篇论文。

该论文主要提出了CFR算法，为双人博弈中寻找纳什均衡解提供了一种通过自我博弈迭代的方法，提供了理论上界证明，在双人有限注德州扑克对弈中进行了实验。CFR算法十分重要，之后的发展大多是在此基础上进行改进。

### 背景知识

对于扑克这一类游戏，可以归纳为不完全信息的扩展式博弈。扩展式博弈是指多个玩家轮流进行决策，而不是同时进行决策。不完全信息是指，与围棋、象棋不同，每个玩家对于场面上的信息不是完全掌握，具体在扑克中就是不知道对方的手牌。所以可以将信息分为私有信息和公共信息，私有信息就是玩家的手牌，公共信息就是公共牌和双方玩家的动作序列。

针对于扩展式博弈轮流进行决策的特性，可以建立出一棵博弈树，博弈树的每条边对应于玩家的一种决策。对于非玩家进行决策的内容，将其抽象为一个玩家，称为chance，可以理解成运气成分。

![2019-10-15 12-59-16 的屏幕截图.png](https://ww1.sinaimg.cn/large/006A69aEly1g7ysspbhu0j30hq0d575c.jpg)

在博弈树中一个重要概念就是信息集，我的理解是对于每个人而言，将所有其具有同样信息的节点聚合起来，称为信息集。就扑克而言，同一个信息集的两个状态，具有相同的初始手牌和双方相同的动作序列，唯一的不同在于对手的手牌不同，但这一点对于本方玩家是不知道的，所以具有相同信息集。信息集的重要意义在于可以为这些状态制定一个统一的策略，这也符合人类制定策略的直觉。即当我们掌握相同的信息时，制定的策略一般都是相同的。

策略就是对于玩家在某个信息集下，执行每一种动作的可能，也就是对于可行动作的概率分布。讲所有玩家的策略组合在一起，就是一个策略组合，当一个策略组合确定下来后，就可以求出博弈树下到达每一个节点的概率了。

进行一场博弈肯定会有输赢，博弈树中的所有节点就是终止节点，可以根据游戏规则计算出输赢的金钱，也就是该节点的收益。当确定了概率组合之后，也就可以算出每个玩家的总体期望收益了。

在制定博弈策略时，两个重要策略是最佳应对策略和纳什均衡策略。

最佳应对策略是指假设对手的策略固定，那么找一个策略使得我能赢的期望收益最大，这个策略就是最佳应对策略。

纳什均衡策略是指双方的策略达到了一种动态均衡，互相为对方的最佳应对策略，假如某一方改变策略，必定会损失金钱。

评价一个策略的好坏的一个指标是可利用性。可利用性是指对方采用最佳应对策略与我的策略进行对打，对方的期望收益是多少。可利用性越低，说明我们在实际中输的可能性越小。当可利用性为0时，就说明我的策略是那是纳什均衡策略，无论对方的策略是什么，我都能保证我不会输钱。

由于双人扑克博弈时零和博弈，所以在纳什均衡解下双方的金币为0。

下面用一些数学符号来定义这些概念。

| 符号                                                         | 意义                                                         |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| $N = {0, 1, ... n - 1}$                                      | 玩家集合                                                     |
| $H $                                                         | 历史动作序列，也可以看成状态，博弈树上的每一个点             |
| $Z,\; Z \subseteq H$                                         | 叶子节点集合                                                 |
| $P(h) \in N \cup \{c\}$                                      | 定义了每一个状态的行动者，可能为玩家或者chance               |
| $f_c(a \vert h)$                                             | 在状态h下，行动玩家为 $P(h)=c$ , 执行动作 $a$ 的概率         |
| $I_i \subseteq \{h\in H:p(h)=i\}$                            | 信息集，为所有决策玩家为i的决策点的一个划分，信息集之间互不相交。 |
| $u_i(z)$                                                     | 为叶子节点 $z$ 对于玩家 $i$ 的收益                           |
| $\triangle_{u, i} = max_zu_i(z) - min_z u_i(z)$              | 玩家i的收益范围                                              |
| $\sigma_i(a\vert I)$                                         | 玩家i的策略                                                  |
| $\pi^\sigma(h)$                                              | 在策略组合$\sigma$ 下到达状态h的概率                         |
| $\pi_i^\sigma(h)$                                            | 在策略组合$\sigma$下玩家i的策略对到达状态h的概率的贡献       |
| $\pi_{-i}^\sigma(h)$                                         | 在策略组合$\sigma$下其他玩家的策略对到达状态h的概率的贡献    |
| $\pi^\sigma(I) = \sum_{h\in I}\pi^\sigma(h)$                 | 在策略组合$\sigma$ 下到达信息集I的概率                       |
| $u_i(\sigma)=\sum_{h\in Z}u_i(h)\pi^\sigma(h)$               | 策略组合下玩家i的期望收益                                    |
| $u_1(\sigma^\star) = \max_{\sigma_1'\in\Sigma_1}u_1(\sigma_1', \sigma_2)$ | 对于$\sigma_2$的最佳应对策略                                 |

### 后悔值最小化

后悔值最小化是一个在线方法，用来求解纳什均衡的通用方法，在每一次迭代中，寻找一个最佳策略，使得用这个策略去和前几轮的的对手打，获得的平均收益最大。

定义平均总后悔值为
$$
R_i^T = \frac{1}{T}\max_{\sigma_i^*\in\Sigma_i}\sum_{t=1}^T (u_i(\sigma_i^*,\sigma_{-i}^t) - u_i(\sigma^t))
$$
再定义平均策略
$$
\overline{\sigma}_i^t(a|I) = \frac{\sum_{t=1}^T\pi_i^{\sigma^t}(I)\sigma^t(a|I)}{\sum_{t=1}^T\pi_i^{\sigma^t}(I)}
$$

> 定理: 当平均总后悔值小于 $\epsilon$ ，平均策略为 $2\epsilon$ 纳什均衡策略。

所以，如果能最小化平均总后悔值，就等于找到求解纳什均衡的一种方法，

### 反事实后悔值

定义反事实收益为在达到信息集I的前提下，所有玩家使用策略组合$\sigma$ ，玩家i使用固定到达信息机 $I$ 的策略下的 期望收益。
$$
u_i(\sigma, I) = \frac{\sum_{h\in I, z\in Z}\pi^\sigma_{-i}(h)\pi^\sigma(h, z)u_i(z)}{\pi_{-i}^\sigma(I)}
$$
定义 $\sigma\vert_{I \to a}$ 表示玩家$i$ 在信息机$I$ 下改变策略选择动作 $a$，其他玩家不变。则立刻反事实后悔值为
$$
R_{i, imm}^T(I) = \frac{1}{T}\max_{a\in A(I)} \sum_{t=1}^T \pi_{-i}^{\sigma^t}(I)(u_i(\sigma^t\vert_{I\to a}, I) - u_i(\sigma^t, I))
$$

$$
R_{i, imm}^{T, +} = max(R_{i, imm}^T, 0)
$$

> 定理: $R_i^T \le \sum_{I \in I_i} R_{i, imm}^{T, +}(I)$

由该定理可知最小化立刻反事实后悔值之和，就可以最小化平均总后悔值，也就是在逼近纳什均衡策略。

 立刻反事实后悔值的好处是，可以通过策略的迭代来最小化该后悔值。

对于所有信息集的每一个动作，维护
$$
R_i^T(I, a) = \frac{1}{T}  \sum_{t=1}^T \pi_{-i}^{\sigma^t}(I)(u_i(\sigma^t\vert_{I\to a}, a) - u_i(\sigma^t, I))
$$
则策略迭代方式为根据该后悔值的大小按比例分配，若所有后悔值的大小都小于0，则随机挑选动作，具体而言
$$
\sigma_i^{T+1}(a \vert I) = \left\{
\begin{aligned}
&\frac{R_i^{T,+}(I, a)}{\sum_{a\in A(I)}R_i^{T, +}(I, a)}  & &if \; \Sigma_{a\in A(i)} R_i^{T, +}(I, a) > 0\\
&\frac{1}{\vert A(I) \vert}  &  & otherwise 
\end{aligned}
\right.
$$

> 定理: $R_{i, imm} ^T \le \triangle_{u, i}\sqrt{\vert A_i} \vert / \sqrt{T}$,   $R_i^T \le \triangle_{u, i}\vert I_i \vert\sqrt{\vert A_i} \vert / \sqrt{T}$

### 双人德州扑克应用

分为两个步骤进行，首先对游戏进行抽象，将游戏状态进行缩减，再使用CFR算法求解。

在进行抽象时，不改变下注结构，而是对卡牌进行抽象。将卡牌按照牌力大小的平方分成10组，使用平方的原因时当牌力越大时，人们对于自己可能会赢越自信。

### 实验结果

#### 计算方法

在德州扑克中，双方的博弈胜负结果用 mbb/h 表示，mbb 表示大盲下注的千分之一，h表示每一手，整体含义是平均每局能赢钱数与大盲下注量的比例。

一般来说小盲下注量为50， 大盲下注量为100，则在双人中每把牌都fold的结果是 750 mbb/h。如果双方对战结果是 10mbb/h, 那么在统计上来说打一百万场才有 95%的胜率。

可利用性可以认为是与纳什均衡之间的差距，但是在大博弈树下直接计算十分困难，一般采用和较高水准的对手进行多次博弈，计算mbb/h来衡量。虽然策略之间不存在传递性，但是可以大概衡量策略的好坏。

在抽象游戏下，计算可利用性的复杂度降低，但是计算出来的可利用性并不等于原空间下的可利用性。

该论文 通过 $2*10^9$ 次迭代，在抽象空间中达到了 2.2 mbb/h。

#### 收敛速度

![2019-10-15 15-09-24 的屏幕截图.png](https://ww1.sinaimg.cn/large/006A69aEgy1g7ywj2njd2j30ve0e0aby.jpg)

对于抽象组数进行了比较，抽象组数可以表示状态空间/信息集的大小，认为收敛所需的迭代次数与状态空间成线性关系，为之前的定理提供依据。

#### 对战比较

![2019-10-15 15-12-29 的屏幕截图.png](https://ww1.sinaimg.cn/large/006A69aEgy1g7ywmhii7xj30pg08habl.jpg)

将CFR和当时流行的方法进行对战。

### 结论

提出了一种在扩展式博弈中的新的后悔值——反事实后悔值。通过最小化反事实后悔值，可以最小化总后悔值，并且提出了一种通用的方法来有效的最小化反事实后悔值。在扑克中验证了这种方法，表明这种方法可以在$10^{12}$的状态空间下计算出近似纳什均衡。